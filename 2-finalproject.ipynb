{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1281c6d0",
   "metadata": {},
   "source": [
    "# Restaurant Rating Prediction (1–5) — EDA + 3 Skenario **Baru** (Per Model)\n",
    "Notebook ini menyajikan **alur skenario yang benar-benar berbeda secara perlakuan**, sehingga S1 vs S2 vs S3\n",
    "tidak terasa sama.\n",
    "\n",
    "## Definisi Skenario\n",
    "### Skenario 1 — Baseline fine-grained **tanpa** penanganan imbalance\n",
    "- Target rating 1–5.\n",
    "- Train original (imbalanced).\n",
    "- 5 model: SVM, NB, LSTM, BERT, RoBERTa.\n",
    "\n",
    "### Skenario 2 — Penanganan imbalance **tanpa augmentasi teks**\n",
    "- Target rating 1–5.\n",
    "- **Teks tidak diubah**.\n",
    "- Teknik balancing non-semantic:\n",
    "  - SVM: `class_weight='balanced'`\n",
    "  - NB: **duplicate oversampling** (duplikasi sampel)\n",
    "  - LSTM: **weighted loss**\n",
    "  - BERT/RoBERTa: **weighted loss** via `WeightedTrainer`\n",
    "\n",
    "### Skenario 3 — Penanganan imbalance **dengan augmentasi teks**\n",
    "- Target rating 1–5.\n",
    "- Augment kelas minoritas pada train.\n",
    "- Strategi: EDA, Modified EDA (opsional: Backtranslation, BERT augmentation).\n",
    "- 5 model dilatih ulang.\n",
    "\n",
    "## Catatan CPU\n",
    "BERT/RoBERTa tetap aktif. Jika tanpa GPU, konfigurasi dibuat ringan\n",
    "(batch kecil, max_length lebih pendek, opsional shrink train).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "288801aa",
   "metadata": {},
   "source": [
    "## 0. Setup & Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "17645da6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cpu\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# !pip -q install pandas numpy scikit-learn matplotlib nltk tqdm\n",
    "# !pip -q install torch transformers datasets evaluate accelerate\n",
    "# !pip -q install nlpaug\n",
    "\n",
    "import os, re, glob, random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, f1_score, accuracy_score, confusion_matrix\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "import torch\n",
    "\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "HAS_GPU = torch.cuda.is_available()\n",
    "DEVICE = \"cuda\" if HAS_GPU else \"cpu\"\n",
    "print(\"Device:\", DEVICE)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2410e670",
   "metadata": {},
   "source": [
    "## 1. Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dac085f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using DATA_PATH: dataset/reviews.csv\n",
      "Shape: (1100, 6)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>business_name</th>\n",
       "      <th>author_name</th>\n",
       "      <th>text</th>\n",
       "      <th>photo</th>\n",
       "      <th>rating</th>\n",
       "      <th>rating_category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Haci'nin Yeri - Yigit Lokantasi</td>\n",
       "      <td>Gulsum Akar</td>\n",
       "      <td>We went to Marmaris with my wife for a holiday...</td>\n",
       "      <td>dataset/taste/hacinin_yeri_gulsum_akar.png</td>\n",
       "      <td>5</td>\n",
       "      <td>taste</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Haci'nin Yeri - Yigit Lokantasi</td>\n",
       "      <td>Oguzhan Cetin</td>\n",
       "      <td>During my holiday in Marmaris we ate here to f...</td>\n",
       "      <td>dataset/menu/hacinin_yeri_oguzhan_cetin.png</td>\n",
       "      <td>4</td>\n",
       "      <td>menu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Haci'nin Yeri - Yigit Lokantasi</td>\n",
       "      <td>Yasin Kuyu</td>\n",
       "      <td>Prices are very affordable. The menu in the ph...</td>\n",
       "      <td>dataset/outdoor_atmosphere/hacinin_yeri_yasin_...</td>\n",
       "      <td>3</td>\n",
       "      <td>outdoor_atmosphere</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Haci'nin Yeri - Yigit Lokantasi</td>\n",
       "      <td>Orhan Kapu</td>\n",
       "      <td>Turkey's cheapest artisan restaurant and its f...</td>\n",
       "      <td>dataset/indoor_atmosphere/hacinin_yeri_orhan_k...</td>\n",
       "      <td>5</td>\n",
       "      <td>indoor_atmosphere</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Haci'nin Yeri - Yigit Lokantasi</td>\n",
       "      <td>Ozgur Sati</td>\n",
       "      <td>I don't know what you will look for in terms o...</td>\n",
       "      <td>dataset/menu/hacinin_yeri_ozgur_sati.png</td>\n",
       "      <td>3</td>\n",
       "      <td>menu</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     business_name    author_name  \\\n",
       "0  Haci'nin Yeri - Yigit Lokantasi    Gulsum Akar   \n",
       "1  Haci'nin Yeri - Yigit Lokantasi  Oguzhan Cetin   \n",
       "2  Haci'nin Yeri - Yigit Lokantasi     Yasin Kuyu   \n",
       "3  Haci'nin Yeri - Yigit Lokantasi     Orhan Kapu   \n",
       "4  Haci'nin Yeri - Yigit Lokantasi     Ozgur Sati   \n",
       "\n",
       "                                                text  \\\n",
       "0  We went to Marmaris with my wife for a holiday...   \n",
       "1  During my holiday in Marmaris we ate here to f...   \n",
       "2  Prices are very affordable. The menu in the ph...   \n",
       "3  Turkey's cheapest artisan restaurant and its f...   \n",
       "4  I don't know what you will look for in terms o...   \n",
       "\n",
       "                                               photo  rating  \\\n",
       "0         dataset/taste/hacinin_yeri_gulsum_akar.png       5   \n",
       "1        dataset/menu/hacinin_yeri_oguzhan_cetin.png       4   \n",
       "2  dataset/outdoor_atmosphere/hacinin_yeri_yasin_...       3   \n",
       "3  dataset/indoor_atmosphere/hacinin_yeri_orhan_k...       5   \n",
       "4           dataset/menu/hacinin_yeri_ozgur_sati.png       3   \n",
       "\n",
       "      rating_category  \n",
       "0               taste  \n",
       "1                menu  \n",
       "2  outdoor_atmosphere  \n",
       "3   indoor_atmosphere  \n",
       "4                menu  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "DATA_PATH = \"dataset/reviews.csv\"  # <-- ganti sesuai file Anda\n",
    "\n",
    "if not os.path.exists(DATA_PATH):\n",
    "    candidates = glob.glob(\"**/*.csv\", recursive=True)\n",
    "    preferred = [c for c in candidates if re.search(r\"(review|google|maps|restaurant)\", c, re.I)]\n",
    "    if preferred:\n",
    "        DATA_PATH = preferred[0]\n",
    "    elif candidates:\n",
    "        DATA_PATH = candidates[0]\n",
    "\n",
    "print(\"Using DATA_PATH:\", DATA_PATH)\n",
    "\n",
    "df = pd.read_csv(DATA_PATH)\n",
    "print(\"Shape:\", df.shape)\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7306c8b8",
   "metadata": {},
   "source": [
    "## 2. Cleaning & Text Preprocessing (Lemmatization)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45d84feb",
   "metadata": {},
   "source": [
    "Preprocessing:\n",
    "- normalisasi spasi\n",
    "- lowercasing\n",
    "- pembersihan karakter non-alfabet sederhana\n",
    "- tokenisasi\n",
    "- **lemmatization** WordNet dengan POS tagger.\n",
    "\n",
    "Blok download NLTK dibuat robust untuk versi lama/baru.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "380eaa10",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\LENOVO\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\LENOVO\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After cleaning: (1100, 9)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>business_name</th>\n",
       "      <th>author_name</th>\n",
       "      <th>text</th>\n",
       "      <th>photo</th>\n",
       "      <th>rating</th>\n",
       "      <th>rating_category</th>\n",
       "      <th>text_basic</th>\n",
       "      <th>text_lemma</th>\n",
       "      <th>text_clean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Haci'nin Yeri - Yigit Lokantasi</td>\n",
       "      <td>Gulsum Akar</td>\n",
       "      <td>We went to Marmaris with my wife for a holiday...</td>\n",
       "      <td>dataset/taste/hacinin_yeri_gulsum_akar.png</td>\n",
       "      <td>5</td>\n",
       "      <td>taste</td>\n",
       "      <td>We went to Marmaris with my wife for a holiday...</td>\n",
       "      <td>we go to marmaris with my wife for a holiday w...</td>\n",
       "      <td>we go to marmaris with my wife for a holiday w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Haci'nin Yeri - Yigit Lokantasi</td>\n",
       "      <td>Oguzhan Cetin</td>\n",
       "      <td>During my holiday in Marmaris we ate here to f...</td>\n",
       "      <td>dataset/menu/hacinin_yeri_oguzhan_cetin.png</td>\n",
       "      <td>4</td>\n",
       "      <td>menu</td>\n",
       "      <td>During my holiday in Marmaris we ate here to f...</td>\n",
       "      <td>during my holiday in marmaris we eat here to f...</td>\n",
       "      <td>during my holiday in marmaris we eat here to f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Haci'nin Yeri - Yigit Lokantasi</td>\n",
       "      <td>Yasin Kuyu</td>\n",
       "      <td>Prices are very affordable. The menu in the ph...</td>\n",
       "      <td>dataset/outdoor_atmosphere/hacinin_yeri_yasin_...</td>\n",
       "      <td>3</td>\n",
       "      <td>outdoor_atmosphere</td>\n",
       "      <td>Prices are very affordable. The menu in the ph...</td>\n",
       "      <td>price be very affordable the menu in the photo...</td>\n",
       "      <td>price be very affordable the menu in the photo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Haci'nin Yeri - Yigit Lokantasi</td>\n",
       "      <td>Orhan Kapu</td>\n",
       "      <td>Turkey's cheapest artisan restaurant and its f...</td>\n",
       "      <td>dataset/indoor_atmosphere/hacinin_yeri_orhan_k...</td>\n",
       "      <td>5</td>\n",
       "      <td>indoor_atmosphere</td>\n",
       "      <td>Turkey's cheapest artisan restaurant and its f...</td>\n",
       "      <td>turkey s cheap artisan restaurant and it food ...</td>\n",
       "      <td>turkey s cheap artisan restaurant and it food ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Haci'nin Yeri - Yigit Lokantasi</td>\n",
       "      <td>Ozgur Sati</td>\n",
       "      <td>I don't know what you will look for in terms o...</td>\n",
       "      <td>dataset/menu/hacinin_yeri_ozgur_sati.png</td>\n",
       "      <td>3</td>\n",
       "      <td>menu</td>\n",
       "      <td>I don't know what you will look for in terms o...</td>\n",
       "      <td>i don t know what you will look for in term of...</td>\n",
       "      <td>i don t know what you will look for in term of...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     business_name    author_name  \\\n",
       "0  Haci'nin Yeri - Yigit Lokantasi    Gulsum Akar   \n",
       "1  Haci'nin Yeri - Yigit Lokantasi  Oguzhan Cetin   \n",
       "2  Haci'nin Yeri - Yigit Lokantasi     Yasin Kuyu   \n",
       "3  Haci'nin Yeri - Yigit Lokantasi     Orhan Kapu   \n",
       "4  Haci'nin Yeri - Yigit Lokantasi     Ozgur Sati   \n",
       "\n",
       "                                                text  \\\n",
       "0  We went to Marmaris with my wife for a holiday...   \n",
       "1  During my holiday in Marmaris we ate here to f...   \n",
       "2  Prices are very affordable. The menu in the ph...   \n",
       "3  Turkey's cheapest artisan restaurant and its f...   \n",
       "4  I don't know what you will look for in terms o...   \n",
       "\n",
       "                                               photo  rating  \\\n",
       "0         dataset/taste/hacinin_yeri_gulsum_akar.png       5   \n",
       "1        dataset/menu/hacinin_yeri_oguzhan_cetin.png       4   \n",
       "2  dataset/outdoor_atmosphere/hacinin_yeri_yasin_...       3   \n",
       "3  dataset/indoor_atmosphere/hacinin_yeri_orhan_k...       5   \n",
       "4           dataset/menu/hacinin_yeri_ozgur_sati.png       3   \n",
       "\n",
       "      rating_category                                         text_basic  \\\n",
       "0               taste  We went to Marmaris with my wife for a holiday...   \n",
       "1                menu  During my holiday in Marmaris we ate here to f...   \n",
       "2  outdoor_atmosphere  Prices are very affordable. The menu in the ph...   \n",
       "3   indoor_atmosphere  Turkey's cheapest artisan restaurant and its f...   \n",
       "4                menu  I don't know what you will look for in terms o...   \n",
       "\n",
       "                                          text_lemma  \\\n",
       "0  we go to marmaris with my wife for a holiday w...   \n",
       "1  during my holiday in marmaris we eat here to f...   \n",
       "2  price be very affordable the menu in the photo...   \n",
       "3  turkey s cheap artisan restaurant and it food ...   \n",
       "4  i don t know what you will look for in term of...   \n",
       "\n",
       "                                          text_clean  \n",
       "0  we go to marmaris with my wife for a holiday w...  \n",
       "1  during my holiday in marmaris we eat here to f...  \n",
       "2  price be very affordable the menu in the photo...  \n",
       "3  turkey s cheap artisan restaurant and it food ...  \n",
       "4  i don t know what you will look for in term of...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "import nltk\n",
    "\n",
    "resources = [\n",
    "    (\"tokenizers/punkt\", \"punkt\"),\n",
    "    (\"taggers/averaged_perceptron_tagger\", \"averaged_perceptron_tagger\"),\n",
    "    (\"taggers/averaged_perceptron_tagger_eng\", \"averaged_perceptron_tagger_eng\"),\n",
    "    (\"corpora/wordnet\", \"wordnet\"),\n",
    "    (\"corpora/omw-1.4\", \"omw-1.4\"),\n",
    "]\n",
    "\n",
    "for path, name in resources:\n",
    "    try:\n",
    "        nltk.data.find(path)\n",
    "    except LookupError:\n",
    "        nltk.download(name)\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def penn_to_wn(tag):\n",
    "    if tag.startswith(\"J\"): return wordnet.ADJ\n",
    "    if tag.startswith(\"V\"): return wordnet.VERB\n",
    "    if tag.startswith(\"N\"): return wordnet.NOUN\n",
    "    if tag.startswith(\"R\"): return wordnet.ADV\n",
    "    return wordnet.NOUN\n",
    "\n",
    "def basic_normalize(text: str) -> str:\n",
    "    text = str(text).strip()\n",
    "    text = re.sub(r\"\\s+\", \" \", text)\n",
    "    return text\n",
    "\n",
    "def clean_for_lemma(text: str) -> str:\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"[^a-z\\s]\", \" \", text)\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "    return text\n",
    "\n",
    "def lemmatize_text(text: str) -> str:\n",
    "    text = clean_for_lemma(basic_normalize(text))\n",
    "    if not text:\n",
    "        return \"\"\n",
    "    toks = word_tokenize(text)\n",
    "    pos_tags = nltk.pos_tag(toks)\n",
    "    lemmas = [lemmatizer.lemmatize(w, penn_to_wn(t)) for w, t in pos_tags]\n",
    "    return \" \".join(lemmas)\n",
    "\n",
    "assert \"text\" in df.columns, \"Kolom 'text' tidak ditemukan.\"\n",
    "assert \"rating\" in df.columns, \"Kolom 'rating' tidak ditemukan.\"\n",
    "\n",
    "df = df.dropna(subset=[\"text\", \"rating\"]).copy()\n",
    "df[\"rating\"] = df[\"rating\"].astype(int)\n",
    "df = df[df[\"rating\"].between(1, 5)]\n",
    "\n",
    "df[\"text_basic\"] = df[\"text\"].apply(basic_normalize)\n",
    "df[\"text_lemma\"] = df[\"text\"].apply(lemmatize_text)\n",
    "df[\"text_clean\"] = df[\"text_lemma\"]\n",
    "\n",
    "print(\"After cleaning:\", df.shape)\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c145864",
   "metadata": {},
   "source": [
    "## 3. Exploratory Data Analysis (EDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "75790131",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1100 entries, 0 to 1099\n",
      "Data columns (total 9 columns):\n",
      " #   Column           Non-Null Count  Dtype \n",
      "---  ------           --------------  ----- \n",
      " 0   business_name    1100 non-null   object\n",
      " 1   author_name      1100 non-null   object\n",
      " 2   text             1100 non-null   object\n",
      " 3   photo            1100 non-null   object\n",
      " 4   rating           1100 non-null   int64 \n",
      " 5   rating_category  1100 non-null   object\n",
      " 6   text_basic       1100 non-null   object\n",
      " 7   text_lemma       1100 non-null   object\n",
      " 8   text_clean       1100 non-null   object\n",
      "dtypes: int64(1), object(8)\n",
      "memory usage: 77.5+ KB\n"
     ]
    }
   ],
   "source": [
    "\n",
    "df.info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9e3664b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "rating\n",
       "1     80\n",
       "2     72\n",
       "3    172\n",
       "4    316\n",
       "5    460\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "rating_counts = df[\"rating\"].value_counts().sort_index()\n",
    "rating_counts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0a34968e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "plt.figure()\n",
    "plt.bar(rating_counts.index.astype(str), rating_counts.values)\n",
    "plt.title(\"Distribusi Rating (1–5)\")\n",
    "plt.xlabel(\"Rating\")\n",
    "plt.ylabel(\"Jumlah Ulasan\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4a057d24",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>char_len</th>\n",
       "      <th>word_len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1100.000000</td>\n",
       "      <td>1100.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>110.827273</td>\n",
       "      <td>20.051818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>69.144698</td>\n",
       "      <td>12.977619</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>5.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>62.000000</td>\n",
       "      <td>11.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>104.000000</td>\n",
       "      <td>19.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>147.000000</td>\n",
       "      <td>27.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>914.000000</td>\n",
       "      <td>179.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          char_len     word_len\n",
       "count  1100.000000  1100.000000\n",
       "mean    110.827273    20.051818\n",
       "std      69.144698    12.977619\n",
       "min       5.000000     1.000000\n",
       "25%      62.000000    11.000000\n",
       "50%     104.000000    19.000000\n",
       "75%     147.000000    27.000000\n",
       "max     914.000000   179.000000"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "df[\"char_len\"] = df[\"text_basic\"].str.len()\n",
    "df[\"word_len\"] = df[\"text_basic\"].str.split().apply(len)\n",
    "\n",
    "df[[\"char_len\", \"word_len\"]].describe()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d92913ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "plt.figure()\n",
    "plt.hist(df[\"word_len\"], bins=50)\n",
    "plt.title(\"Distribusi Panjang Ulasan (Jumlah Kata)\")\n",
    "plt.xlabel(\"Jumlah kata\")\n",
    "plt.ylabel(\"Frekuensi\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfdfcee3",
   "metadata": {},
   "source": [
    "## 4. Train/Test Split (Konsisten)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8a4b0b9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 880 Test: 220\n",
      "Distribusi Train: Counter({np.int64(5): 368, np.int64(4): 253, np.int64(3): 137, np.int64(1): 64, np.int64(2): 58})\n",
      "Distribusi Test : Counter({np.int64(5): 92, np.int64(4): 63, np.int64(3): 35, np.int64(1): 16, np.int64(2): 14})\n"
     ]
    }
   ],
   "source": [
    "\n",
    "X = df[\"text_clean\"].values\n",
    "y = df[\"rating\"].values\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y,\n",
    "    test_size=0.2,\n",
    "    random_state=SEED,\n",
    "    stratify=y\n",
    ")\n",
    "\n",
    "print(\"Train:\", len(X_train), \"Test:\", len(X_test))\n",
    "print(\"Distribusi Train:\", Counter(y_train))\n",
    "print(\"Distribusi Test :\", Counter(y_test))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f12bfbb9",
   "metadata": {},
   "source": [
    "## 5. Utilitas Evaluasi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c5e0a925",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def eval_report(y_true, y_pred, title=\"Model\"):\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    f1m = f1_score(y_true, y_pred, average=\"macro\")\n",
    "    f1w = f1_score(y_true, y_pred, average=\"weighted\")\n",
    "\n",
    "    print(f\"=== {title} ===\")\n",
    "    print(\"Accuracy :\", round(acc, 4))\n",
    "    print(\"F1-macro :\", round(f1m, 4))\n",
    "    print(\"F1-weight:\", round(f1w, 4))\n",
    "    print()\n",
    "    print(classification_report(y_true, y_pred, digits=4, zero_division=0))\n",
    "    print()\n",
    "    return {\"model\": title, \"accuracy\": acc, \"f1_macro\": f1m, \"f1_weighted\": f1w}\n",
    "\n",
    "def plot_confusion(y_true, y_pred, labels=range(1, 6), title=\"Confusion Matrix\"):\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=list(labels))\n",
    "    plt.figure()\n",
    "    plt.imshow(cm, interpolation=\"nearest\")\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(labels))\n",
    "    plt.xticks(tick_marks, labels)\n",
    "    plt.yticks(tick_marks, labels)\n",
    "    plt.ylabel(\"True\")\n",
    "    plt.xlabel(\"Predicted\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b7f47d8",
   "metadata": {},
   "source": [
    "## 6. Helper Model Klasik (TF–IDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5f716413",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def fit_tfidf(train_texts, test_texts):\n",
    "    vec = TfidfVectorizer(lowercase=True, max_features=50000, ngram_range=(1, 2))\n",
    "    Xtr = vec.fit_transform(train_texts)\n",
    "    Xte = vec.transform(test_texts)\n",
    "    return vec, Xtr, Xte\n",
    "\n",
    "def train_eval_svm(train_texts, train_labels, test_texts, test_labels, suffix=\"\", class_weight=None):\n",
    "    vec, Xtr, Xte = fit_tfidf(train_texts, test_texts)\n",
    "    svm = LinearSVC(class_weight=class_weight)\n",
    "    svm.fit(Xtr, train_labels)\n",
    "    pred = svm.predict(Xte)\n",
    "    res = eval_report(test_labels, pred, f\"Linear SVM{suffix}\")\n",
    "    return res, vec, svm, pred\n",
    "\n",
    "def train_eval_nb(train_texts, train_labels, test_texts, test_labels, suffix=\"\"):\n",
    "    vec, Xtr, Xte = fit_tfidf(train_texts, test_texts)\n",
    "    nb = MultinomialNB()\n",
    "    nb.fit(Xtr, train_labels)\n",
    "    pred = nb.predict(Xte)\n",
    "    res = eval_report(test_labels, pred, f\"Naive Bayes{suffix}\")\n",
    "    return res, vec, nb, pred\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c7820fc",
   "metadata": {},
   "source": [
    "## 7. Duplicate Oversampling (Non-Aug, untuk S2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fbefa197",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train original: Counter({np.int64(5): 368, np.int64(4): 253, np.int64(3): 137, np.int64(1): 64, np.int64(2): 58})\n",
      "Train after duplicate oversampling: Counter({np.int64(4): 368, np.int64(5): 368, np.int64(2): 368, np.int64(3): 368, np.int64(1): 368})\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def duplicate_oversample(texts, labels):\n",
    "    texts, labels = list(texts), list(labels)\n",
    "    counts = Counter(labels)\n",
    "    max_count = max(counts.values())\n",
    "\n",
    "    idx_by = {c: [] for c in counts}\n",
    "    for i, y in enumerate(labels):\n",
    "        idx_by[y].append(i)\n",
    "\n",
    "    new_texts, new_labels = texts[:], labels[:]\n",
    "\n",
    "    for c, idxs in idx_by.items():\n",
    "        need = max_count - counts[c]\n",
    "        if need <= 0:\n",
    "            continue\n",
    "        for _ in range(need):\n",
    "            i = random.choice(idxs)\n",
    "            new_texts.append(texts[i])   # teks sama persis\n",
    "            new_labels.append(c)\n",
    "\n",
    "    return np.array(new_texts), np.array(new_labels)\n",
    "\n",
    "print(\"Train original:\", Counter(y_train))\n",
    "Xt_dup, yt_dup = duplicate_oversample(X_train, y_train)\n",
    "print(\"Train after duplicate oversampling:\", Counter(yt_dup))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54588787",
   "metadata": {},
   "source": [
    "## 8. Helper LSTM (PyTorch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a9535ff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "def build_vocab(texts, max_vocab=30000, min_freq=2):\n",
    "    from collections import Counter as CCounter\n",
    "    counter = CCounter()\n",
    "    for t in texts:\n",
    "        counter.update(str(t).split())\n",
    "    vocab = {\"<pad>\": 0, \"<unk>\": 1}\n",
    "    for w, c in counter.most_common():\n",
    "        if c < min_freq:\n",
    "            continue\n",
    "        if len(vocab) >= max_vocab:\n",
    "            break\n",
    "        vocab[w] = len(vocab)\n",
    "    return vocab\n",
    "\n",
    "def encode(text, vocab, max_len=200):\n",
    "    tokens = str(text).split()\n",
    "    ids = [vocab.get(t, vocab[\"<unk>\"]) for t in tokens[:max_len]]\n",
    "    if len(ids) < max_len:\n",
    "        ids += [vocab[\"<pad>\"]] * (max_len - len(ids))\n",
    "    return ids\n",
    "\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, texts, labels, vocab, max_len=200):\n",
    "        self.texts = list(texts)\n",
    "        self.labels = list(labels)\n",
    "        self.vocab = vocab\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x = torch.tensor(encode(self.texts[idx], self.vocab, self.max_len), dtype=torch.long)\n",
    "        y = torch.tensor(int(self.labels[idx]) - 1, dtype=torch.long)  # 1-5 -> 0-4\n",
    "        return x, y\n",
    "\n",
    "class LSTMClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size, emb_dim=128, hidden_dim=128, num_classes=5):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, emb_dim, padding_idx=0)\n",
    "        self.lstm = nn.LSTM(emb_dim, hidden_dim, batch_first=True, bidirectional=True)\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        self.fc = nn.Linear(hidden_dim * 2, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        emb = self.embedding(x)\n",
    "        out, _ = self.lstm(emb)\n",
    "        last = out[:, -1, :]\n",
    "        last = self.dropout(last)\n",
    "        return self.fc(last)\n",
    "\n",
    "def train_eval_lstm(\n",
    "    train_texts, train_labels, test_texts, test_labels,\n",
    "    epochs=3, max_len=200, batch_size=64, suffix=\"\",\n",
    "    use_class_weights=False\n",
    "):\n",
    "    vocab = build_vocab(train_texts)\n",
    "    train_ds = TextDataset(train_texts, train_labels, vocab, max_len)\n",
    "    test_ds  = TextDataset(test_texts, test_labels, vocab, max_len)\n",
    "\n",
    "    train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n",
    "    test_loader  = DataLoader(test_ds, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    model = LSTMClassifier(len(vocab)).to(DEVICE)\n",
    "\n",
    "    if use_class_weights:\n",
    "        classes = np.array([1,2,3,4,5])\n",
    "        cw = compute_class_weight(class_weight=\"balanced\", classes=classes, y=np.array(train_labels))\n",
    "        class_weights = torch.tensor(cw, dtype=torch.float).to(DEVICE)\n",
    "        criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
    "    else:\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "    def train_one_epoch():\n",
    "        model.train()\n",
    "        total = 0.0\n",
    "        for xb, yb in train_loader:\n",
    "            xb, yb = xb.to(DEVICE), yb.to(DEVICE)\n",
    "            optimizer.zero_grad()\n",
    "            logits = model(xb)\n",
    "            loss = criterion(logits, yb)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total += float(loss.item())\n",
    "        return total / max(1, len(train_loader))\n",
    "\n",
    "    def predict():\n",
    "        model.eval()\n",
    "        preds, trues = [], []\n",
    "        with torch.no_grad():\n",
    "            for xb, yb in test_loader:\n",
    "                xb = xb.to(DEVICE)\n",
    "                logits = model(xb)\n",
    "                p = torch.argmax(logits, dim=1).cpu().numpy()\n",
    "                preds.extend(list(p))\n",
    "                trues.extend(list(yb.numpy()))\n",
    "        preds = np.array(preds) + 1\n",
    "        trues = np.array(trues) + 1\n",
    "        return trues, preds\n",
    "\n",
    "    for ep in range(epochs):\n",
    "        loss = train_one_epoch()\n",
    "        print(f\"[LSTM{suffix}] Epoch {ep+1}/{epochs} | loss={loss:.4f}\")\n",
    "\n",
    "    yt, yp = predict()\n",
    "    return eval_report(yt, yp, f\"LSTM{suffix}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73016ce6",
   "metadata": {},
   "source": [
    "## 9. Helper Transformer (BERT & RoBERTa) — WeightedTrainer + Output Seragam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "becbce25",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "import evaluate\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    "\n",
    "accuracy_metric = evaluate.load(\"accuracy\")\n",
    "f1_metric = evaluate.load(\"f1\")\n",
    "\n",
    "DEFAULT_MAX_LEN = 256 if HAS_GPU else 128\n",
    "DEFAULT_BATCH = 8 if HAS_GPU else 2\n",
    "DEFAULT_EPOCHS = 1\n",
    "\n",
    "USE_SMALL_TRAIN_IF_CPU = True\n",
    "SMALL_TRAIN_N = 2000\n",
    "\n",
    "def make_hf_dataset(texts, labels):\n",
    "    return Dataset.from_dict({\"text\": list(texts), \"label\": [int(l) - 1 for l in labels]})\n",
    "\n",
    "def make_tokenize_fn(tokenizer, max_length):\n",
    "    def _tok(batch):\n",
    "        return tokenizer(batch[\"text\"], truncation=True, padding=\"max_length\", max_length=max_length)\n",
    "    return _tok\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    preds = np.argmax(logits, axis=-1)\n",
    "    acc = accuracy_metric.compute(predictions=preds, references=labels)[\"accuracy\"]\n",
    "    f1m = f1_metric.compute(predictions=preds, references=labels, average=\"macro\")[\"f1\"]\n",
    "    f1w = f1_metric.compute(predictions=preds, references=labels, average=\"weighted\")[\"f1\"]\n",
    "    return {\"accuracy\": acc, \"f1_macro\": f1m, \"f1_weighted\": f1w}\n",
    "\n",
    "def maybe_shrink_train(train_texts, train_labels):\n",
    "    if (not HAS_GPU) and USE_SMALL_TRAIN_IF_CPU and len(train_texts) > SMALL_TRAIN_N:\n",
    "        return train_texts[:SMALL_TRAIN_N], train_labels[:SMALL_TRAIN_N]\n",
    "    return train_texts, train_labels\n",
    "\n",
    "def build_training_args(out_dir, epochs, batch_size):\n",
    "    common = dict(\n",
    "        output_dir=out_dir,\n",
    "        save_strategy=\"no\",\n",
    "        learning_rate=2e-5,\n",
    "        per_device_train_batch_size=batch_size,\n",
    "        per_device_eval_batch_size=batch_size,\n",
    "        num_train_epochs=epochs,\n",
    "        weight_decay=0.01,\n",
    "        logging_steps=50,\n",
    "        report_to=\"none\",\n",
    "        seed=SEED,\n",
    "        fp16=HAS_GPU,\n",
    "        dataloader_num_workers=0,\n",
    "    )\n",
    "    try:\n",
    "        return TrainingArguments(**common, evaluation_strategy=\"epoch\")\n",
    "    except TypeError:\n",
    "        return TrainingArguments(**common, eval_strategy=\"epoch\")\n",
    "\n",
    "class WeightedTrainer(Trainer):\n",
    "    def __init__(self, class_weights=None, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.class_weights = class_weights\n",
    "\n",
    "    # NOTE: Transformers versi baru meneruskan argumen tambahan seperti\n",
    "    # num_items_in_batch ke compute_loss. Kita terima via **kwargs agar kompatibel.\n",
    "    def compute_loss(self, model, inputs, return_outputs=False, **kwargs):\n",
    "        inputs = dict(inputs)\n",
    "        labels = inputs.get(\"labels\")\n",
    "\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.get(\"logits\")\n",
    "\n",
    "        if self.class_weights is not None:\n",
    "            cw = self.class_weights.to(logits.device)\n",
    "            loss_fct = torch.nn.CrossEntropyLoss(weight=cw)\n",
    "        else:\n",
    "            loss_fct = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "        loss = loss_fct(logits.view(-1, model.config.num_labels), labels.view(-1))\n",
    "        return (loss, outputs) if return_outputs else loss\n",
    "\n",
    "def _build_trainer(model, args, train_tok, test_tok, tokenizer, use_class_weights, class_weights):\n",
    "    base_kwargs = dict(\n",
    "        model=model,\n",
    "        args=args,\n",
    "        train_dataset=train_tok,\n",
    "        eval_dataset=test_tok,\n",
    "        compute_metrics=compute_metrics,\n",
    "    )\n",
    "\n",
    "    if use_class_weights:\n",
    "        cls = WeightedTrainer\n",
    "        base_kwargs[\"class_weights\"] = class_weights\n",
    "    else:\n",
    "        cls = Trainer\n",
    "\n",
    "    # Kompatibilitas argumen tokenizer -> processing_class\n",
    "    try:\n",
    "        return cls(**base_kwargs, tokenizer=tokenizer)\n",
    "    except TypeError:\n",
    "        return cls(**base_kwargs, processing_class=tokenizer)\n",
    "\n",
    "def train_eval_transformer(\n",
    "    model_name,\n",
    "    train_texts, train_labels,\n",
    "    test_texts, test_labels,\n",
    "    epochs=DEFAULT_EPOCHS, batch_size=DEFAULT_BATCH,\n",
    "    max_length=DEFAULT_MAX_LEN, out_dir=\"./tmp_tr\", suffix=\"\",\n",
    "    use_class_weights=False\n",
    "):\n",
    "    train_texts2, train_labels2 = maybe_shrink_train(train_texts, train_labels)\n",
    "\n",
    "    train_hf = make_hf_dataset(train_texts2, train_labels2)\n",
    "    test_hf  = make_hf_dataset(test_texts, test_labels)\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=5)\n",
    "\n",
    "    tok_fn = make_tokenize_fn(tokenizer, max_length=max_length)\n",
    "    train_tok = train_hf.map(tok_fn, batched=True)\n",
    "    test_tok  = test_hf.map(tok_fn, batched=True)\n",
    "\n",
    "    # rename label column to labels for custom loss safety\n",
    "    train_tok = train_tok.remove_columns([\"text\"]).rename_column(\"label\", \"labels\")\n",
    "    test_tok  = test_tok.remove_columns([\"text\"]).rename_column(\"label\", \"labels\")\n",
    "\n",
    "    args = build_training_args(out_dir, epochs, batch_size)\n",
    "\n",
    "    class_weights = None\n",
    "    if use_class_weights:\n",
    "        classes = np.array([1,2,3,4,5])\n",
    "        cw = compute_class_weight(class_weight=\"balanced\", classes=classes, y=np.array(train_labels2))\n",
    "        class_weights = torch.tensor(cw, dtype=torch.float)\n",
    "\n",
    "    trainer = _build_trainer(\n",
    "        model=model,\n",
    "        args=args,\n",
    "        train_tok=train_tok,\n",
    "        test_tok=test_tok,\n",
    "        tokenizer=tokenizer,\n",
    "        use_class_weights=use_class_weights,\n",
    "        class_weights=class_weights\n",
    "    )\n",
    "\n",
    "    trainer.train()\n",
    "    res = trainer.evaluate()\n",
    "\n",
    "    out = {\n",
    "        \"model\": model_name + suffix,\n",
    "        \"accuracy\": res.get(\"eval_accuracy\"),\n",
    "        \"f1_macro\": res.get(\"eval_f1_macro\"),\n",
    "        \"f1_weighted\": res.get(\"eval_f1_weighted\"),\n",
    "        \"eval_loss\": res.get(\"eval_loss\"),\n",
    "        \"eval_runtime\": res.get(\"eval_runtime\"),\n",
    "        \"eval_samples_per_second\": res.get(\"eval_samples_per_second\"),\n",
    "        \"eval_steps_per_second\": res.get(\"eval_steps_per_second\"),\n",
    "        \"epoch\": res.get(\"epoch\"),\n",
    "    }\n",
    "\n",
    "    print(f\"=== {model_name}{suffix} ===\")\n",
    "    print(res)\n",
    "    print()\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d69ff40",
   "metadata": {},
   "source": [
    "## 10. Skenario 1 — Baseline (5 Model, No Handling)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "affe9062",
   "metadata": {},
   "source": [
    "### 10.1 Linear SVM (S1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "78a6f4c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Linear SVM (S1 baseline) ===\n",
      "Accuracy : 0.5\n",
      "F1-macro : 0.3758\n",
      "F1-weight: 0.4734\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1     0.5556    0.3125    0.4000        16\n",
      "           2     0.3333    0.0714    0.1176        14\n",
      "           3     0.3571    0.2857    0.3175        35\n",
      "           4     0.3966    0.3651    0.3802        63\n",
      "           5     0.5820    0.7717    0.6636        92\n",
      "\n",
      "    accuracy                         0.5000       220\n",
      "   macro avg     0.4449    0.3613    0.3758       220\n",
      "weighted avg     0.4754    0.5000    0.4734       220\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "results_s1 = []\n",
    "\n",
    "svm_s1_res, _, _, svm_s1_pred = train_eval_svm(\n",
    "    X_train, y_train, X_test, y_test,\n",
    "    suffix=\" (S1 baseline)\",\n",
    "    class_weight=None\n",
    ")\n",
    "results_s1.append(svm_s1_res)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f362225f",
   "metadata": {},
   "source": [
    "### 10.2 Naive Bayes (S1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "20edf946",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Naive Bayes (S1 baseline) ===\n",
      "Accuracy : 0.4364\n",
      "F1-macro : 0.1479\n",
      "F1-weight: 0.2912\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1     0.0000    0.0000    0.0000        16\n",
      "           2     0.0000    0.0000    0.0000        14\n",
      "           3     0.0000    0.0000    0.0000        35\n",
      "           4     0.5000    0.0794    0.1370        63\n",
      "           5     0.4333    0.9891    0.6026        92\n",
      "\n",
      "    accuracy                         0.4364       220\n",
      "   macro avg     0.1867    0.2137    0.1479       220\n",
      "weighted avg     0.3244    0.4364    0.2912       220\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "nb_s1_res, _, _, _ = train_eval_nb(\n",
    "    X_train, y_train, X_test, y_test,\n",
    "    suffix=\" (S1 baseline)\"\n",
    ")\n",
    "results_s1.append(nb_s1_res)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "702f7971",
   "metadata": {},
   "source": [
    "### 10.3 LSTM (S1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b2db7103",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LSTM (S1 baseline)] Epoch 1/3 | loss=1.5551\n",
      "[LSTM (S1 baseline)] Epoch 2/3 | loss=1.4000\n",
      "[LSTM (S1 baseline)] Epoch 3/3 | loss=1.3936\n",
      "=== LSTM (S1 baseline) ===\n",
      "Accuracy : 0.4182\n",
      "F1-macro : 0.1179\n",
      "F1-weight: 0.2466\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1     0.0000    0.0000    0.0000        16\n",
      "           2     0.0000    0.0000    0.0000        14\n",
      "           3     0.0000    0.0000    0.0000        35\n",
      "           4     0.0000    0.0000    0.0000        63\n",
      "           5     0.4182    1.0000    0.5897        92\n",
      "\n",
      "    accuracy                         0.4182       220\n",
      "   macro avg     0.0836    0.2000    0.1179       220\n",
      "weighted avg     0.1749    0.4182    0.2466       220\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "try:\n",
    "    lstm_s1_res = train_eval_lstm(\n",
    "        X_train, y_train, X_test, y_test,\n",
    "        epochs=3, suffix=\" (S1 baseline)\",\n",
    "        use_class_weights=False\n",
    "    )\n",
    "    results_s1.append(lstm_s1_res)\n",
    "except Exception as e:\n",
    "    print(\"LSTM S1 error:\", e)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5f20967",
   "metadata": {},
   "source": [
    "### 10.4 BERT (S1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c49d7fe0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f1d252db2ca460bbc127144ee8fb35e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/880 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "363a576c9e314bbbb3dc8394dec1e6ab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/220 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\LENOVO\\AppData\\Local\\Temp\\ipykernel_24684\\2636105220.py:96: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  return cls(**base_kwargs, tokenizer=tokenizer)\n",
      "c:\\Users\\LENOVO\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:668: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='440' max='440' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [440/440 14:07, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1 Macro</th>\n",
       "      <th>F1 Weighted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.179300</td>\n",
       "      <td>1.147911</td>\n",
       "      <td>0.486364</td>\n",
       "      <td>0.210667</td>\n",
       "      <td>0.400061</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\LENOVO\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:668: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='110' max='110' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [110/110 00:39]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== bert-base-uncased (S1 baseline) ===\n",
      "{'eval_loss': 1.1479114294052124, 'eval_accuracy': 0.4863636363636364, 'eval_f1_macro': 0.21066666666666664, 'eval_f1_weighted': 0.4000606060606061, 'eval_runtime': 40.4475, 'eval_samples_per_second': 5.439, 'eval_steps_per_second': 2.72, 'epoch': 1.0}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "try:\n",
    "    bert_s1_res = train_eval_transformer(\n",
    "        \"bert-base-uncased\",\n",
    "        X_train, y_train, X_test, y_test,\n",
    "        epochs=DEFAULT_EPOCHS,\n",
    "        batch_size=DEFAULT_BATCH,\n",
    "        max_length=DEFAULT_MAX_LEN,\n",
    "        out_dir=\"./tmp_bert_s1\",\n",
    "        suffix=\" (S1 baseline)\",\n",
    "        use_class_weights=False\n",
    "    )\n",
    "    results_s1.append(bert_s1_res)\n",
    "except Exception as e:\n",
    "    print(\"BERT S1 error:\", e)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aa8a12a",
   "metadata": {},
   "source": [
    "### 10.5 RoBERTa (S1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "48c2128d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d38e0a8e2224a438ad45ecc2f42082a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/880 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ec80ad05e8343e2b5e7bb1566f7dc29",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/220 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\LENOVO\\AppData\\Local\\Temp\\ipykernel_24684\\2636105220.py:96: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  return cls(**base_kwargs, tokenizer=tokenizer)\n",
      "c:\\Users\\LENOVO\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:668: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='440' max='440' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [440/440 10:06, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1 Macro</th>\n",
       "      <th>F1 Weighted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.220700</td>\n",
       "      <td>1.171993</td>\n",
       "      <td>0.490909</td>\n",
       "      <td>0.208292</td>\n",
       "      <td>0.396256</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\LENOVO\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:668: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='110' max='110' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [110/110 00:23]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== roberta-base (S1 baseline) ===\n",
      "{'eval_loss': 1.1719926595687866, 'eval_accuracy': 0.4909090909090909, 'eval_f1_macro': 0.2082924168030551, 'eval_f1_weighted': 0.3962555175321133, 'eval_runtime': 24.1666, 'eval_samples_per_second': 9.103, 'eval_steps_per_second': 4.552, 'epoch': 1.0}\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>f1_macro</th>\n",
       "      <th>f1_weighted</th>\n",
       "      <th>eval_loss</th>\n",
       "      <th>eval_runtime</th>\n",
       "      <th>eval_samples_per_second</th>\n",
       "      <th>eval_steps_per_second</th>\n",
       "      <th>epoch</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Linear SVM (S1 baseline)</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.375765</td>\n",
       "      <td>0.473433</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>bert-base-uncased (S1 baseline)</td>\n",
       "      <td>0.486364</td>\n",
       "      <td>0.210667</td>\n",
       "      <td>0.400061</td>\n",
       "      <td>1.147911</td>\n",
       "      <td>40.4475</td>\n",
       "      <td>5.439</td>\n",
       "      <td>2.720</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>roberta-base (S1 baseline)</td>\n",
       "      <td>0.490909</td>\n",
       "      <td>0.208292</td>\n",
       "      <td>0.396256</td>\n",
       "      <td>1.171993</td>\n",
       "      <td>24.1666</td>\n",
       "      <td>9.103</td>\n",
       "      <td>4.552</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Naive Bayes (S1 baseline)</td>\n",
       "      <td>0.436364</td>\n",
       "      <td>0.147927</td>\n",
       "      <td>0.291245</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>LSTM (S1 baseline)</td>\n",
       "      <td>0.418182</td>\n",
       "      <td>0.117949</td>\n",
       "      <td>0.246620</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             model  accuracy  f1_macro  f1_weighted  \\\n",
       "0         Linear SVM (S1 baseline)  0.500000  0.375765     0.473433   \n",
       "3  bert-base-uncased (S1 baseline)  0.486364  0.210667     0.400061   \n",
       "4       roberta-base (S1 baseline)  0.490909  0.208292     0.396256   \n",
       "1        Naive Bayes (S1 baseline)  0.436364  0.147927     0.291245   \n",
       "2               LSTM (S1 baseline)  0.418182  0.117949     0.246620   \n",
       "\n",
       "   eval_loss  eval_runtime  eval_samples_per_second  eval_steps_per_second  \\\n",
       "0        NaN           NaN                      NaN                    NaN   \n",
       "3   1.147911       40.4475                    5.439                  2.720   \n",
       "4   1.171993       24.1666                    9.103                  4.552   \n",
       "1        NaN           NaN                      NaN                    NaN   \n",
       "2        NaN           NaN                      NaN                    NaN   \n",
       "\n",
       "   epoch  \n",
       "0    NaN  \n",
       "3    1.0  \n",
       "4    1.0  \n",
       "1    NaN  \n",
       "2    NaN  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "try:\n",
    "    roberta_s1_res = train_eval_transformer(\n",
    "        \"roberta-base\",\n",
    "        X_train, y_train, X_test, y_test,\n",
    "        epochs=DEFAULT_EPOCHS,\n",
    "        batch_size=DEFAULT_BATCH,\n",
    "        max_length=DEFAULT_MAX_LEN,\n",
    "        out_dir=\"./tmp_roberta_s1\",\n",
    "        suffix=\" (S1 baseline)\",\n",
    "        use_class_weights=False\n",
    "    )\n",
    "    results_s1.append(roberta_s1_res)\n",
    "except Exception as e:\n",
    "    print(\"RoBERTa S1 error:\", e)\n",
    "\n",
    "s1_table = pd.DataFrame(results_s1)\n",
    "s1_table.sort_values(\"f1_macro\", ascending=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acea9da3",
   "metadata": {},
   "source": [
    "## 11. Skenario 2 — Balancing **Tanpa** Augmentasi Teks (5 Model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e806963",
   "metadata": {},
   "source": [
    "### 11.1 Linear SVM (S2 — class_weight balanced)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5c27642c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Linear SVM (S2 balanced no-aug) ===\n",
      "Accuracy : 0.4909\n",
      "F1-macro : 0.384\n",
      "F1-weight: 0.4769\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1     0.4545    0.3125    0.3704        16\n",
      "           2     0.2500    0.1429    0.1818        14\n",
      "           3     0.3333    0.3429    0.3380        35\n",
      "           4     0.3684    0.3333    0.3500        63\n",
      "           5     0.6296    0.7391    0.6800        92\n",
      "\n",
      "    accuracy                         0.4909       220\n",
      "   macro avg     0.4072    0.3741    0.3840       220\n",
      "weighted avg     0.4708    0.4909    0.4769       220\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "results_s2 = []\n",
    "\n",
    "svm_s2_res, _, _, _ = train_eval_svm(\n",
    "    X_train, y_train, X_test, y_test,\n",
    "    suffix=\" (S2 balanced no-aug)\",\n",
    "    class_weight=\"balanced\"\n",
    ")\n",
    "results_s2.append(svm_s2_res)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "641e244e",
   "metadata": {},
   "source": [
    "### 11.2 Naive Bayes (S2 — duplicate oversampling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2fe2a47a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Naive Bayes (S2 balanced no-aug) ===\n",
      "Accuracy : 0.4227\n",
      "F1-macro : 0.3703\n",
      "F1-weight: 0.444\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1     0.3529    0.3750    0.3636        16\n",
      "           2     0.1395    0.4286    0.2105        14\n",
      "           3     0.3023    0.3714    0.3333        35\n",
      "           4     0.3913    0.2857    0.3303        63\n",
      "           5     0.7042    0.5435    0.6135        92\n",
      "\n",
      "    accuracy                         0.4227       220\n",
      "   macro avg     0.3781    0.4008    0.3703       220\n",
      "weighted avg     0.4892    0.4227    0.4440       220\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "Xt_dup, yt_dup = duplicate_oversample(X_train, y_train)\n",
    "\n",
    "nb_s2_res, _, _, _ = train_eval_nb(\n",
    "    Xt_dup, yt_dup, X_test, y_test,\n",
    "    suffix=\" (S2 balanced no-aug)\"\n",
    ")\n",
    "results_s2.append(nb_s2_res)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31423483",
   "metadata": {},
   "source": [
    "### 11.3 LSTM (S2 — weighted loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "de83a7c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LSTM (S2 balanced no-aug)] Epoch 1/3 | loss=1.6113\n",
      "[LSTM (S2 balanced no-aug)] Epoch 2/3 | loss=1.6106\n",
      "[LSTM (S2 balanced no-aug)] Epoch 3/3 | loss=1.6103\n",
      "=== LSTM (S2 balanced no-aug) ===\n",
      "Accuracy : 0.2864\n",
      "F1-macro : 0.089\n",
      "F1-weight: 0.1275\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1     0.0000    0.0000    0.0000        16\n",
      "           2     0.0000    0.0000    0.0000        14\n",
      "           3     0.0000    0.0000    0.0000        35\n",
      "           4     0.2864    1.0000    0.4452        63\n",
      "           5     0.0000    0.0000    0.0000        92\n",
      "\n",
      "    accuracy                         0.2864       220\n",
      "   macro avg     0.0573    0.2000    0.0890       220\n",
      "weighted avg     0.0820    0.2864    0.1275       220\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "try:\n",
    "    lstm_s2_res = train_eval_lstm(\n",
    "        X_train, y_train, X_test, y_test,\n",
    "        epochs=3, suffix=\" (S2 balanced no-aug)\",\n",
    "        use_class_weights=True\n",
    "    )\n",
    "    results_s2.append(lstm_s2_res)\n",
    "except Exception as e:\n",
    "    print(\"LSTM S2 error:\", e)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d0da80e",
   "metadata": {},
   "source": [
    "### 11.4 BERT (S2 — weighted loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "610613c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "75ec40acc4b34b878cffd4533c75730e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/880 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "764f69e6484a437095d0c253a24c60d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/220 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\LENOVO\\AppData\\Local\\Temp\\ipykernel_24684\\2636105220.py:58: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `WeightedTrainer.__init__`. Use `processing_class` instead.\n",
      "  super().__init__(*args, **kwargs)\n",
      "c:\\Users\\LENOVO\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:668: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='440' max='440' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [440/440 09:08, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1 Macro</th>\n",
       "      <th>F1 Weighted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.259200</td>\n",
       "      <td>1.234063</td>\n",
       "      <td>0.559091</td>\n",
       "      <td>0.373691</td>\n",
       "      <td>0.510886</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\LENOVO\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:668: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='110' max='110' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [110/110 00:28]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== bert-base-uncased (S2 balanced no-aug) ===\n",
      "{'eval_loss': 1.234062910079956, 'eval_accuracy': 0.5590909090909091, 'eval_f1_macro': 0.37369107547222463, 'eval_f1_weighted': 0.5108859004215062, 'eval_runtime': 29.1339, 'eval_samples_per_second': 7.551, 'eval_steps_per_second': 3.776, 'epoch': 1.0}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "try:\n",
    "    bert_s2_res = train_eval_transformer(\n",
    "        \"bert-base-uncased\",\n",
    "        X_train, y_train, X_test, y_test,\n",
    "        epochs=DEFAULT_EPOCHS,\n",
    "        batch_size=DEFAULT_BATCH,\n",
    "        max_length=DEFAULT_MAX_LEN,\n",
    "        out_dir=\"./tmp_bert_s2\",\n",
    "        suffix=\" (S2 balanced no-aug)\",\n",
    "        use_class_weights=True\n",
    "    )\n",
    "    results_s2.append(bert_s2_res)\n",
    "except Exception as e:\n",
    "    print(\"BERT S2 error:\", e)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "548d20f6",
   "metadata": {},
   "source": [
    "### 11.5 RoBERTa (S2 — weighted loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4b175473",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fea3d04fc59b4edbade301cadccc5c7d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/880 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3aa2b34b5bf34623928b0fd3563f6b93",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/220 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\LENOVO\\AppData\\Local\\Temp\\ipykernel_24684\\2636105220.py:58: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `WeightedTrainer.__init__`. Use `processing_class` instead.\n",
      "  super().__init__(*args, **kwargs)\n",
      "c:\\Users\\LENOVO\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:668: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='440' max='440' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [440/440 08:09, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1 Macro</th>\n",
       "      <th>F1 Weighted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.458300</td>\n",
       "      <td>1.426181</td>\n",
       "      <td>0.445455</td>\n",
       "      <td>0.203278</td>\n",
       "      <td>0.376389</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\LENOVO\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:668: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='110' max='110' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [110/110 00:21]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== roberta-base (S2 balanced no-aug) ===\n",
      "{'eval_loss': 1.4261813163757324, 'eval_accuracy': 0.44545454545454544, 'eval_f1_macro': 0.20327812284334024, 'eval_f1_weighted': 0.3763888888888889, 'eval_runtime': 21.6329, 'eval_samples_per_second': 10.17, 'eval_steps_per_second': 5.085, 'epoch': 1.0}\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>f1_macro</th>\n",
       "      <th>f1_weighted</th>\n",
       "      <th>eval_loss</th>\n",
       "      <th>eval_runtime</th>\n",
       "      <th>eval_samples_per_second</th>\n",
       "      <th>eval_steps_per_second</th>\n",
       "      <th>epoch</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Linear SVM (S2 balanced no-aug)</td>\n",
       "      <td>0.490909</td>\n",
       "      <td>0.384043</td>\n",
       "      <td>0.476874</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>bert-base-uncased (S2 balanced no-aug)</td>\n",
       "      <td>0.559091</td>\n",
       "      <td>0.373691</td>\n",
       "      <td>0.510886</td>\n",
       "      <td>1.234063</td>\n",
       "      <td>29.1339</td>\n",
       "      <td>7.551</td>\n",
       "      <td>3.776</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Naive Bayes (S2 balanced no-aug)</td>\n",
       "      <td>0.422727</td>\n",
       "      <td>0.370254</td>\n",
       "      <td>0.444006</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>roberta-base (S2 balanced no-aug)</td>\n",
       "      <td>0.445455</td>\n",
       "      <td>0.203278</td>\n",
       "      <td>0.376389</td>\n",
       "      <td>1.426181</td>\n",
       "      <td>21.6329</td>\n",
       "      <td>10.170</td>\n",
       "      <td>5.085</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>LSTM (S2 balanced no-aug)</td>\n",
       "      <td>0.286364</td>\n",
       "      <td>0.089046</td>\n",
       "      <td>0.127498</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    model  accuracy  f1_macro  f1_weighted  \\\n",
       "0         Linear SVM (S2 balanced no-aug)  0.490909  0.384043     0.476874   \n",
       "3  bert-base-uncased (S2 balanced no-aug)  0.559091  0.373691     0.510886   \n",
       "1        Naive Bayes (S2 balanced no-aug)  0.422727  0.370254     0.444006   \n",
       "4       roberta-base (S2 balanced no-aug)  0.445455  0.203278     0.376389   \n",
       "2               LSTM (S2 balanced no-aug)  0.286364  0.089046     0.127498   \n",
       "\n",
       "   eval_loss  eval_runtime  eval_samples_per_second  eval_steps_per_second  \\\n",
       "0        NaN           NaN                      NaN                    NaN   \n",
       "3   1.234063       29.1339                    7.551                  3.776   \n",
       "1        NaN           NaN                      NaN                    NaN   \n",
       "4   1.426181       21.6329                   10.170                  5.085   \n",
       "2        NaN           NaN                      NaN                    NaN   \n",
       "\n",
       "   epoch  \n",
       "0    NaN  \n",
       "3    1.0  \n",
       "1    NaN  \n",
       "4    1.0  \n",
       "2    NaN  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "try:\n",
    "    roberta_s2_res = train_eval_transformer(\n",
    "        \"roberta-base\",\n",
    "        X_train, y_train, X_test, y_test,\n",
    "        epochs=DEFAULT_EPOCHS,\n",
    "        batch_size=DEFAULT_BATCH,\n",
    "        max_length=DEFAULT_MAX_LEN,\n",
    "        out_dir=\"./tmp_roberta_s2\",\n",
    "        suffix=\" (S2 balanced no-aug)\",\n",
    "        use_class_weights=True\n",
    "    )\n",
    "    results_s2.append(roberta_s2_res)\n",
    "except Exception as e:\n",
    "    print(\"RoBERTa S2 error:\", e)\n",
    "\n",
    "s2_table = pd.DataFrame(results_s2)\n",
    "s2_table.sort_values(\"f1_macro\", ascending=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32a92953",
   "metadata": {},
   "source": [
    "## 12. Skenario 3 — Balancing **Dengan** Augmentasi Teks (5 Model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa2b6b72",
   "metadata": {},
   "source": [
    "### 12.1 Implementasi Augmentasi (EDA family + optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "68706424",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# EDA-style augmentation helpers\n",
    "def get_synonyms(word):\n",
    "    syns = set()\n",
    "    for syn in wordnet.synsets(word):\n",
    "        for lemma in syn.lemmas():\n",
    "            w = lemma.name().replace(\"_\", \" \").lower()\n",
    "            if w != word.lower():\n",
    "                syns.add(w)\n",
    "    return list(syns)\n",
    "\n",
    "def random_deletion(words, p=0.1):\n",
    "    if len(words) == 1:\n",
    "        return words\n",
    "    new_words = [w for w in words if random.random() > p]\n",
    "    return new_words if new_words else [random.choice(words)]\n",
    "\n",
    "def random_swap(words, n=1):\n",
    "    new_words = words[:]\n",
    "    for _ in range(n):\n",
    "        if len(new_words) < 2:\n",
    "            break\n",
    "        i, j = random.sample(range(len(new_words)), 2)\n",
    "        new_words[i], new_words[j] = new_words[j], new_words[i]\n",
    "    return new_words\n",
    "\n",
    "def random_insertion(words, n=1):\n",
    "    new_words = words[:]\n",
    "    for _ in range(n):\n",
    "        w = random.choice(new_words)\n",
    "        syns = get_synonyms(w)\n",
    "        if not syns:\n",
    "            continue\n",
    "        new_words.insert(random.randint(0, len(new_words)), random.choice(syns))\n",
    "    return new_words\n",
    "\n",
    "def synonym_replacement(words, n=1):\n",
    "    new_words = words[:]\n",
    "    candidates = [w for w in set(words) if get_synonyms(w)]\n",
    "    random.shuffle(candidates)\n",
    "    num_replaced = 0\n",
    "    for w in candidates:\n",
    "        syns = get_synonyms(w)\n",
    "        if not syns:\n",
    "            continue\n",
    "        new_words = [random.choice(syns) if x == w else x for x in new_words]\n",
    "        num_replaced += 1\n",
    "        if num_replaced >= n:\n",
    "            break\n",
    "    return new_words\n",
    "\n",
    "def eda(text, alpha=0.1, num_aug=1):\n",
    "    words = str(text).split()\n",
    "    n = max(1, int(alpha * len(words)))\n",
    "    augmented = []\n",
    "    for _ in range(num_aug):\n",
    "        choice = random.choice([\"sr\", \"ri\", \"rs\", \"rd\"])\n",
    "        if choice == \"sr\":\n",
    "            w = synonym_replacement(words, n)\n",
    "        elif choice == \"ri\":\n",
    "            w = random_insertion(words, n)\n",
    "        elif choice == \"rs\":\n",
    "            w = random_swap(words, n)\n",
    "        else:\n",
    "            w = random_deletion(words, p=alpha)\n",
    "        augmented.append(\" \".join(w))\n",
    "    return augmented\n",
    "\n",
    "def modified_eda(text, alpha=0.05, num_aug=1):\n",
    "    return eda(text, alpha=alpha, num_aug=num_aug)\n",
    "\n",
    "# Optional methods\n",
    "def backtranslate(text, src_lang=\"en\", mid_lang=\"fr\"):\n",
    "    try:\n",
    "        from transformers import MarianMTModel, MarianTokenizer\n",
    "        model_name_1 = f\"Helsinki-NLP/opus-mt-{src_lang}-{mid_lang}\"\n",
    "        model_name_2 = f\"Helsinki-NLP/opus-mt-{mid_lang}-{src_lang}\"\n",
    "        tok1 = MarianTokenizer.from_pretrained(model_name_1)\n",
    "        mod1 = MarianMTModel.from_pretrained(model_name_1)\n",
    "        tok2 = MarianTokenizer.from_pretrained(model_name_2)\n",
    "        mod2 = MarianMTModel.from_pretrained(model_name_2)\n",
    "\n",
    "        def translate(t, tok, mod):\n",
    "            batch = tok([t], return_tensors=\"pt\", truncation=True, padding=True)\n",
    "            gen = mod.generate(**batch, max_length=256)\n",
    "            return tok.batch_decode(gen, skip_special_tokens=True)[0]\n",
    "\n",
    "        mid = translate(text, tok1, mod1)\n",
    "        back = translate(mid, tok2, mod2)\n",
    "        return back\n",
    "    except Exception:\n",
    "        return text\n",
    "\n",
    "def bert_augment(text, num_aug=1):\n",
    "    try:\n",
    "        import nlpaug.augmenter.word as naw\n",
    "        aug = naw.ContextualWordEmbsAug(model_path=\"bert-base-uncased\", action=\"substitute\")\n",
    "        return aug.augment(text, n=num_aug)\n",
    "    except Exception:\n",
    "        return [text] * num_aug\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ebaffa0",
   "metadata": {},
   "source": [
    "### 12.2 Augment Minority Helper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3b2c8950",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distribusi train original: Counter({np.int64(5): 368, np.int64(4): 253, np.int64(3): 137, np.int64(1): 64, np.int64(2): 58})\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def augment_minority(\n",
    "    texts, labels,\n",
    "    strategy=\"eda\",\n",
    "    target_multiplier=1.0,\n",
    "    max_aug_per_sample=2\n",
    "):\n",
    "    texts = list(texts)\n",
    "    labels = list(labels)\n",
    "\n",
    "    counts = Counter(labels)\n",
    "    max_count = max(counts.values())\n",
    "    target_count = int(max_count * target_multiplier)\n",
    "\n",
    "    new_texts = texts[:]\n",
    "    new_labels = labels[:]\n",
    "\n",
    "    idx_by_class = {c: [] for c in counts}\n",
    "    for i, lab in enumerate(labels):\n",
    "        idx_by_class[lab].append(i)\n",
    "\n",
    "    for c, idxs in idx_by_class.items():\n",
    "        need = max(0, target_count - counts[c])\n",
    "        if need == 0:\n",
    "            continue\n",
    "\n",
    "        pool = idxs[:]\n",
    "        random.shuffle(pool)\n",
    "        ptr = 0\n",
    "\n",
    "        while need > 0 and pool:\n",
    "            i = pool[ptr % len(pool)]\n",
    "            base = texts[i]\n",
    "            k = min(max_aug_per_sample, need)\n",
    "\n",
    "            if strategy == \"eda\":\n",
    "                aug_texts = eda(base, alpha=0.1, num_aug=k)\n",
    "            elif strategy == \"modified_eda\":\n",
    "                aug_texts = modified_eda(base, alpha=0.05, num_aug=k)\n",
    "            elif strategy == \"backtranslation\":\n",
    "                aug_texts = [backtranslate(base) for _ in range(k)]\n",
    "            elif strategy == \"bert\":\n",
    "                aug_texts = bert_augment(base, num_aug=k)\n",
    "            else:\n",
    "                aug_texts = []\n",
    "\n",
    "            for t in aug_texts:\n",
    "                if need <= 0:\n",
    "                    break\n",
    "                new_texts.append(t)\n",
    "                new_labels.append(c)\n",
    "                need -= 1\n",
    "\n",
    "            ptr += 1\n",
    "\n",
    "    return np.array(new_texts), np.array(new_labels)\n",
    "\n",
    "print(\"Distribusi train original:\", Counter(y_train))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3906f73",
   "metadata": {},
   "source": [
    "### 12.3 Pilih Strategi Augmentasi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b50fa2c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "AUG_STRATEGIES = [\"eda\", \"modified_eda\", \"backtranslation\", \"bert\"]  # tambah \"backtranslation\", \"bert\" jika ingin\n",
    "TARGET_MULTIPLIER = 1.0\n",
    "MAX_AUG_PER_SAMPLE = 2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b30c050",
   "metadata": {},
   "source": [
    "### 12.4 Runner S3 — Blok per Model dalam Loop Strategi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "22c77ea8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "S3 Strategy: eda\n",
      "Distribusi train after aug: Counter({np.int64(4): 368, np.int64(5): 368, np.int64(2): 368, np.int64(3): 368, np.int64(1): 368})\n",
      "=== Linear SVM (S3 aug=eda) ===\n",
      "Accuracy : 0.4545\n",
      "F1-macro : 0.3601\n",
      "F1-weight: 0.4484\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1     0.4167    0.3125    0.3571        16\n",
      "           2     0.1538    0.1429    0.1481        14\n",
      "           3     0.3077    0.3429    0.3243        35\n",
      "           4     0.3636    0.3175    0.3390        63\n",
      "           5     0.6040    0.6630    0.6321        92\n",
      "\n",
      "    accuracy                         0.4545       220\n",
      "   macro avg     0.3692    0.3557    0.3601       220\n",
      "weighted avg     0.4457    0.4545    0.4484       220\n",
      "\n",
      "\n",
      "=== Naive Bayes (S3 aug=eda) ===\n",
      "Accuracy : 0.4682\n",
      "F1-macro : 0.4024\n",
      "F1-weight: 0.4787\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1     0.5000    0.3750    0.4286        16\n",
      "           2     0.1714    0.4286    0.2449        14\n",
      "           3     0.2927    0.3429    0.3158        35\n",
      "           4     0.4130    0.3016    0.3486        63\n",
      "           5     0.6977    0.6522    0.6742        92\n",
      "\n",
      "    accuracy                         0.4682       220\n",
      "   macro avg     0.4150    0.4200    0.4024       220\n",
      "weighted avg     0.5039    0.4682    0.4787       220\n",
      "\n",
      "\n",
      "[LSTM (S3 aug=eda)] Epoch 1/3 | loss=1.6117\n",
      "[LSTM (S3 aug=eda)] Epoch 2/3 | loss=1.6100\n",
      "[LSTM (S3 aug=eda)] Epoch 3/3 | loss=1.6097\n",
      "=== LSTM (S3 aug=eda) ===\n",
      "Accuracy : 0.0636\n",
      "F1-macro : 0.0239\n",
      "F1-weight: 0.0076\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1     0.0000    0.0000    0.0000        16\n",
      "           2     0.0636    1.0000    0.1197        14\n",
      "           3     0.0000    0.0000    0.0000        35\n",
      "           4     0.0000    0.0000    0.0000        63\n",
      "           5     0.0000    0.0000    0.0000        92\n",
      "\n",
      "    accuracy                         0.0636       220\n",
      "   macro avg     0.0127    0.2000    0.0239       220\n",
      "weighted avg     0.0040    0.0636    0.0076       220\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "39493697e19d4cc2a6b6b0881110d912",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1840 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "38ddc6edfdde40a5b5aa92797230458e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/220 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\LENOVO\\AppData\\Local\\Temp\\ipykernel_24684\\2636105220.py:58: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `WeightedTrainer.__init__`. Use `processing_class` instead.\n",
      "  super().__init__(*args, **kwargs)\n",
      "c:\\Users\\LENOVO\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:668: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='920' max='920' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [920/920 16:02, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1 Macro</th>\n",
       "      <th>F1 Weighted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.339000</td>\n",
       "      <td>1.181916</td>\n",
       "      <td>0.486364</td>\n",
       "      <td>0.337168</td>\n",
       "      <td>0.460908</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\LENOVO\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:668: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='110' max='110' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [110/110 00:22]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== bert-base-uncased (S3 aug=eda) ===\n",
      "{'eval_loss': 1.1819164752960205, 'eval_accuracy': 0.4863636363636364, 'eval_f1_macro': 0.33716783683989976, 'eval_f1_weighted': 0.4609075823703483, 'eval_runtime': 22.7037, 'eval_samples_per_second': 9.69, 'eval_steps_per_second': 4.845, 'epoch': 1.0}\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3405936530c54d11b1c49e2eb0d95dc8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1840 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b3c89f16887b44bdae3ce080ea350244",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/220 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\LENOVO\\AppData\\Local\\Temp\\ipykernel_24684\\2636105220.py:58: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `WeightedTrainer.__init__`. Use `processing_class` instead.\n",
      "  super().__init__(*args, **kwargs)\n",
      "c:\\Users\\LENOVO\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:668: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='920' max='920' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [920/920 16:12, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1 Macro</th>\n",
       "      <th>F1 Weighted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.133200</td>\n",
       "      <td>1.151978</td>\n",
       "      <td>0.454545</td>\n",
       "      <td>0.404059</td>\n",
       "      <td>0.466966</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\LENOVO\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:668: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='110' max='110' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [110/110 00:22]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== roberta-base (S3 aug=eda) ===\n",
      "{'eval_loss': 1.1519784927368164, 'eval_accuracy': 0.45454545454545453, 'eval_f1_macro': 0.4040594756892436, 'eval_f1_weighted': 0.4669662554925292, 'eval_runtime': 22.6491, 'eval_samples_per_second': 9.713, 'eval_steps_per_second': 4.857, 'epoch': 1.0}\n",
      "\n",
      "\n",
      "================================================================================\n",
      "S3 Strategy: modified_eda\n",
      "Distribusi train after aug: Counter({np.int64(4): 368, np.int64(5): 368, np.int64(2): 368, np.int64(3): 368, np.int64(1): 368})\n",
      "=== Linear SVM (S3 aug=modified_eda) ===\n",
      "Accuracy : 0.4636\n",
      "F1-macro : 0.3617\n",
      "F1-weight: 0.4518\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1     0.4167    0.3125    0.3571        16\n",
      "           2     0.2000    0.1429    0.1667        14\n",
      "           3     0.3030    0.2857    0.2941        35\n",
      "           4     0.3621    0.3333    0.3471        63\n",
      "           5     0.5981    0.6957    0.6432        92\n",
      "\n",
      "    accuracy                         0.4636       220\n",
      "   macro avg     0.3760    0.3540    0.3617       220\n",
      "weighted avg     0.4451    0.4636    0.4518       220\n",
      "\n",
      "\n",
      "=== Naive Bayes (S3 aug=modified_eda) ===\n",
      "Accuracy : 0.4545\n",
      "F1-macro : 0.4091\n",
      "F1-weight: 0.4656\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1     0.4706    0.5000    0.4848        16\n",
      "           2     0.1750    0.5000    0.2593        14\n",
      "           3     0.3333    0.3714    0.3514        35\n",
      "           4     0.3636    0.2540    0.2991        63\n",
      "           5     0.7000    0.6087    0.6512        92\n",
      "\n",
      "    accuracy                         0.4545       220\n",
      "   macro avg     0.4085    0.4468    0.4091       220\n",
      "weighted avg     0.4953    0.4545    0.4656       220\n",
      "\n",
      "\n",
      "[LSTM (S3 aug=modified_eda)] Epoch 1/3 | loss=1.6117\n",
      "[LSTM (S3 aug=modified_eda)] Epoch 2/3 | loss=1.6102\n",
      "[LSTM (S3 aug=modified_eda)] Epoch 3/3 | loss=1.6103\n",
      "=== LSTM (S3 aug=modified_eda) ===\n",
      "Accuracy : 0.0636\n",
      "F1-macro : 0.0239\n",
      "F1-weight: 0.0076\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1     0.0000    0.0000    0.0000        16\n",
      "           2     0.0636    1.0000    0.1197        14\n",
      "           3     0.0000    0.0000    0.0000        35\n",
      "           4     0.0000    0.0000    0.0000        63\n",
      "           5     0.0000    0.0000    0.0000        92\n",
      "\n",
      "    accuracy                         0.0636       220\n",
      "   macro avg     0.0127    0.2000    0.0239       220\n",
      "weighted avg     0.0040    0.0636    0.0076       220\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd87ea83735346e8a2072cc673636721",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1840 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc11fdfc5e7f462e8cc154fecb8cf960",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/220 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\LENOVO\\AppData\\Local\\Temp\\ipykernel_24684\\2636105220.py:58: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `WeightedTrainer.__init__`. Use `processing_class` instead.\n",
      "  super().__init__(*args, **kwargs)\n",
      "c:\\Users\\LENOVO\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:668: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='920' max='920' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [920/920 15:25, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1 Macro</th>\n",
       "      <th>F1 Weighted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.957800</td>\n",
       "      <td>1.138444</td>\n",
       "      <td>0.522727</td>\n",
       "      <td>0.434754</td>\n",
       "      <td>0.524214</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\LENOVO\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:668: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='110' max='110' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [110/110 00:22]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== bert-base-uncased (S3 aug=modified_eda) ===\n",
      "{'eval_loss': 1.138444185256958, 'eval_accuracy': 0.5227272727272727, 'eval_f1_macro': 0.4347544085458005, 'eval_f1_weighted': 0.5242142047705061, 'eval_runtime': 22.8031, 'eval_samples_per_second': 9.648, 'eval_steps_per_second': 4.824, 'epoch': 1.0}\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ada3e661a554db69c6bc707399ab6b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1840 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c0dec5bee24b434a88e12a67942a8197",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/220 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\LENOVO\\AppData\\Local\\Temp\\ipykernel_24684\\2636105220.py:58: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `WeightedTrainer.__init__`. Use `processing_class` instead.\n",
      "  super().__init__(*args, **kwargs)\n",
      "c:\\Users\\LENOVO\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:668: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='920' max='920' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [920/920 16:13, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1 Macro</th>\n",
       "      <th>F1 Weighted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.262100</td>\n",
       "      <td>1.209656</td>\n",
       "      <td>0.486364</td>\n",
       "      <td>0.416236</td>\n",
       "      <td>0.492973</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\LENOVO\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:668: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='110' max='110' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [110/110 00:22]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== roberta-base (S3 aug=modified_eda) ===\n",
      "{'eval_loss': 1.2096561193466187, 'eval_accuracy': 0.4863636363636364, 'eval_f1_macro': 0.41623637825665194, 'eval_f1_weighted': 0.49297284234307026, 'eval_runtime': 22.6716, 'eval_samples_per_second': 9.704, 'eval_steps_per_second': 4.852, 'epoch': 1.0}\n",
      "\n",
      "\n",
      "================================================================================\n",
      "S3 Strategy: backtranslation\n",
      "Distribusi train after aug: Counter({np.int64(4): 368, np.int64(5): 368, np.int64(2): 368, np.int64(3): 368, np.int64(1): 368})\n",
      "=== Linear SVM (S3 aug=backtranslation) ===\n",
      "Accuracy : 0.4682\n",
      "F1-macro : 0.3735\n",
      "F1-weight: 0.4562\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1     0.5000    0.3125    0.3846        16\n",
      "           2     0.2000    0.1429    0.1667        14\n",
      "           3     0.3429    0.3429    0.3429        35\n",
      "           4     0.3509    0.3175    0.3333        63\n",
      "           5     0.5926    0.6957    0.6400        92\n",
      "\n",
      "    accuracy                         0.4682       220\n",
      "   macro avg     0.3973    0.3623    0.3735       220\n",
      "weighted avg     0.4519    0.4682    0.4562       220\n",
      "\n",
      "\n",
      "=== Naive Bayes (S3 aug=backtranslation) ===\n",
      "Accuracy : 0.3955\n",
      "F1-macro : 0.348\n",
      "F1-weight: 0.4138\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1     0.3750    0.3750    0.3750        16\n",
      "           2     0.1500    0.4286    0.2222        14\n",
      "           3     0.2292    0.3143    0.2651        35\n",
      "           4     0.3409    0.2381    0.2804        63\n",
      "           5     0.6806    0.5326    0.5976        92\n",
      "\n",
      "    accuracy                         0.3955       220\n",
      "   macro avg     0.3551    0.3777    0.3480       220\n",
      "weighted avg     0.4555    0.3955    0.4138       220\n",
      "\n",
      "\n",
      "[LSTM (S3 aug=backtranslation)] Epoch 1/3 | loss=1.6107\n",
      "[LSTM (S3 aug=backtranslation)] Epoch 2/3 | loss=1.6102\n",
      "[LSTM (S3 aug=backtranslation)] Epoch 3/3 | loss=1.6104\n",
      "=== LSTM (S3 aug=backtranslation) ===\n",
      "Accuracy : 0.0727\n",
      "F1-macro : 0.0271\n",
      "F1-weight: 0.0099\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1     0.0727    1.0000    0.1356        16\n",
      "           2     0.0000    0.0000    0.0000        14\n",
      "           3     0.0000    0.0000    0.0000        35\n",
      "           4     0.0000    0.0000    0.0000        63\n",
      "           5     0.0000    0.0000    0.0000        92\n",
      "\n",
      "    accuracy                         0.0727       220\n",
      "   macro avg     0.0145    0.2000    0.0271       220\n",
      "weighted avg     0.0053    0.0727    0.0099       220\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc453b854d414704b46a15bbeb00f511",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1840 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d1a3e74bffd641c8b958f1084edccb77",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/220 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\LENOVO\\AppData\\Local\\Temp\\ipykernel_24684\\2636105220.py:58: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `WeightedTrainer.__init__`. Use `processing_class` instead.\n",
      "  super().__init__(*args, **kwargs)\n",
      "c:\\Users\\LENOVO\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:668: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='920' max='920' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [920/920 16:53, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1 Macro</th>\n",
       "      <th>F1 Weighted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.982900</td>\n",
       "      <td>1.128783</td>\n",
       "      <td>0.490909</td>\n",
       "      <td>0.404342</td>\n",
       "      <td>0.486915</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\LENOVO\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:668: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='110' max='110' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [110/110 00:22]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== bert-base-uncased (S3 aug=backtranslation) ===\n",
      "{'eval_loss': 1.1287832260131836, 'eval_accuracy': 0.4909090909090909, 'eval_f1_macro': 0.40434152047320976, 'eval_f1_weighted': 0.486914645674216, 'eval_runtime': 23.3152, 'eval_samples_per_second': 9.436, 'eval_steps_per_second': 4.718, 'epoch': 1.0}\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f90dc9c9a8545e390ad3eff8762c80b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1840 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb930452ac6d4caebde50afccd37aa6e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/220 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\LENOVO\\AppData\\Local\\Temp\\ipykernel_24684\\2636105220.py:58: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `WeightedTrainer.__init__`. Use `processing_class` instead.\n",
      "  super().__init__(*args, **kwargs)\n",
      "c:\\Users\\LENOVO\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:668: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='920' max='920' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [920/920 17:20, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1 Macro</th>\n",
       "      <th>F1 Weighted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.887500</td>\n",
       "      <td>1.315057</td>\n",
       "      <td>0.472727</td>\n",
       "      <td>0.422455</td>\n",
       "      <td>0.485666</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\LENOVO\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:668: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='110' max='110' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [110/110 00:25]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== roberta-base (S3 aug=backtranslation) ===\n",
      "{'eval_loss': 1.3150568008422852, 'eval_accuracy': 0.4727272727272727, 'eval_f1_macro': 0.4224545743610223, 'eval_f1_weighted': 0.4856659100177356, 'eval_runtime': 25.8554, 'eval_samples_per_second': 8.509, 'eval_steps_per_second': 4.254, 'epoch': 1.0}\n",
      "\n",
      "\n",
      "================================================================================\n",
      "S3 Strategy: bert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following layers were not sharded: bert.encoder.layer.*.intermediate.dense.weight, cls.predictions.decoder.weight, cls.predictions.decoder.bias, bert.encoder.layer.*.attention.self.key.weight, bert.encoder.layer.*.output.LayerNorm.bias, bert.encoder.layer.*.attention.self.query.weight, bert.embeddings.token_type_embeddings.weight, bert.encoder.layer.*.attention.self.value.bias, cls.predictions.transform.dense.weight, cls.predictions.transform.dense.bias, cls.predictions.transform.LayerNorm.weight, bert.encoder.layer.*.attention.output.dense.bias, bert.encoder.layer.*.attention.self.key.bias, bert.embeddings.LayerNorm.bias, bert.encoder.layer.*.output.LayerNorm.weight, bert.encoder.layer.*.attention.self.value.weight, bert.embeddings.word_embeddings.weight, bert.encoder.layer.*.attention.self.query.bias, bert.encoder.layer.*.attention.output.dense.weight, cls.predictions.transform.LayerNorm.bias, bert.encoder.layer.*.intermediate.dense.bias, bert.encoder.layer.*.output.dense.weight, bert.encoder.layer.*.attention.output.LayerNorm.bias, bert.encoder.layer.*.output.dense.bias, bert.encoder.layer.*.attention.output.LayerNorm.weight, cls.predictions.bias, bert.embeddings.LayerNorm.weight, bert.embeddings.position_embeddings.weight\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distribusi train after aug: Counter({np.int64(4): 368, np.int64(5): 368, np.int64(2): 368, np.int64(3): 368, np.int64(1): 368})\n",
      "=== Linear SVM (S3 aug=bert) ===\n",
      "Accuracy : 0.4545\n",
      "F1-macro : 0.3753\n",
      "F1-weight: 0.4482\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1     0.4286    0.3750    0.4000        16\n",
      "           2     0.2353    0.2857    0.2581        14\n",
      "           3     0.2581    0.2286    0.2424        35\n",
      "           4     0.3509    0.3175    0.3333        63\n",
      "           5     0.6139    0.6739    0.6425        92\n",
      "\n",
      "    accuracy                         0.4545       220\n",
      "   macro avg     0.3773    0.3761    0.3753       220\n",
      "weighted avg     0.4444    0.4545    0.4482       220\n",
      "\n",
      "\n",
      "=== Naive Bayes (S3 aug=bert) ===\n",
      "Accuracy : 0.4682\n",
      "F1-macro : 0.3721\n",
      "F1-weight: 0.4552\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1     0.4167    0.3125    0.3571        16\n",
      "           2     0.1905    0.2857    0.2286        14\n",
      "           3     0.3030    0.2857    0.2941        35\n",
      "           4     0.3953    0.2698    0.3208        63\n",
      "           5     0.6036    0.7283    0.6601        92\n",
      "\n",
      "    accuracy                         0.4682       220\n",
      "   macro avg     0.3818    0.3764    0.3721       220\n",
      "weighted avg     0.4563    0.4682    0.4552       220\n",
      "\n",
      "\n",
      "[LSTM (S3 aug=bert)] Epoch 1/3 | loss=1.6109\n",
      "[LSTM (S3 aug=bert)] Epoch 2/3 | loss=1.6096\n",
      "[LSTM (S3 aug=bert)] Epoch 3/3 | loss=1.6105\n",
      "=== LSTM (S3 aug=bert) ===\n",
      "Accuracy : 0.1591\n",
      "F1-macro : 0.0549\n",
      "F1-weight: 0.0437\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1     0.0000    0.0000    0.0000        16\n",
      "           2     0.0000    0.0000    0.0000        14\n",
      "           3     0.1591    1.0000    0.2745        35\n",
      "           4     0.0000    0.0000    0.0000        63\n",
      "           5     0.0000    0.0000    0.0000        92\n",
      "\n",
      "    accuracy                         0.1591       220\n",
      "   macro avg     0.0318    0.2000    0.0549       220\n",
      "weighted avg     0.0253    0.1591    0.0437       220\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following layers were not sharded: bert.pooler.dense.weight, bert.encoder.layer.*.intermediate.dense.weight, bert.encoder.layer.*.attention.self.key.weight, bert.encoder.layer.*.output.LayerNorm.bias, bert.encoder.layer.*.attention.self.query.weight, bert.embeddings.token_type_embeddings.weight, bert.encoder.layer.*.attention.self.value.bias, bert.encoder.layer.*.attention.output.dense.bias, classifier.weight, bert.encoder.layer.*.attention.self.key.bias, bert.embeddings.LayerNorm.bias, bert.encoder.layer.*.output.LayerNorm.weight, bert.encoder.layer.*.attention.self.value.weight, bert.embeddings.word_embeddings.weight, bert.encoder.layer.*.attention.output.dense.weight, bert.pooler.dense.bias, bert.encoder.layer.*.intermediate.dense.bias, bert.encoder.layer.*.output.dense.weight, bert.encoder.layer.*.attention.output.LayerNorm.bias, bert.encoder.layer.*.output.dense.bias, classifier.bias, bert.encoder.layer.*.attention.output.LayerNorm.weight, bert.encoder.layer.*.attention.self.query.bias, bert.embeddings.LayerNorm.weight, bert.embeddings.position_embeddings.weight\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "83e22a7ad0894409ac24a16ef43e5f7d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1840 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2dfb09931c174c018329bce3f89e6d42",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/220 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\LENOVO\\AppData\\Local\\Temp\\ipykernel_24684\\2636105220.py:58: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `WeightedTrainer.__init__`. Use `processing_class` instead.\n",
      "  super().__init__(*args, **kwargs)\n",
      "c:\\Users\\LENOVO\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:668: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='920' max='920' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [920/920 17:15, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1 Macro</th>\n",
       "      <th>F1 Weighted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.163400</td>\n",
       "      <td>1.090398</td>\n",
       "      <td>0.559091</td>\n",
       "      <td>0.486401</td>\n",
       "      <td>0.565191</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\LENOVO\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:668: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='110' max='110' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [110/110 00:24]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== bert-base-uncased (S3 aug=bert) ===\n",
      "{'eval_loss': 1.090397834777832, 'eval_accuracy': 0.5590909090909091, 'eval_f1_macro': 0.48640140294850437, 'eval_f1_weighted': 0.5651912195767086, 'eval_runtime': 25.1028, 'eval_samples_per_second': 8.764, 'eval_steps_per_second': 4.382, 'epoch': 1.0}\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following layers were not sharded: roberta.encoder.layer.*.attention.self.key.weight, classifier.dense.weight, roberta.encoder.layer.*.attention.output.dense.bias, classifier.out_proj.bias, roberta.encoder.layer.*.output.LayerNorm.bias, roberta.encoder.layer.*.attention.self.key.bias, roberta.encoder.layer.*.attention.output.dense.weight, roberta.encoder.layer.*.attention.self.value.bias, roberta.encoder.layer.*.intermediate.dense.bias, roberta.encoder.layer.*.attention.output.LayerNorm.bias, roberta.encoder.layer.*.attention.self.value.weight, roberta.encoder.layer.*.attention.self.query.weight, roberta.embeddings.LayerNorm.weight, roberta.encoder.layer.*.attention.self.query.bias, roberta.encoder.layer.*.attention.output.LayerNorm.weight, roberta.encoder.layer.*.output.dense.weight, classifier.out_proj.weight, roberta.embeddings.LayerNorm.bias, roberta.embeddings.token_type_embeddings.weight, roberta.encoder.layer.*.intermediate.dense.weight, roberta.embeddings.position_embeddings.weight, roberta.embeddings.word_embeddings.weight, roberta.encoder.layer.*.output.LayerNorm.weight, roberta.encoder.layer.*.output.dense.bias, classifier.dense.bias\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db6991dfb7154b889d10ac1c8c6b2645",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1840 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e1823c7f9d294da79233d5359a91bbbd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/220 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\LENOVO\\AppData\\Local\\Temp\\ipykernel_24684\\2636105220.py:58: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `WeightedTrainer.__init__`. Use `processing_class` instead.\n",
      "  super().__init__(*args, **kwargs)\n",
      "c:\\Users\\LENOVO\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:668: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='920' max='920' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [920/920 55:50, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1 Macro</th>\n",
       "      <th>F1 Weighted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.348400</td>\n",
       "      <td>1.133677</td>\n",
       "      <td>0.545455</td>\n",
       "      <td>0.458125</td>\n",
       "      <td>0.547961</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\LENOVO\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:668: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='110' max='110' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [110/110 00:40]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== roberta-base (S3 aug=bert) ===\n",
      "{'eval_loss': 1.133677363395691, 'eval_accuracy': 0.5454545454545454, 'eval_f1_macro': 0.45812548169971395, 'eval_f1_weighted': 0.5479611373169443, 'eval_runtime': 41.311, 'eval_samples_per_second': 5.325, 'eval_steps_per_second': 2.663, 'epoch': 1.0}\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>f1_macro</th>\n",
       "      <th>f1_weighted</th>\n",
       "      <th>eval_loss</th>\n",
       "      <th>eval_runtime</th>\n",
       "      <th>eval_samples_per_second</th>\n",
       "      <th>eval_steps_per_second</th>\n",
       "      <th>epoch</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>bert-base-uncased (S3 aug=bert)</td>\n",
       "      <td>0.559091</td>\n",
       "      <td>0.486401</td>\n",
       "      <td>0.565191</td>\n",
       "      <td>1.090398</td>\n",
       "      <td>25.1028</td>\n",
       "      <td>8.764</td>\n",
       "      <td>4.382</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>roberta-base (S3 aug=bert)</td>\n",
       "      <td>0.545455</td>\n",
       "      <td>0.458125</td>\n",
       "      <td>0.547961</td>\n",
       "      <td>1.133677</td>\n",
       "      <td>41.3110</td>\n",
       "      <td>5.325</td>\n",
       "      <td>2.663</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>bert-base-uncased (S3 aug=modified_eda)</td>\n",
       "      <td>0.522727</td>\n",
       "      <td>0.434754</td>\n",
       "      <td>0.524214</td>\n",
       "      <td>1.138444</td>\n",
       "      <td>22.8031</td>\n",
       "      <td>9.648</td>\n",
       "      <td>4.824</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>roberta-base (S3 aug=backtranslation)</td>\n",
       "      <td>0.472727</td>\n",
       "      <td>0.422455</td>\n",
       "      <td>0.485666</td>\n",
       "      <td>1.315057</td>\n",
       "      <td>25.8554</td>\n",
       "      <td>8.509</td>\n",
       "      <td>4.254</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>roberta-base (S3 aug=modified_eda)</td>\n",
       "      <td>0.486364</td>\n",
       "      <td>0.416236</td>\n",
       "      <td>0.492973</td>\n",
       "      <td>1.209656</td>\n",
       "      <td>22.6716</td>\n",
       "      <td>9.704</td>\n",
       "      <td>4.852</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Naive Bayes (S3 aug=modified_eda)</td>\n",
       "      <td>0.454545</td>\n",
       "      <td>0.409137</td>\n",
       "      <td>0.465603</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>bert-base-uncased (S3 aug=backtranslation)</td>\n",
       "      <td>0.490909</td>\n",
       "      <td>0.404342</td>\n",
       "      <td>0.486915</td>\n",
       "      <td>1.128783</td>\n",
       "      <td>23.3152</td>\n",
       "      <td>9.436</td>\n",
       "      <td>4.718</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>roberta-base (S3 aug=eda)</td>\n",
       "      <td>0.454545</td>\n",
       "      <td>0.404059</td>\n",
       "      <td>0.466966</td>\n",
       "      <td>1.151978</td>\n",
       "      <td>22.6491</td>\n",
       "      <td>9.713</td>\n",
       "      <td>4.857</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Naive Bayes (S3 aug=eda)</td>\n",
       "      <td>0.468182</td>\n",
       "      <td>0.402408</td>\n",
       "      <td>0.478746</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Linear SVM (S3 aug=bert)</td>\n",
       "      <td>0.454545</td>\n",
       "      <td>0.375262</td>\n",
       "      <td>0.448212</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Linear SVM (S3 aug=backtranslation)</td>\n",
       "      <td>0.468182</td>\n",
       "      <td>0.373495</td>\n",
       "      <td>0.456214</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Naive Bayes (S3 aug=bert)</td>\n",
       "      <td>0.468182</td>\n",
       "      <td>0.372137</td>\n",
       "      <td>0.455205</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Linear SVM (S3 aug=modified_eda)</td>\n",
       "      <td>0.463636</td>\n",
       "      <td>0.361650</td>\n",
       "      <td>0.451752</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Linear SVM (S3 aug=eda)</td>\n",
       "      <td>0.454545</td>\n",
       "      <td>0.360145</td>\n",
       "      <td>0.448414</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Naive Bayes (S3 aug=backtranslation)</td>\n",
       "      <td>0.395455</td>\n",
       "      <td>0.348043</td>\n",
       "      <td>0.413761</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>bert-base-uncased (S3 aug=eda)</td>\n",
       "      <td>0.486364</td>\n",
       "      <td>0.337168</td>\n",
       "      <td>0.460908</td>\n",
       "      <td>1.181916</td>\n",
       "      <td>22.7037</td>\n",
       "      <td>9.690</td>\n",
       "      <td>4.845</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>LSTM (S3 aug=bert)</td>\n",
       "      <td>0.159091</td>\n",
       "      <td>0.054902</td>\n",
       "      <td>0.043672</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>LSTM (S3 aug=backtranslation)</td>\n",
       "      <td>0.072727</td>\n",
       "      <td>0.027119</td>\n",
       "      <td>0.009861</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>LSTM (S3 aug=eda)</td>\n",
       "      <td>0.063636</td>\n",
       "      <td>0.023932</td>\n",
       "      <td>0.007615</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>LSTM (S3 aug=modified_eda)</td>\n",
       "      <td>0.063636</td>\n",
       "      <td>0.023932</td>\n",
       "      <td>0.007615</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         model  accuracy  f1_macro  \\\n",
       "18             bert-base-uncased (S3 aug=bert)  0.559091  0.486401   \n",
       "19                  roberta-base (S3 aug=bert)  0.545455  0.458125   \n",
       "8      bert-base-uncased (S3 aug=modified_eda)  0.522727  0.434754   \n",
       "14       roberta-base (S3 aug=backtranslation)  0.472727  0.422455   \n",
       "9           roberta-base (S3 aug=modified_eda)  0.486364  0.416236   \n",
       "6            Naive Bayes (S3 aug=modified_eda)  0.454545  0.409137   \n",
       "13  bert-base-uncased (S3 aug=backtranslation)  0.490909  0.404342   \n",
       "4                    roberta-base (S3 aug=eda)  0.454545  0.404059   \n",
       "1                     Naive Bayes (S3 aug=eda)  0.468182  0.402408   \n",
       "15                    Linear SVM (S3 aug=bert)  0.454545  0.375262   \n",
       "10         Linear SVM (S3 aug=backtranslation)  0.468182  0.373495   \n",
       "16                   Naive Bayes (S3 aug=bert)  0.468182  0.372137   \n",
       "5             Linear SVM (S3 aug=modified_eda)  0.463636  0.361650   \n",
       "0                      Linear SVM (S3 aug=eda)  0.454545  0.360145   \n",
       "11        Naive Bayes (S3 aug=backtranslation)  0.395455  0.348043   \n",
       "3               bert-base-uncased (S3 aug=eda)  0.486364  0.337168   \n",
       "17                          LSTM (S3 aug=bert)  0.159091  0.054902   \n",
       "12               LSTM (S3 aug=backtranslation)  0.072727  0.027119   \n",
       "2                            LSTM (S3 aug=eda)  0.063636  0.023932   \n",
       "7                   LSTM (S3 aug=modified_eda)  0.063636  0.023932   \n",
       "\n",
       "    f1_weighted  eval_loss  eval_runtime  eval_samples_per_second  \\\n",
       "18     0.565191   1.090398       25.1028                    8.764   \n",
       "19     0.547961   1.133677       41.3110                    5.325   \n",
       "8      0.524214   1.138444       22.8031                    9.648   \n",
       "14     0.485666   1.315057       25.8554                    8.509   \n",
       "9      0.492973   1.209656       22.6716                    9.704   \n",
       "6      0.465603        NaN           NaN                      NaN   \n",
       "13     0.486915   1.128783       23.3152                    9.436   \n",
       "4      0.466966   1.151978       22.6491                    9.713   \n",
       "1      0.478746        NaN           NaN                      NaN   \n",
       "15     0.448212        NaN           NaN                      NaN   \n",
       "10     0.456214        NaN           NaN                      NaN   \n",
       "16     0.455205        NaN           NaN                      NaN   \n",
       "5      0.451752        NaN           NaN                      NaN   \n",
       "0      0.448414        NaN           NaN                      NaN   \n",
       "11     0.413761        NaN           NaN                      NaN   \n",
       "3      0.460908   1.181916       22.7037                    9.690   \n",
       "17     0.043672        NaN           NaN                      NaN   \n",
       "12     0.009861        NaN           NaN                      NaN   \n",
       "2      0.007615        NaN           NaN                      NaN   \n",
       "7      0.007615        NaN           NaN                      NaN   \n",
       "\n",
       "    eval_steps_per_second  epoch  \n",
       "18                  4.382    1.0  \n",
       "19                  2.663    1.0  \n",
       "8                   4.824    1.0  \n",
       "14                  4.254    1.0  \n",
       "9                   4.852    1.0  \n",
       "6                     NaN    NaN  \n",
       "13                  4.718    1.0  \n",
       "4                   4.857    1.0  \n",
       "1                     NaN    NaN  \n",
       "15                    NaN    NaN  \n",
       "10                    NaN    NaN  \n",
       "16                    NaN    NaN  \n",
       "5                     NaN    NaN  \n",
       "0                     NaN    NaN  \n",
       "11                    NaN    NaN  \n",
       "3                   4.845    1.0  \n",
       "17                    NaN    NaN  \n",
       "12                    NaN    NaN  \n",
       "2                     NaN    NaN  \n",
       "7                     NaN    NaN  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "results_s3 = []\n",
    "\n",
    "for strat in AUG_STRATEGIES:\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"S3 Strategy:\", strat)\n",
    "\n",
    "    Xa, ya = augment_minority(\n",
    "        X_train, y_train,\n",
    "        strategy=strat,\n",
    "        target_multiplier=TARGET_MULTIPLIER,\n",
    "        max_aug_per_sample=MAX_AUG_PER_SAMPLE\n",
    "    )\n",
    "\n",
    "    print(\"Distribusi train after aug:\", Counter(ya))\n",
    "    suffix = f\" (S3 aug={strat})\"\n",
    "\n",
    "    # SVM\n",
    "    svm_s3_res, _, _, _ = train_eval_svm(\n",
    "        Xa, ya, X_test, y_test,\n",
    "        suffix=suffix,\n",
    "        class_weight=None\n",
    "    )\n",
    "    results_s3.append(svm_s3_res)\n",
    "\n",
    "    # NB\n",
    "    nb_s3_res, _, _, _ = train_eval_nb(\n",
    "        Xa, ya, X_test, y_test,\n",
    "        suffix=suffix\n",
    "    )\n",
    "    results_s3.append(nb_s3_res)\n",
    "\n",
    "    # LSTM (weighted loss optional in S3)\n",
    "    try:\n",
    "        lstm_s3_res = train_eval_lstm(\n",
    "            Xa, ya, X_test, y_test,\n",
    "            epochs=3, suffix=suffix,\n",
    "            use_class_weights=True\n",
    "        )\n",
    "        results_s3.append(lstm_s3_res)\n",
    "    except Exception as e:\n",
    "        print(\"LSTM S3 error:\", e)\n",
    "\n",
    "    # BERT (weighted loss)\n",
    "    try:\n",
    "        bert_s3_res = train_eval_transformer(\n",
    "            \"bert-base-uncased\",\n",
    "            Xa, ya, X_test, y_test,\n",
    "            epochs=DEFAULT_EPOCHS,\n",
    "            batch_size=DEFAULT_BATCH,\n",
    "            max_length=DEFAULT_MAX_LEN,\n",
    "            out_dir=f\"./tmp_bert_s3_{strat}\",\n",
    "            suffix=suffix,\n",
    "            use_class_weights=True\n",
    "        )\n",
    "        results_s3.append(bert_s3_res)\n",
    "    except Exception as e:\n",
    "        print(\"BERT S3 error:\", e)\n",
    "\n",
    "    # RoBERTa (weighted loss)\n",
    "    try:\n",
    "        roberta_s3_res = train_eval_transformer(\n",
    "            \"roberta-base\",\n",
    "            Xa, ya, X_test, y_test,\n",
    "            epochs=DEFAULT_EPOCHS,\n",
    "            batch_size=DEFAULT_BATCH,\n",
    "            max_length=DEFAULT_MAX_LEN,\n",
    "            out_dir=f\"./tmp_roberta_s3_{strat}\",\n",
    "            suffix=suffix,\n",
    "            use_class_weights=True\n",
    "        )\n",
    "        results_s3.append(roberta_s3_res)\n",
    "    except Exception as e:\n",
    "        print(\"RoBERTa S3 error:\", e)\n",
    "\n",
    "s3_table = pd.DataFrame(results_s3)\n",
    "s3_table.sort_values(\"f1_macro\", ascending=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c94b73c4",
   "metadata": {},
   "source": [
    "## 13. Rekap Per Skenario & Summary Akhir (Seragam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "95c06ccc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "S1 (baseline):\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>f1_macro</th>\n",
       "      <th>f1_weighted</th>\n",
       "      <th>eval_loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Linear SVM (S1 baseline)</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.375765</td>\n",
       "      <td>0.473433</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>bert-base-uncased (S1 baseline)</td>\n",
       "      <td>0.486364</td>\n",
       "      <td>0.210667</td>\n",
       "      <td>0.400061</td>\n",
       "      <td>1.147911</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>roberta-base (S1 baseline)</td>\n",
       "      <td>0.490909</td>\n",
       "      <td>0.208292</td>\n",
       "      <td>0.396256</td>\n",
       "      <td>1.171993</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Naive Bayes (S1 baseline)</td>\n",
       "      <td>0.436364</td>\n",
       "      <td>0.147927</td>\n",
       "      <td>0.291245</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>LSTM (S1 baseline)</td>\n",
       "      <td>0.418182</td>\n",
       "      <td>0.117949</td>\n",
       "      <td>0.246620</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             model  accuracy  f1_macro  f1_weighted  eval_loss\n",
       "0         Linear SVM (S1 baseline)  0.500000  0.375765     0.473433        NaN\n",
       "3  bert-base-uncased (S1 baseline)  0.486364  0.210667     0.400061   1.147911\n",
       "4       roberta-base (S1 baseline)  0.490909  0.208292     0.396256   1.171993\n",
       "1        Naive Bayes (S1 baseline)  0.436364  0.147927     0.291245        NaN\n",
       "2               LSTM (S1 baseline)  0.418182  0.117949     0.246620        NaN"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "S2 (balanced no-aug):\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>f1_macro</th>\n",
       "      <th>f1_weighted</th>\n",
       "      <th>eval_loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Linear SVM (S2 balanced no-aug)</td>\n",
       "      <td>0.490909</td>\n",
       "      <td>0.384043</td>\n",
       "      <td>0.476874</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>bert-base-uncased (S2 balanced no-aug)</td>\n",
       "      <td>0.559091</td>\n",
       "      <td>0.373691</td>\n",
       "      <td>0.510886</td>\n",
       "      <td>1.234063</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Naive Bayes (S2 balanced no-aug)</td>\n",
       "      <td>0.422727</td>\n",
       "      <td>0.370254</td>\n",
       "      <td>0.444006</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>roberta-base (S2 balanced no-aug)</td>\n",
       "      <td>0.445455</td>\n",
       "      <td>0.203278</td>\n",
       "      <td>0.376389</td>\n",
       "      <td>1.426181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>LSTM (S2 balanced no-aug)</td>\n",
       "      <td>0.286364</td>\n",
       "      <td>0.089046</td>\n",
       "      <td>0.127498</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    model  accuracy  f1_macro  f1_weighted  \\\n",
       "0         Linear SVM (S2 balanced no-aug)  0.490909  0.384043     0.476874   \n",
       "3  bert-base-uncased (S2 balanced no-aug)  0.559091  0.373691     0.510886   \n",
       "1        Naive Bayes (S2 balanced no-aug)  0.422727  0.370254     0.444006   \n",
       "4       roberta-base (S2 balanced no-aug)  0.445455  0.203278     0.376389   \n",
       "2               LSTM (S2 balanced no-aug)  0.286364  0.089046     0.127498   \n",
       "\n",
       "   eval_loss  \n",
       "0        NaN  \n",
       "3   1.234063  \n",
       "1        NaN  \n",
       "4   1.426181  \n",
       "2        NaN  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "S3 (aug):\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>f1_macro</th>\n",
       "      <th>f1_weighted</th>\n",
       "      <th>eval_loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>bert-base-uncased (S3 aug=bert)</td>\n",
       "      <td>0.559091</td>\n",
       "      <td>0.486401</td>\n",
       "      <td>0.565191</td>\n",
       "      <td>1.090398</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>roberta-base (S3 aug=bert)</td>\n",
       "      <td>0.545455</td>\n",
       "      <td>0.458125</td>\n",
       "      <td>0.547961</td>\n",
       "      <td>1.133677</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>bert-base-uncased (S3 aug=modified_eda)</td>\n",
       "      <td>0.522727</td>\n",
       "      <td>0.434754</td>\n",
       "      <td>0.524214</td>\n",
       "      <td>1.138444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>roberta-base (S3 aug=backtranslation)</td>\n",
       "      <td>0.472727</td>\n",
       "      <td>0.422455</td>\n",
       "      <td>0.485666</td>\n",
       "      <td>1.315057</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>roberta-base (S3 aug=modified_eda)</td>\n",
       "      <td>0.486364</td>\n",
       "      <td>0.416236</td>\n",
       "      <td>0.492973</td>\n",
       "      <td>1.209656</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Naive Bayes (S3 aug=modified_eda)</td>\n",
       "      <td>0.454545</td>\n",
       "      <td>0.409137</td>\n",
       "      <td>0.465603</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>bert-base-uncased (S3 aug=backtranslation)</td>\n",
       "      <td>0.490909</td>\n",
       "      <td>0.404342</td>\n",
       "      <td>0.486915</td>\n",
       "      <td>1.128783</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>roberta-base (S3 aug=eda)</td>\n",
       "      <td>0.454545</td>\n",
       "      <td>0.404059</td>\n",
       "      <td>0.466966</td>\n",
       "      <td>1.151978</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Naive Bayes (S3 aug=eda)</td>\n",
       "      <td>0.468182</td>\n",
       "      <td>0.402408</td>\n",
       "      <td>0.478746</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Linear SVM (S3 aug=bert)</td>\n",
       "      <td>0.454545</td>\n",
       "      <td>0.375262</td>\n",
       "      <td>0.448212</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Linear SVM (S3 aug=backtranslation)</td>\n",
       "      <td>0.468182</td>\n",
       "      <td>0.373495</td>\n",
       "      <td>0.456214</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Naive Bayes (S3 aug=bert)</td>\n",
       "      <td>0.468182</td>\n",
       "      <td>0.372137</td>\n",
       "      <td>0.455205</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Linear SVM (S3 aug=modified_eda)</td>\n",
       "      <td>0.463636</td>\n",
       "      <td>0.361650</td>\n",
       "      <td>0.451752</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Linear SVM (S3 aug=eda)</td>\n",
       "      <td>0.454545</td>\n",
       "      <td>0.360145</td>\n",
       "      <td>0.448414</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Naive Bayes (S3 aug=backtranslation)</td>\n",
       "      <td>0.395455</td>\n",
       "      <td>0.348043</td>\n",
       "      <td>0.413761</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>bert-base-uncased (S3 aug=eda)</td>\n",
       "      <td>0.486364</td>\n",
       "      <td>0.337168</td>\n",
       "      <td>0.460908</td>\n",
       "      <td>1.181916</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>LSTM (S3 aug=bert)</td>\n",
       "      <td>0.159091</td>\n",
       "      <td>0.054902</td>\n",
       "      <td>0.043672</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>LSTM (S3 aug=backtranslation)</td>\n",
       "      <td>0.072727</td>\n",
       "      <td>0.027119</td>\n",
       "      <td>0.009861</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>LSTM (S3 aug=eda)</td>\n",
       "      <td>0.063636</td>\n",
       "      <td>0.023932</td>\n",
       "      <td>0.007615</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>LSTM (S3 aug=modified_eda)</td>\n",
       "      <td>0.063636</td>\n",
       "      <td>0.023932</td>\n",
       "      <td>0.007615</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         model  accuracy  f1_macro  \\\n",
       "18             bert-base-uncased (S3 aug=bert)  0.559091  0.486401   \n",
       "19                  roberta-base (S3 aug=bert)  0.545455  0.458125   \n",
       "8      bert-base-uncased (S3 aug=modified_eda)  0.522727  0.434754   \n",
       "14       roberta-base (S3 aug=backtranslation)  0.472727  0.422455   \n",
       "9           roberta-base (S3 aug=modified_eda)  0.486364  0.416236   \n",
       "6            Naive Bayes (S3 aug=modified_eda)  0.454545  0.409137   \n",
       "13  bert-base-uncased (S3 aug=backtranslation)  0.490909  0.404342   \n",
       "4                    roberta-base (S3 aug=eda)  0.454545  0.404059   \n",
       "1                     Naive Bayes (S3 aug=eda)  0.468182  0.402408   \n",
       "15                    Linear SVM (S3 aug=bert)  0.454545  0.375262   \n",
       "10         Linear SVM (S3 aug=backtranslation)  0.468182  0.373495   \n",
       "16                   Naive Bayes (S3 aug=bert)  0.468182  0.372137   \n",
       "5             Linear SVM (S3 aug=modified_eda)  0.463636  0.361650   \n",
       "0                      Linear SVM (S3 aug=eda)  0.454545  0.360145   \n",
       "11        Naive Bayes (S3 aug=backtranslation)  0.395455  0.348043   \n",
       "3               bert-base-uncased (S3 aug=eda)  0.486364  0.337168   \n",
       "17                          LSTM (S3 aug=bert)  0.159091  0.054902   \n",
       "12               LSTM (S3 aug=backtranslation)  0.072727  0.027119   \n",
       "2                            LSTM (S3 aug=eda)  0.063636  0.023932   \n",
       "7                   LSTM (S3 aug=modified_eda)  0.063636  0.023932   \n",
       "\n",
       "    f1_weighted  eval_loss  \n",
       "18     0.565191   1.090398  \n",
       "19     0.547961   1.133677  \n",
       "8      0.524214   1.138444  \n",
       "14     0.485666   1.315057  \n",
       "9      0.492973   1.209656  \n",
       "6      0.465603        NaN  \n",
       "13     0.486915   1.128783  \n",
       "4      0.466966   1.151978  \n",
       "1      0.478746        NaN  \n",
       "15     0.448212        NaN  \n",
       "10     0.456214        NaN  \n",
       "16     0.455205        NaN  \n",
       "5      0.451752        NaN  \n",
       "0      0.448414        NaN  \n",
       "11     0.413761        NaN  \n",
       "3      0.460908   1.181916  \n",
       "17     0.043672        NaN  \n",
       "12     0.009861        NaN  \n",
       "2      0.007615        NaN  \n",
       "7      0.007615        NaN  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "SUMMARY:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>f1_macro</th>\n",
       "      <th>f1_weighted</th>\n",
       "      <th>eval_loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>bert-base-uncased (S3 aug=bert)</td>\n",
       "      <td>0.559091</td>\n",
       "      <td>0.486401</td>\n",
       "      <td>0.565191</td>\n",
       "      <td>1.090398</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>roberta-base (S3 aug=bert)</td>\n",
       "      <td>0.545455</td>\n",
       "      <td>0.458125</td>\n",
       "      <td>0.547961</td>\n",
       "      <td>1.133677</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>bert-base-uncased (S3 aug=modified_eda)</td>\n",
       "      <td>0.522727</td>\n",
       "      <td>0.434754</td>\n",
       "      <td>0.524214</td>\n",
       "      <td>1.138444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>roberta-base (S3 aug=backtranslation)</td>\n",
       "      <td>0.472727</td>\n",
       "      <td>0.422455</td>\n",
       "      <td>0.485666</td>\n",
       "      <td>1.315057</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>roberta-base (S3 aug=modified_eda)</td>\n",
       "      <td>0.486364</td>\n",
       "      <td>0.416236</td>\n",
       "      <td>0.492973</td>\n",
       "      <td>1.209656</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Naive Bayes (S3 aug=modified_eda)</td>\n",
       "      <td>0.454545</td>\n",
       "      <td>0.409137</td>\n",
       "      <td>0.465603</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>bert-base-uncased (S3 aug=backtranslation)</td>\n",
       "      <td>0.490909</td>\n",
       "      <td>0.404342</td>\n",
       "      <td>0.486915</td>\n",
       "      <td>1.128783</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>roberta-base (S3 aug=eda)</td>\n",
       "      <td>0.454545</td>\n",
       "      <td>0.404059</td>\n",
       "      <td>0.466966</td>\n",
       "      <td>1.151978</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Naive Bayes (S3 aug=eda)</td>\n",
       "      <td>0.468182</td>\n",
       "      <td>0.402408</td>\n",
       "      <td>0.478746</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Linear SVM (S2 balanced no-aug)</td>\n",
       "      <td>0.490909</td>\n",
       "      <td>0.384043</td>\n",
       "      <td>0.476874</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Linear SVM (S1 baseline)</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.375765</td>\n",
       "      <td>0.473433</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>Linear SVM (S3 aug=bert)</td>\n",
       "      <td>0.454545</td>\n",
       "      <td>0.375262</td>\n",
       "      <td>0.448212</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>bert-base-uncased (S2 balanced no-aug)</td>\n",
       "      <td>0.559091</td>\n",
       "      <td>0.373691</td>\n",
       "      <td>0.510886</td>\n",
       "      <td>1.234063</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Linear SVM (S3 aug=backtranslation)</td>\n",
       "      <td>0.468182</td>\n",
       "      <td>0.373495</td>\n",
       "      <td>0.456214</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>Naive Bayes (S3 aug=bert)</td>\n",
       "      <td>0.468182</td>\n",
       "      <td>0.372137</td>\n",
       "      <td>0.455205</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Naive Bayes (S2 balanced no-aug)</td>\n",
       "      <td>0.422727</td>\n",
       "      <td>0.370254</td>\n",
       "      <td>0.444006</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Linear SVM (S3 aug=modified_eda)</td>\n",
       "      <td>0.463636</td>\n",
       "      <td>0.361650</td>\n",
       "      <td>0.451752</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Linear SVM (S3 aug=eda)</td>\n",
       "      <td>0.454545</td>\n",
       "      <td>0.360145</td>\n",
       "      <td>0.448414</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Naive Bayes (S3 aug=backtranslation)</td>\n",
       "      <td>0.395455</td>\n",
       "      <td>0.348043</td>\n",
       "      <td>0.413761</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>bert-base-uncased (S3 aug=eda)</td>\n",
       "      <td>0.486364</td>\n",
       "      <td>0.337168</td>\n",
       "      <td>0.460908</td>\n",
       "      <td>1.181916</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>bert-base-uncased (S1 baseline)</td>\n",
       "      <td>0.486364</td>\n",
       "      <td>0.210667</td>\n",
       "      <td>0.400061</td>\n",
       "      <td>1.147911</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>roberta-base (S1 baseline)</td>\n",
       "      <td>0.490909</td>\n",
       "      <td>0.208292</td>\n",
       "      <td>0.396256</td>\n",
       "      <td>1.171993</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>roberta-base (S2 balanced no-aug)</td>\n",
       "      <td>0.445455</td>\n",
       "      <td>0.203278</td>\n",
       "      <td>0.376389</td>\n",
       "      <td>1.426181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Naive Bayes (S1 baseline)</td>\n",
       "      <td>0.436364</td>\n",
       "      <td>0.147927</td>\n",
       "      <td>0.291245</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>LSTM (S1 baseline)</td>\n",
       "      <td>0.418182</td>\n",
       "      <td>0.117949</td>\n",
       "      <td>0.246620</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>LSTM (S2 balanced no-aug)</td>\n",
       "      <td>0.286364</td>\n",
       "      <td>0.089046</td>\n",
       "      <td>0.127498</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>LSTM (S3 aug=bert)</td>\n",
       "      <td>0.159091</td>\n",
       "      <td>0.054902</td>\n",
       "      <td>0.043672</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>LSTM (S3 aug=backtranslation)</td>\n",
       "      <td>0.072727</td>\n",
       "      <td>0.027119</td>\n",
       "      <td>0.009861</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>LSTM (S3 aug=eda)</td>\n",
       "      <td>0.063636</td>\n",
       "      <td>0.023932</td>\n",
       "      <td>0.007615</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>LSTM (S3 aug=modified_eda)</td>\n",
       "      <td>0.063636</td>\n",
       "      <td>0.023932</td>\n",
       "      <td>0.007615</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         model  accuracy  f1_macro  \\\n",
       "28             bert-base-uncased (S3 aug=bert)  0.559091  0.486401   \n",
       "29                  roberta-base (S3 aug=bert)  0.545455  0.458125   \n",
       "18     bert-base-uncased (S3 aug=modified_eda)  0.522727  0.434754   \n",
       "24       roberta-base (S3 aug=backtranslation)  0.472727  0.422455   \n",
       "19          roberta-base (S3 aug=modified_eda)  0.486364  0.416236   \n",
       "16           Naive Bayes (S3 aug=modified_eda)  0.454545  0.409137   \n",
       "23  bert-base-uncased (S3 aug=backtranslation)  0.490909  0.404342   \n",
       "14                   roberta-base (S3 aug=eda)  0.454545  0.404059   \n",
       "11                    Naive Bayes (S3 aug=eda)  0.468182  0.402408   \n",
       "5              Linear SVM (S2 balanced no-aug)  0.490909  0.384043   \n",
       "0                     Linear SVM (S1 baseline)  0.500000  0.375765   \n",
       "25                    Linear SVM (S3 aug=bert)  0.454545  0.375262   \n",
       "8       bert-base-uncased (S2 balanced no-aug)  0.559091  0.373691   \n",
       "20         Linear SVM (S3 aug=backtranslation)  0.468182  0.373495   \n",
       "26                   Naive Bayes (S3 aug=bert)  0.468182  0.372137   \n",
       "6             Naive Bayes (S2 balanced no-aug)  0.422727  0.370254   \n",
       "15            Linear SVM (S3 aug=modified_eda)  0.463636  0.361650   \n",
       "10                     Linear SVM (S3 aug=eda)  0.454545  0.360145   \n",
       "21        Naive Bayes (S3 aug=backtranslation)  0.395455  0.348043   \n",
       "13              bert-base-uncased (S3 aug=eda)  0.486364  0.337168   \n",
       "3              bert-base-uncased (S1 baseline)  0.486364  0.210667   \n",
       "4                   roberta-base (S1 baseline)  0.490909  0.208292   \n",
       "9            roberta-base (S2 balanced no-aug)  0.445455  0.203278   \n",
       "1                    Naive Bayes (S1 baseline)  0.436364  0.147927   \n",
       "2                           LSTM (S1 baseline)  0.418182  0.117949   \n",
       "7                    LSTM (S2 balanced no-aug)  0.286364  0.089046   \n",
       "27                          LSTM (S3 aug=bert)  0.159091  0.054902   \n",
       "22               LSTM (S3 aug=backtranslation)  0.072727  0.027119   \n",
       "12                           LSTM (S3 aug=eda)  0.063636  0.023932   \n",
       "17                  LSTM (S3 aug=modified_eda)  0.063636  0.023932   \n",
       "\n",
       "    f1_weighted  eval_loss  \n",
       "28     0.565191   1.090398  \n",
       "29     0.547961   1.133677  \n",
       "18     0.524214   1.138444  \n",
       "24     0.485666   1.315057  \n",
       "19     0.492973   1.209656  \n",
       "16     0.465603        NaN  \n",
       "23     0.486915   1.128783  \n",
       "14     0.466966   1.151978  \n",
       "11     0.478746        NaN  \n",
       "5      0.476874        NaN  \n",
       "0      0.473433        NaN  \n",
       "25     0.448212        NaN  \n",
       "8      0.510886   1.234063  \n",
       "20     0.456214        NaN  \n",
       "26     0.455205        NaN  \n",
       "6      0.444006        NaN  \n",
       "15     0.451752        NaN  \n",
       "10     0.448414        NaN  \n",
       "21     0.413761        NaN  \n",
       "13     0.460908   1.181916  \n",
       "3      0.400061   1.147911  \n",
       "4      0.396256   1.171993  \n",
       "9      0.376389   1.426181  \n",
       "1      0.291245        NaN  \n",
       "2      0.246620        NaN  \n",
       "7      0.127498        NaN  \n",
       "27     0.043672        NaN  \n",
       "22     0.009861        NaN  \n",
       "12     0.007615        NaN  \n",
       "17     0.007615        NaN  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "def pick_cols(df):\n",
    "    cols = [\"model\", \"accuracy\", \"f1_macro\", \"f1_weighted\"]\n",
    "    if \"eval_loss\" in df.columns:\n",
    "        cols.append(\"eval_loss\")\n",
    "    return df[cols].copy()\n",
    "\n",
    "s1_clean = pick_cols(s1_table)\n",
    "s2_clean = pick_cols(s2_table)\n",
    "s3_clean = pick_cols(s3_table)\n",
    "\n",
    "print(\"S1 (baseline):\")\n",
    "display(s1_clean.sort_values(\"f1_macro\", ascending=False))\n",
    "print(\"\\nS2 (balanced no-aug):\")\n",
    "display(s2_clean.sort_values(\"f1_macro\", ascending=False))\n",
    "print(\"\\nS3 (aug):\")\n",
    "display(s3_clean.sort_values(\"f1_macro\", ascending=False))\n",
    "\n",
    "summary = pd.concat([s1_clean, s2_clean, s3_clean], ignore_index=True)\n",
    "summary = summary.sort_values(\"f1_macro\", ascending=False)\n",
    "print(\"\\nSUMMARY:\")\n",
    "summary\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74d08a34",
   "metadata": {},
   "source": [
    "## 14. Template Narasi Singkat (Opsional)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c64c176",
   "metadata": {},
   "source": [
    "- **S1**: baseline natural (tanpa penanganan imbalance) untuk 5 arsitektur.\n",
    "- **S2**: menguji efektivitas penanganan imbalance **non-semantic** tanpa mengubah teks.\n",
    "- **S3**: menguji apakah augmentasi teks memberikan peningkatan tambahan di atas balancing non-aug.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

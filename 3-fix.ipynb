{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "22613a0b",
   "metadata": {},
   "source": [
    "# Restaurant Rating Prediction (1–5) — EDA + 3 Skenario **Baru**\n",
    "Notebook ini menyajikan **alur skenario yang benar-benar berbeda secara perlakuan**.\n",
    "\n",
    "Struktur eksperimen:\n",
    "- **Skenario 1**\n",
    "  - SVM\n",
    "  - NB\n",
    "  - LSTM\n",
    "  - BERT\n",
    "  - DistilBERT\n",
    "- **Skenario 2**\n",
    "  - SVM\n",
    "  - NB\n",
    "  - LSTM\n",
    "  - BERT\n",
    "  - DistilBERT\n",
    "- **Skenario 3**\n",
    "  - **Tiap model dipisah + loop augmentasi per model**\n",
    "  - SVM (EDA → Modified EDA → ...)\n",
    "  - NB (EDA → Modified EDA → ...)\n",
    "  - dst.\n",
    "\n",
    "## Definisi Skenario\n",
    "### Skenario 1 — Baseline fine-grained **tanpa** penanganan imbalance\n",
    "- Target rating 1–5.\n",
    "- Train original (imbalanced).\n",
    "- 5 model: **SVM, NB, LSTM, BERT, DistilBERT**.\n",
    "\n",
    "### Skenario 2 — Penanganan imbalance **tanpa augmentasi teks**\n",
    "- Target rating 1–5.\n",
    "- **Teks tidak diubah**.\n",
    "- Teknik balancing non-semantic:\n",
    "  - SVM: `class_weight='balanced'`\n",
    "  - NB: **duplicate oversampling**\n",
    "  - LSTM: **weighted loss**\n",
    "  - BERT/DistilBERT: **weighted loss** via `WeightedTrainer`\n",
    "\n",
    "### Skenario 3 — Penanganan imbalance **dengan augmentasi teks**\n",
    "- Target rating 1–5.\n",
    "- Augment kelas minoritas pada train.\n",
    "- Strategi utama: **EDA** dan **Modified EDA**  \n",
    "  (opsional: Backtranslation, BERT augmentation).\n",
    "- 5 model dilatih ulang.\n",
    "\n",
    "## Catatan Google Colab\n",
    "- Aktifkan GPU: **Runtime → Change runtime type → GPU**\n",
    "- Untuk mengurangi risiko stuck `*.safetensors`, helper transformer di notebook ini\n",
    "  **default mencoba mematikan safetensors**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d89deb5",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "064137e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cpu\n"
     ]
    }
   ],
   "source": [
    "## 0. Setup & Library\n",
    "\n",
    "# !pip -q install pandas numpy scikit-learn matplotlib nltk tqdm\n",
    "# !pip -q install torch transformers datasets accelerate\n",
    "# !pip -q install nlpaug\n",
    "\n",
    "import os, re, glob, random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, f1_score, accuracy_score, confusion_matrix\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "import torch\n",
    "\n",
    "# Reduce TensorFlow/Flax side-effects in Colab\n",
    "os.environ[\"TRANSFORMERS_NO_TF\"] = \"1\"\n",
    "os.environ[\"TRANSFORMERS_NO_FLAX\"] = \"1\"\n",
    "\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "HAS_GPU = torch.cuda.is_available()\n",
    "DEVICE = \"cuda\" if HAS_GPU else \"cpu\"\n",
    "print(\"Device:\", DEVICE)\n",
    "if HAS_GPU:\n",
    "    print(\"GPU:\", torch.cuda.get_device_name(0))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bd758cc",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "74c15aa6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using DATA_PATH: dataset/reviews.csv\n",
      "Shape: (1100, 6)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>business_name</th>\n",
       "      <th>author_name</th>\n",
       "      <th>text</th>\n",
       "      <th>photo</th>\n",
       "      <th>rating</th>\n",
       "      <th>rating_category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Haci'nin Yeri - Yigit Lokantasi</td>\n",
       "      <td>Gulsum Akar</td>\n",
       "      <td>We went to Marmaris with my wife for a holiday...</td>\n",
       "      <td>dataset/taste/hacinin_yeri_gulsum_akar.png</td>\n",
       "      <td>5</td>\n",
       "      <td>taste</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Haci'nin Yeri - Yigit Lokantasi</td>\n",
       "      <td>Oguzhan Cetin</td>\n",
       "      <td>During my holiday in Marmaris we ate here to f...</td>\n",
       "      <td>dataset/menu/hacinin_yeri_oguzhan_cetin.png</td>\n",
       "      <td>4</td>\n",
       "      <td>menu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Haci'nin Yeri - Yigit Lokantasi</td>\n",
       "      <td>Yasin Kuyu</td>\n",
       "      <td>Prices are very affordable. The menu in the ph...</td>\n",
       "      <td>dataset/outdoor_atmosphere/hacinin_yeri_yasin_...</td>\n",
       "      <td>3</td>\n",
       "      <td>outdoor_atmosphere</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Haci'nin Yeri - Yigit Lokantasi</td>\n",
       "      <td>Orhan Kapu</td>\n",
       "      <td>Turkey's cheapest artisan restaurant and its f...</td>\n",
       "      <td>dataset/indoor_atmosphere/hacinin_yeri_orhan_k...</td>\n",
       "      <td>5</td>\n",
       "      <td>indoor_atmosphere</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Haci'nin Yeri - Yigit Lokantasi</td>\n",
       "      <td>Ozgur Sati</td>\n",
       "      <td>I don't know what you will look for in terms o...</td>\n",
       "      <td>dataset/menu/hacinin_yeri_ozgur_sati.png</td>\n",
       "      <td>3</td>\n",
       "      <td>menu</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     business_name    author_name  \\\n",
       "0  Haci'nin Yeri - Yigit Lokantasi    Gulsum Akar   \n",
       "1  Haci'nin Yeri - Yigit Lokantasi  Oguzhan Cetin   \n",
       "2  Haci'nin Yeri - Yigit Lokantasi     Yasin Kuyu   \n",
       "3  Haci'nin Yeri - Yigit Lokantasi     Orhan Kapu   \n",
       "4  Haci'nin Yeri - Yigit Lokantasi     Ozgur Sati   \n",
       "\n",
       "                                                text  \\\n",
       "0  We went to Marmaris with my wife for a holiday...   \n",
       "1  During my holiday in Marmaris we ate here to f...   \n",
       "2  Prices are very affordable. The menu in the ph...   \n",
       "3  Turkey's cheapest artisan restaurant and its f...   \n",
       "4  I don't know what you will look for in terms o...   \n",
       "\n",
       "                                               photo  rating  \\\n",
       "0         dataset/taste/hacinin_yeri_gulsum_akar.png       5   \n",
       "1        dataset/menu/hacinin_yeri_oguzhan_cetin.png       4   \n",
       "2  dataset/outdoor_atmosphere/hacinin_yeri_yasin_...       3   \n",
       "3  dataset/indoor_atmosphere/hacinin_yeri_orhan_k...       5   \n",
       "4           dataset/menu/hacinin_yeri_ozgur_sati.png       3   \n",
       "\n",
       "      rating_category  \n",
       "0               taste  \n",
       "1                menu  \n",
       "2  outdoor_atmosphere  \n",
       "3   indoor_atmosphere  \n",
       "4                menu  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## 1. Load Dataset\n",
    "\n",
    "DATA_PATH = \"dataset/reviews.csv\"  # <-- ganti sesuai file Anda\n",
    "\n",
    "if not os.path.exists(DATA_PATH):\n",
    "    candidates = glob.glob(\"**/*.csv\", recursive=True)\n",
    "    preferred = [c for c in candidates if re.search(r\"(review|google|maps|restaurant)\", c, re.I)]\n",
    "    if preferred:\n",
    "        DATA_PATH = preferred[0]\n",
    "    elif candidates:\n",
    "        DATA_PATH = candidates[0]\n",
    "\n",
    "print(\"Using DATA_PATH:\", DATA_PATH)\n",
    "\n",
    "df = pd.read_csv(DATA_PATH)\n",
    "print(\"Shape:\", df.shape)\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66c0da54",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. Cleaning & Text Preprocessing (Lemmatization)\n",
    "Preprocessing:\n",
    "- normalisasi spasi\n",
    "- lowercasing\n",
    "- pembersihan karakter non-alfabet sederhana\n",
    "- tokenisasi\n",
    "- **lemmatization** WordNet dengan POS tagger."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "46949cdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\LENOVO\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\LENOVO\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After cleaning: (1100, 9)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>business_name</th>\n",
       "      <th>author_name</th>\n",
       "      <th>text</th>\n",
       "      <th>photo</th>\n",
       "      <th>rating</th>\n",
       "      <th>rating_category</th>\n",
       "      <th>text_basic</th>\n",
       "      <th>text_lemma</th>\n",
       "      <th>text_clean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Haci'nin Yeri - Yigit Lokantasi</td>\n",
       "      <td>Gulsum Akar</td>\n",
       "      <td>We went to Marmaris with my wife for a holiday...</td>\n",
       "      <td>dataset/taste/hacinin_yeri_gulsum_akar.png</td>\n",
       "      <td>5</td>\n",
       "      <td>taste</td>\n",
       "      <td>We went to Marmaris with my wife for a holiday...</td>\n",
       "      <td>we go to marmaris with my wife for a holiday w...</td>\n",
       "      <td>we go to marmaris with my wife for a holiday w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Haci'nin Yeri - Yigit Lokantasi</td>\n",
       "      <td>Oguzhan Cetin</td>\n",
       "      <td>During my holiday in Marmaris we ate here to f...</td>\n",
       "      <td>dataset/menu/hacinin_yeri_oguzhan_cetin.png</td>\n",
       "      <td>4</td>\n",
       "      <td>menu</td>\n",
       "      <td>During my holiday in Marmaris we ate here to f...</td>\n",
       "      <td>during my holiday in marmaris we eat here to f...</td>\n",
       "      <td>during my holiday in marmaris we eat here to f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Haci'nin Yeri - Yigit Lokantasi</td>\n",
       "      <td>Yasin Kuyu</td>\n",
       "      <td>Prices are very affordable. The menu in the ph...</td>\n",
       "      <td>dataset/outdoor_atmosphere/hacinin_yeri_yasin_...</td>\n",
       "      <td>3</td>\n",
       "      <td>outdoor_atmosphere</td>\n",
       "      <td>Prices are very affordable. The menu in the ph...</td>\n",
       "      <td>price be very affordable the menu in the photo...</td>\n",
       "      <td>price be very affordable the menu in the photo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Haci'nin Yeri - Yigit Lokantasi</td>\n",
       "      <td>Orhan Kapu</td>\n",
       "      <td>Turkey's cheapest artisan restaurant and its f...</td>\n",
       "      <td>dataset/indoor_atmosphere/hacinin_yeri_orhan_k...</td>\n",
       "      <td>5</td>\n",
       "      <td>indoor_atmosphere</td>\n",
       "      <td>Turkey's cheapest artisan restaurant and its f...</td>\n",
       "      <td>turkey s cheap artisan restaurant and it food ...</td>\n",
       "      <td>turkey s cheap artisan restaurant and it food ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Haci'nin Yeri - Yigit Lokantasi</td>\n",
       "      <td>Ozgur Sati</td>\n",
       "      <td>I don't know what you will look for in terms o...</td>\n",
       "      <td>dataset/menu/hacinin_yeri_ozgur_sati.png</td>\n",
       "      <td>3</td>\n",
       "      <td>menu</td>\n",
       "      <td>I don't know what you will look for in terms o...</td>\n",
       "      <td>i don t know what you will look for in term of...</td>\n",
       "      <td>i don t know what you will look for in term of...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     business_name    author_name  \\\n",
       "0  Haci'nin Yeri - Yigit Lokantasi    Gulsum Akar   \n",
       "1  Haci'nin Yeri - Yigit Lokantasi  Oguzhan Cetin   \n",
       "2  Haci'nin Yeri - Yigit Lokantasi     Yasin Kuyu   \n",
       "3  Haci'nin Yeri - Yigit Lokantasi     Orhan Kapu   \n",
       "4  Haci'nin Yeri - Yigit Lokantasi     Ozgur Sati   \n",
       "\n",
       "                                                text  \\\n",
       "0  We went to Marmaris with my wife for a holiday...   \n",
       "1  During my holiday in Marmaris we ate here to f...   \n",
       "2  Prices are very affordable. The menu in the ph...   \n",
       "3  Turkey's cheapest artisan restaurant and its f...   \n",
       "4  I don't know what you will look for in terms o...   \n",
       "\n",
       "                                               photo  rating  \\\n",
       "0         dataset/taste/hacinin_yeri_gulsum_akar.png       5   \n",
       "1        dataset/menu/hacinin_yeri_oguzhan_cetin.png       4   \n",
       "2  dataset/outdoor_atmosphere/hacinin_yeri_yasin_...       3   \n",
       "3  dataset/indoor_atmosphere/hacinin_yeri_orhan_k...       5   \n",
       "4           dataset/menu/hacinin_yeri_ozgur_sati.png       3   \n",
       "\n",
       "      rating_category                                         text_basic  \\\n",
       "0               taste  We went to Marmaris with my wife for a holiday...   \n",
       "1                menu  During my holiday in Marmaris we ate here to f...   \n",
       "2  outdoor_atmosphere  Prices are very affordable. The menu in the ph...   \n",
       "3   indoor_atmosphere  Turkey's cheapest artisan restaurant and its f...   \n",
       "4                menu  I don't know what you will look for in terms o...   \n",
       "\n",
       "                                          text_lemma  \\\n",
       "0  we go to marmaris with my wife for a holiday w...   \n",
       "1  during my holiday in marmaris we eat here to f...   \n",
       "2  price be very affordable the menu in the photo...   \n",
       "3  turkey s cheap artisan restaurant and it food ...   \n",
       "4  i don t know what you will look for in term of...   \n",
       "\n",
       "                                          text_clean  \n",
       "0  we go to marmaris with my wife for a holiday w...  \n",
       "1  during my holiday in marmaris we eat here to f...  \n",
       "2  price be very affordable the menu in the photo...  \n",
       "3  turkey s cheap artisan restaurant and it food ...  \n",
       "4  i don t know what you will look for in term of...  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "resources = [\n",
    "    (\"tokenizers/punkt\", \"punkt\"),\n",
    "    (\"taggers/averaged_perceptron_tagger\", \"averaged_perceptron_tagger\"),\n",
    "    (\"taggers/averaged_perceptron_tagger_eng\", \"averaged_perceptron_tagger_eng\"),\n",
    "    (\"corpora/wordnet\", \"wordnet\"),\n",
    "    (\"corpora/omw-1.4\", \"omw-1.4\"),\n",
    "]\n",
    "\n",
    "for path, name in resources:\n",
    "    try:\n",
    "        nltk.data.find(path)\n",
    "    except LookupError:\n",
    "        nltk.download(name)\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def penn_to_wn(tag):\n",
    "    if tag.startswith(\"J\"): return wordnet.ADJ\n",
    "    if tag.startswith(\"V\"): return wordnet.VERB\n",
    "    if tag.startswith(\"N\"): return wordnet.NOUN\n",
    "    if tag.startswith(\"R\"): return wordnet.ADV\n",
    "    return wordnet.NOUN\n",
    "\n",
    "def basic_normalize(text: str) -> str:\n",
    "    text = str(text).strip()\n",
    "    text = re.sub(r\"\\s+\", \" \", text)\n",
    "    return text\n",
    "\n",
    "def clean_for_lemma(text: str) -> str:\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"[^a-z\\s]\", \" \", text)\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "    return text\n",
    "\n",
    "def lemmatize_text(text: str) -> str:\n",
    "    text = clean_for_lemma(basic_normalize(text))\n",
    "    if not text:\n",
    "        return \"\"\n",
    "    toks = word_tokenize(text)\n",
    "    pos_tags = nltk.pos_tag(toks)\n",
    "    lemmas = [lemmatizer.lemmatize(w, penn_to_wn(t)) for w, t in pos_tags]\n",
    "    return \" \".join(lemmas)\n",
    "\n",
    "assert \"text\" in df.columns, \"Kolom 'text' tidak ditemukan.\"\n",
    "assert \"rating\" in df.columns, \"Kolom 'rating' tidak ditemukan.\"\n",
    "\n",
    "df = df.dropna(subset=[\"text\", \"rating\"]).copy()\n",
    "df[\"rating\"] = df[\"rating\"].astype(int)\n",
    "df = df[df[\"rating\"].between(1, 5)]\n",
    "\n",
    "df[\"text_basic\"] = df[\"text\"].apply(basic_normalize)\n",
    "df[\"text_lemma\"] = df[\"text\"].apply(lemmatize_text)\n",
    "df[\"text_clean\"] = df[\"text_lemma\"]\n",
    "\n",
    "print(\"After cleaning:\", df.shape)\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc606ac8",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. Exploratory Data Analysis (EDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "91b3998b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1100 entries, 0 to 1099\n",
      "Data columns (total 9 columns):\n",
      " #   Column           Non-Null Count  Dtype \n",
      "---  ------           --------------  ----- \n",
      " 0   business_name    1100 non-null   object\n",
      " 1   author_name      1100 non-null   object\n",
      " 2   text             1100 non-null   object\n",
      " 3   photo            1100 non-null   object\n",
      " 4   rating           1100 non-null   int64 \n",
      " 5   rating_category  1100 non-null   object\n",
      " 6   text_basic       1100 non-null   object\n",
      " 7   text_lemma       1100 non-null   object\n",
      " 8   text_clean       1100 non-null   object\n",
      "dtypes: int64(1), object(8)\n",
      "memory usage: 77.5+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "57e648d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "rating\n",
       "1     80\n",
       "2     72\n",
       "3    172\n",
       "4    316\n",
       "5    460\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rating_counts = df[\"rating\"].value_counts().sort_index()\n",
    "rating_counts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "836493ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure()\n",
    "plt.bar(rating_counts.index.astype(str), rating_counts.values)\n",
    "plt.title(\"Distribusi Rating (1–5)\")\n",
    "plt.xlabel(\"Rating\")\n",
    "plt.ylabel(\"Jumlah Ulasan\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c6f3060d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>char_len</th>\n",
       "      <th>word_len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1100.000000</td>\n",
       "      <td>1100.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>110.827273</td>\n",
       "      <td>20.051818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>69.144698</td>\n",
       "      <td>12.977619</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>5.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>62.000000</td>\n",
       "      <td>11.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>104.000000</td>\n",
       "      <td>19.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>147.000000</td>\n",
       "      <td>27.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>914.000000</td>\n",
       "      <td>179.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          char_len     word_len\n",
       "count  1100.000000  1100.000000\n",
       "mean    110.827273    20.051818\n",
       "std      69.144698    12.977619\n",
       "min       5.000000     1.000000\n",
       "25%      62.000000    11.000000\n",
       "50%     104.000000    19.000000\n",
       "75%     147.000000    27.000000\n",
       "max     914.000000   179.000000"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"char_len\"] = df[\"text_basic\"].str.len()\n",
    "df[\"word_len\"] = df[\"text_basic\"].str.split().apply(len)\n",
    "df[[\"char_len\", \"word_len\"]].describe()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e49ea564",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure()\n",
    "plt.hist(df[\"word_len\"], bins=50)\n",
    "plt.title(\"Distribusi Panjang Ulasan (Jumlah Kata)\")\n",
    "plt.xlabel(\"Jumlah kata\")\n",
    "plt.ylabel(\"Frekuensi\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f69ab82",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. Train/Test Split (Konsisten)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ef6e0e42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 880 Test: 220\n",
      "Distribusi Train: Counter({np.int64(5): 368, np.int64(4): 253, np.int64(3): 137, np.int64(1): 64, np.int64(2): 58})\n",
      "Distribusi Test : Counter({np.int64(5): 92, np.int64(4): 63, np.int64(3): 35, np.int64(1): 16, np.int64(2): 14})\n"
     ]
    }
   ],
   "source": [
    "X = df[\"text_clean\"].values\n",
    "y = df[\"rating\"].values\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y,\n",
    "    test_size=0.2,\n",
    "    random_state=SEED,\n",
    "    stratify=y\n",
    ")\n",
    "\n",
    "print(\"Train:\", len(X_train), \"Test:\", len(X_test))\n",
    "print(\"Distribusi Train:\", Counter(y_train))\n",
    "print(\"Distribusi Test :\", Counter(y_test))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b556b425",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. Utilitas Evaluasi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "2d12c322",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_report(y_true, y_pred, title=\"Model\"):\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    f1m = f1_score(y_true, y_pred, average=\"macro\")\n",
    "    f1w = f1_score(y_true, y_pred, average=\"weighted\")\n",
    "\n",
    "    print(f\"=== {title} ===\")\n",
    "    print(\"Accuracy :\", round(acc, 4))\n",
    "    print(\"F1-macro :\", round(f1m, 4))\n",
    "    print(\"F1-weight:\", round(f1w, 4))\n",
    "    print()\n",
    "    print(classification_report(y_true, y_pred, digits=4, zero_division=0))\n",
    "    print()\n",
    "    return {\"model\": title, \"accuracy\": acc, \"f1_macro\": f1m, \"f1_weighted\": f1w}\n",
    "\n",
    "def plot_confusion(y_true, y_pred, labels=range(1, 6), title=\"Confusion Matrix\"):\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=list(labels))\n",
    "    plt.figure()\n",
    "    plt.imshow(cm, interpolation=\"nearest\")\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(labels))\n",
    "    plt.xticks(tick_marks, labels)\n",
    "    plt.yticks(tick_marks, labels)\n",
    "    plt.ylabel(\"True\")\n",
    "    plt.xlabel(\"Predicted\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e7b60ff",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6. Helper Model Klasik (TF–IDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "8559defe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_tfidf(train_texts, test_texts):\n",
    "    vec = TfidfVectorizer(lowercase=True, max_features=50000, ngram_range=(1, 2))\n",
    "    Xtr = vec.fit_transform(train_texts)\n",
    "    Xte = vec.transform(test_texts)\n",
    "    return vec, Xtr, Xte\n",
    "\n",
    "def train_eval_svm(train_texts, train_labels, test_texts, test_labels, suffix=\"\", class_weight=None):\n",
    "    vec, Xtr, Xte = fit_tfidf(train_texts, test_texts)\n",
    "    svm = LinearSVC(class_weight=class_weight)\n",
    "    svm.fit(Xtr, train_labels)\n",
    "    pred = svm.predict(Xte)\n",
    "    res = eval_report(test_labels, pred, f\"Linear SVM{suffix}\")\n",
    "    return res, vec, svm, pred\n",
    "\n",
    "def train_eval_nb(train_texts, train_labels, test_texts, test_labels, suffix=\"\"):\n",
    "    vec, Xtr, Xte = fit_tfidf(train_texts, test_texts)\n",
    "    nb = MultinomialNB()\n",
    "    nb.fit(Xtr, train_labels)\n",
    "    pred = nb.predict(Xte)\n",
    "    res = eval_report(test_labels, pred, f\"Naive Bayes{suffix}\")\n",
    "    return res, vec, nb, pred\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1fa291c",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 7. Duplicate Oversampling (Non-Aug, untuk S2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d30320dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train original: Counter({np.int64(5): 368, np.int64(4): 253, np.int64(3): 137, np.int64(1): 64, np.int64(2): 58})\n",
      "Train after duplicate oversampling (preview): Counter({np.int64(4): 368, np.int64(5): 368, np.int64(2): 368, np.int64(3): 368, np.int64(1): 368})\n"
     ]
    }
   ],
   "source": [
    "def duplicate_oversample(texts, labels):\n",
    "    texts, labels = list(texts), list(labels)\n",
    "    counts = Counter(labels)\n",
    "    max_count = max(counts.values())\n",
    "\n",
    "    idx_by = {c: [] for c in counts}\n",
    "    for i, y in enumerate(labels):\n",
    "        idx_by[y].append(i)\n",
    "\n",
    "    new_texts, new_labels = texts[:], labels[:]\n",
    "\n",
    "    for c, idxs in idx_by.items():\n",
    "        need = max_count - counts[c]\n",
    "        if need <= 0:\n",
    "            continue\n",
    "        for _ in range(need):\n",
    "            i = random.choice(idxs)\n",
    "            new_texts.append(texts[i])   # teks sama persis\n",
    "            new_labels.append(c)\n",
    "\n",
    "    return np.array(new_texts), np.array(new_labels)\n",
    "\n",
    "print(\"Train original:\", Counter(y_train))\n",
    "Xt_dup_preview, yt_dup_preview = duplicate_oversample(X_train, y_train)\n",
    "print(\"Train after duplicate oversampling (preview):\", Counter(yt_dup_preview))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "153529c0",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 8. Helper LSTM (PyTorch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "3e4063ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "def build_vocab(texts, max_vocab=30000, min_freq=2):\n",
    "    from collections import Counter as CCounter\n",
    "    counter = CCounter()\n",
    "    for t in texts:\n",
    "        counter.update(str(t).split())\n",
    "    vocab = {\"<pad>\": 0, \"<unk>\": 1}\n",
    "    for w, c in counter.most_common():\n",
    "        if c < min_freq:\n",
    "            continue\n",
    "        if len(vocab) >= max_vocab:\n",
    "            break\n",
    "        vocab[w] = len(vocab)\n",
    "    return vocab\n",
    "\n",
    "def encode(text, vocab, max_len=200):\n",
    "    tokens = str(text).split()\n",
    "    ids = [vocab.get(t, vocab[\"<unk>\"]) for t in tokens[:max_len]]\n",
    "    if len(ids) < max_len:\n",
    "        ids += [vocab[\"<pad>\"]] * (max_len - len(ids))\n",
    "    return ids\n",
    "\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, texts, labels, vocab, max_len=200):\n",
    "        self.texts = list(texts)\n",
    "        self.labels = list(labels)\n",
    "        self.vocab = vocab\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x = torch.tensor(encode(self.texts[idx], self.vocab, self.max_len), dtype=torch.long)\n",
    "        y = torch.tensor(int(self.labels[idx]) - 1, dtype=torch.long)  # 1-5 -> 0-4\n",
    "        return x, y\n",
    "\n",
    "class LSTMClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size, emb_dim=128, hidden_dim=128, num_classes=5):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, emb_dim, padding_idx=0)\n",
    "        self.lstm = nn.LSTM(emb_dim, hidden_dim, batch_first=True, bidirectional=True)\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        self.fc = nn.Linear(hidden_dim * 2, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        emb = self.embedding(x)\n",
    "        out, _ = self.lstm(emb)\n",
    "        last = out[:, -1, :]\n",
    "        last = self.dropout(last)\n",
    "        return self.fc(last)\n",
    "\n",
    "def train_eval_lstm(\n",
    "    train_texts, train_labels, test_texts, test_labels,\n",
    "    epochs=3, max_len=200, batch_size=64, suffix=\"\",\n",
    "    use_class_weights=False\n",
    "):\n",
    "    vocab = build_vocab(train_texts)\n",
    "    train_ds = TextDataset(train_texts, train_labels, vocab, max_len)\n",
    "    test_ds  = TextDataset(test_texts, test_labels, vocab, max_len)\n",
    "\n",
    "    train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n",
    "    test_loader  = DataLoader(test_ds, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    model = LSTMClassifier(len(vocab)).to(DEVICE)\n",
    "\n",
    "    if use_class_weights:\n",
    "        classes = np.array([1,2,3,4,5])\n",
    "        cw = compute_class_weight(class_weight=\"balanced\", classes=classes, y=np.array(train_labels))\n",
    "        class_weights = torch.tensor(cw, dtype=torch.float).to(DEVICE)\n",
    "        criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
    "    else:\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "    def train_one_epoch():\n",
    "        model.train()\n",
    "        total = 0.0\n",
    "        for xb, yb in train_loader:\n",
    "            xb, yb = xb.to(DEVICE), yb.to(DEVICE)\n",
    "            optimizer.zero_grad()\n",
    "            logits = model(xb)\n",
    "            loss = criterion(logits, yb)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total += float(loss.item())\n",
    "        return total / max(1, len(train_loader))\n",
    "\n",
    "    def predict():\n",
    "        model.eval()\n",
    "        preds, trues = [], []\n",
    "        with torch.no_grad():\n",
    "            for xb, yb in test_loader:\n",
    "                xb = xb.to(DEVICE)\n",
    "                logits = model(xb)\n",
    "                p = torch.argmax(logits, dim=1).cpu().numpy()\n",
    "                preds.extend(list(p))\n",
    "                trues.extend(list(yb.numpy()))\n",
    "        preds = np.array(preds) + 1\n",
    "        trues = np.array(trues) + 1\n",
    "        return trues, preds\n",
    "\n",
    "    for ep in range(epochs):\n",
    "        loss = train_one_epoch()\n",
    "        print(f\"[LSTM{suffix}] Epoch {ep+1}/{epochs} | loss={loss:.4f}\")\n",
    "\n",
    "    yt, yp = predict()\n",
    "    return eval_report(yt, yp, f\"LSTM{suffix}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e72a3929",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 9. Helper Transformer (BERT & DistilBERT) — WeightedTrainer + Output Seragam\n",
    "Tanpa dependency `evaluate` (biar aman di Colab)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "5c513a06",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import EarlyStoppingCallback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "100e6848",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    "\n",
    "DEFAULT_MAX_LEN = 256 if HAS_GPU else 128\n",
    "DEFAULT_BATCH = 8 if HAS_GPU else 2\n",
    "DEFAULT_EPOCHS = 3\n",
    "\n",
    "USE_SMALL_TRAIN_IF_CPU = True\n",
    "SMALL_TRAIN_N = 2000\n",
    "\n",
    "def make_hf_dataset(texts, labels):\n",
    "    return Dataset.from_dict({\"text\": list(texts), \"label\": [int(l) - 1 for l in labels]})\n",
    "\n",
    "def make_tokenize_fn(tokenizer, max_length):\n",
    "    def _tok(batch):\n",
    "        return tokenizer(batch[\"text\"], truncation=True, padding=\"max_length\", max_length=max_length)\n",
    "    return _tok\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    preds = np.argmax(logits, axis=-1)\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    f1m = f1_score(labels, preds, average=\"macro\")\n",
    "    f1w = f1_score(labels, preds, average=\"weighted\")\n",
    "    return {\"accuracy\": acc, \"f1_macro\": f1m, \"f1_weighted\": f1w}\n",
    "\n",
    "def maybe_shrink_train(train_texts, train_labels):\n",
    "    if (not HAS_GPU) and USE_SMALL_TRAIN_IF_CPU and len(train_texts) > SMALL_TRAIN_N:\n",
    "        return train_texts[:SMALL_TRAIN_N], train_labels[:SMALL_TRAIN_N]\n",
    "    return train_texts, train_labels\n",
    "\n",
    "def build_training_args(out_dir, epochs, batch_size):\n",
    "    common = dict(\n",
    "        output_dir=out_dir,\n",
    "        save_strategy=\"epoch\",\n",
    "        learning_rate=2e-5,\n",
    "        per_device_train_batch_size=batch_size,\n",
    "        per_device_eval_batch_size=batch_size,\n",
    "        num_train_epochs=epochs,\n",
    "        weight_decay=0.01,\n",
    "        logging_steps=50,\n",
    "        report_to=\"none\",\n",
    "        seed=SEED,\n",
    "        fp16=HAS_GPU,\n",
    "        dataloader_num_workers=0,\n",
    "\n",
    "        evaluation_strategy=\"epoch\",\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model=\"f1_macro\",\n",
    "        greater_is_better=True,\n",
    "    )\n",
    "    try:\n",
    "        return TrainingArguments(**common)\n",
    "    except TypeError:\n",
    "        # fallback for newer API naming\n",
    "        common.pop(\"evaluation_strategy\", None)\n",
    "        common[\"eval_strategy\"] = \"epoch\"\n",
    "        return TrainingArguments(**common)\n",
    "\n",
    "\n",
    "class WeightedTrainer(Trainer):\n",
    "    def __init__(self, class_weights=None, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.class_weights = class_weights\n",
    "\n",
    "    def compute_loss(self, model, inputs, return_outputs=False, **kwargs):\n",
    "        inputs = dict(inputs)\n",
    "        labels = inputs.get(\"labels\")\n",
    "\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.get(\"logits\")\n",
    "\n",
    "        if self.class_weights is not None:\n",
    "            cw = self.class_weights.to(logits.device)\n",
    "            loss_fct = torch.nn.CrossEntropyLoss(weight=cw)\n",
    "        else:\n",
    "            loss_fct = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "        loss = loss_fct(logits.view(-1, model.config.num_labels), labels.view(-1))\n",
    "        return (loss, outputs) if return_outputs else loss\n",
    "\n",
    "def _build_trainer(model, args, train_tok, test_tok, tokenizer, use_class_weights, class_weights):\n",
    "    base_kwargs = dict(\n",
    "        model=model,\n",
    "        args=args,\n",
    "        train_dataset=train_tok,\n",
    "        eval_dataset=test_tok,\n",
    "        compute_metrics=compute_metrics,\n",
    "    )\n",
    "\n",
    "    if use_class_weights:\n",
    "        cls = WeightedTrainer\n",
    "        base_kwargs[\"class_weights\"] = class_weights\n",
    "    else:\n",
    "        cls = Trainer\n",
    "\n",
    "    callbacks = [EarlyStoppingCallback(early_stopping_patience=1)]\n",
    "\n",
    "    try:\n",
    "        return cls(**base_kwargs, tokenizer=tokenizer, callbacks=callbacks)\n",
    "    except TypeError:\n",
    "        return cls(**base_kwargs, processing_class=tokenizer, callbacks=callbacks)\n",
    "\n",
    "\n",
    "def safe_load_sequence_classifier(model_name, num_labels=5):\n",
    "    # Try avoiding safetensors when supported\n",
    "    try:\n",
    "        return AutoModelForSequenceClassification.from_pretrained(\n",
    "            model_name, num_labels=num_labels, use_safetensors=False\n",
    "        )\n",
    "    except TypeError:\n",
    "        return AutoModelForSequenceClassification.from_pretrained(\n",
    "            model_name, num_labels=num_labels\n",
    "        )\n",
    "\n",
    "def train_eval_transformer(\n",
    "    model_name,\n",
    "    train_texts, train_labels,\n",
    "    test_texts, test_labels,\n",
    "    epochs=DEFAULT_EPOCHS, batch_size=DEFAULT_BATCH,\n",
    "    max_length=DEFAULT_MAX_LEN, out_dir=\"./tmp_tr\", suffix=\"\",\n",
    "    use_class_weights=False\n",
    "):\n",
    "    train_texts2, train_labels2 = maybe_shrink_train(train_texts, train_labels)\n",
    "\n",
    "    train_hf = make_hf_dataset(train_texts2, train_labels2)\n",
    "    test_hf  = make_hf_dataset(test_texts, test_labels)\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = safe_load_sequence_classifier(model_name, num_labels=5)\n",
    "\n",
    "    tok_fn = make_tokenize_fn(tokenizer, max_length=max_length)\n",
    "    train_tok = train_hf.map(tok_fn, batched=True)\n",
    "    test_tok  = test_hf.map(tok_fn, batched=True)\n",
    "\n",
    "    train_tok = train_tok.remove_columns([\"text\"]).rename_column(\"label\", \"labels\")\n",
    "    test_tok  = test_tok.remove_columns([\"text\"]).rename_column(\"label\", \"labels\")\n",
    "\n",
    "    args = build_training_args(out_dir, epochs, batch_size)\n",
    "\n",
    "    class_weights = None\n",
    "    if use_class_weights:\n",
    "        classes = np.array([1,2,3,4,5])\n",
    "        cw = compute_class_weight(class_weight=\"balanced\", classes=classes, y=np.array(train_labels2))\n",
    "        class_weights = torch.tensor(cw, dtype=torch.float)\n",
    "\n",
    "    trainer = _build_trainer(\n",
    "        model=model,\n",
    "        args=args,\n",
    "        train_tok=train_tok,\n",
    "        test_tok=test_tok,\n",
    "        tokenizer=tokenizer,\n",
    "        use_class_weights=use_class_weights,\n",
    "        class_weights=class_weights\n",
    "    )\n",
    "\n",
    "    trainer.train()\n",
    "    res = trainer.evaluate()\n",
    "\n",
    "    out = {\n",
    "        \"model\": model_name + suffix,\n",
    "        \"accuracy\": res.get(\"eval_accuracy\"),\n",
    "        \"f1_macro\": res.get(\"eval_f1_macro\"),\n",
    "        \"f1_weighted\": res.get(\"eval_f1_weighted\"),\n",
    "        \"eval_loss\": res.get(\"eval_loss\"),\n",
    "        \"epoch\": res.get(\"epoch\"),\n",
    "    }\n",
    "\n",
    "    print(f\"=== {model_name}{suffix} ===\")\n",
    "    print(res)\n",
    "    print()\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d125c67d",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 10. Augmentasi Kelas Minoritas (Helper S3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "56a3ab83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distribusi train original: Counter({np.int64(5): 368, np.int64(4): 253, np.int64(3): 137, np.int64(1): 64, np.int64(2): 58})\n",
      "AUG_STRATEGIES: ['eda', 'modified_eda', 'backtranslation', 'bert']\n"
     ]
    }
   ],
   "source": [
    "# EDA-style augmentation helpers\n",
    "def get_synonyms(word):\n",
    "    syns = set()\n",
    "    for syn in wordnet.synsets(word):\n",
    "        for lemma in syn.lemmas():\n",
    "            w = lemma.name().replace(\"_\", \" \").lower()\n",
    "            if w != word.lower():\n",
    "                syns.add(w)\n",
    "    return list(syns)\n",
    "\n",
    "def random_deletion(words, p=0.1):\n",
    "    if len(words) == 1:\n",
    "        return words\n",
    "    new_words = [w for w in words if random.random() > p]\n",
    "    return new_words if new_words else [random.choice(words)]\n",
    "\n",
    "def random_swap(words, n=1):\n",
    "    new_words = words[:]\n",
    "    for _ in range(n):\n",
    "        if len(new_words) < 2:\n",
    "            break\n",
    "        i, j = random.sample(range(len(new_words)), 2)\n",
    "        new_words[i], new_words[j] = new_words[j], new_words[i]\n",
    "    return new_words\n",
    "\n",
    "def random_insertion(words, n=1):\n",
    "    new_words = new_words = words[:]\n",
    "    for _ in range(n):\n",
    "        w = random.choice(new_words)\n",
    "        syns = get_synonyms(w)\n",
    "        if not syns:\n",
    "            continue\n",
    "        new_words.insert(random.randint(0, len(new_words)), random.choice(syns))\n",
    "    return new_words\n",
    "\n",
    "def synonym_replacement(words, n=1):\n",
    "    new_words = words[:]\n",
    "    candidates = [w for w in set(words) if get_synonyms(w)]\n",
    "    random.shuffle(candidates)\n",
    "    num_replaced = 0\n",
    "    for w in candidates:\n",
    "        syns = get_synonyms(w)\n",
    "        if not syns:\n",
    "            continue\n",
    "        new_words = [random.choice(syns) if x == w else x for x in new_words]\n",
    "        num_replaced += 1\n",
    "        if num_replaced >= n:\n",
    "            break\n",
    "    return new_words\n",
    "\n",
    "def eda(text, alpha=0.1, num_aug=1):\n",
    "    words = str(text).split()\n",
    "    if not words:\n",
    "        return [str(text)] * num_aug\n",
    "    n = max(1, int(alpha * len(words)))\n",
    "    augmented = []\n",
    "    for _ in range(num_aug):\n",
    "        choice = random.choice([\"sr\", \"ri\", \"rs\", \"rd\"])\n",
    "        if choice == \"sr\":\n",
    "            w = synonym_replacement(words, n)\n",
    "        elif choice == \"ri\":\n",
    "            w = random_insertion(words, n)\n",
    "        elif choice == \"rs\":\n",
    "            w = random_swap(words, n)\n",
    "        else:\n",
    "            w = random_deletion(words, p=alpha)\n",
    "        augmented.append(\" \".join(w))\n",
    "    return augmented\n",
    "\n",
    "def modified_eda(text, alpha=0.05, num_aug=1):\n",
    "    return eda(text, alpha=alpha, num_aug=num_aug)\n",
    "\n",
    "# Optional heavy methods\n",
    "def backtranslate(text, src_lang=\"en\", mid_lang=\"fr\"):\n",
    "    try:\n",
    "        from transformers import MarianMTModel, MarianTokenizer\n",
    "        model_name_1 = f\"Helsinki-NLP/opus-mt-{src_lang}-{mid_lang}\"\n",
    "        model_name_2 = f\"Helsinki-NLP/opus-mt-{mid_lang}-{src_lang}\"\n",
    "        tok1 = MarianTokenizer.from_pretrained(model_name_1)\n",
    "        mod1 = MarianMTModel.from_pretrained(model_name_1)\n",
    "        tok2 = MarianTokenizer.from_pretrained(model_name_2)\n",
    "        mod2 = MarianMTModel.from_pretrained(model_name_2)\n",
    "\n",
    "        def translate(t, tok, mod):\n",
    "            batch = tok([t], return_tensors=\"pt\", truncation=True, padding=True)\n",
    "            gen = mod.generate(**batch, max_length=256)\n",
    "            return tok.batch_decode(gen, skip_special_tokens=True)[0]\n",
    "\n",
    "        mid = translate(text, tok1, mod1)\n",
    "        back = translate(mid, tok2, mod2)\n",
    "        return back\n",
    "    except Exception:\n",
    "        return text\n",
    "\n",
    "def bert_augment(text, num_aug=1):\n",
    "    try:\n",
    "        import nlpaug.augmenter.word as naw\n",
    "        aug = naw.ContextualWordEmbsAug(model_path=\"bert-base-uncased\", action=\"substitute\")\n",
    "        return aug.augment(text, n=num_aug)\n",
    "    except Exception:\n",
    "        return [text] * num_aug\n",
    "\n",
    "def augment_minority(\n",
    "    texts, labels,\n",
    "    strategy=\"eda\",\n",
    "    target_multiplier=1.0,\n",
    "    max_aug_per_sample=2\n",
    "):\n",
    "    texts = list(texts)\n",
    "    labels = list(labels)\n",
    "\n",
    "    counts = Counter(labels)\n",
    "    max_count = max(counts.values())\n",
    "    target_count = int(max_count * target_multiplier)\n",
    "\n",
    "    new_texts = texts[:]\n",
    "    new_labels = labels[:]\n",
    "\n",
    "    idx_by_class = {c: [] for c in counts}\n",
    "    for i, lab in enumerate(labels):\n",
    "        idx_by_class[lab].append(i)\n",
    "\n",
    "    for c, idxs in idx_by_class.items():\n",
    "        need = max(0, target_count - counts[c])\n",
    "        if need == 0:\n",
    "            continue\n",
    "\n",
    "        pool = idxs[:]\n",
    "        random.shuffle(pool)\n",
    "        ptr = 0\n",
    "\n",
    "        while need > 0 and pool:\n",
    "            i = pool[ptr % len(pool)]\n",
    "            base = texts[i]\n",
    "            k = min(max_aug_per_sample, need)\n",
    "\n",
    "            if strategy == \"eda\":\n",
    "                aug_texts = eda(base, alpha=0.1, num_aug=k)\n",
    "            elif strategy == \"modified_eda\":\n",
    "                aug_texts = modified_eda(base, alpha=0.05, num_aug=k)\n",
    "            elif strategy == \"backtranslation\":\n",
    "                aug_texts = [backtranslate(base) for _ in range(k)]\n",
    "            elif strategy == \"bert\":\n",
    "                aug_texts = bert_augment(base, num_aug=k)\n",
    "            else:\n",
    "                aug_texts = []\n",
    "\n",
    "            for t in aug_texts:\n",
    "                if need <= 0:\n",
    "                    break\n",
    "                new_texts.append(t)\n",
    "                new_labels.append(c)\n",
    "                need -= 1\n",
    "\n",
    "            ptr += 1\n",
    "\n",
    "    return np.array(new_texts), np.array(new_labels)\n",
    "\n",
    "# Default strategies (ringan)\n",
    "# AUG_STRATEGIES = [\"eda\", \"modified_eda\"]\n",
    "# Untuk eksperimen lebih berat, uncomment:\n",
    "AUG_STRATEGIES = [\"eda\", \"modified_eda\", \"backtranslation\", \"bert\"]\n",
    "\n",
    "TARGET_MULTIPLIER = 1.0\n",
    "MAX_AUG_PER_SAMPLE = 2\n",
    "\n",
    "print(\"Distribusi train original:\", Counter(y_train))\n",
    "print(\"AUG_STRATEGIES:\", AUG_STRATEGIES)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e40ba2b1",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Skenario 1 — Baseline (Tanpa Penanganan Imbalance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "9a1b00a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train base: Counter({np.int64(5): 294, np.int64(4): 202, np.int64(3): 110, np.int64(1): 51, np.int64(2): 47})\n",
      "Val       : Counter({np.int64(5): 74, np.int64(4): 51, np.int64(3): 27, np.int64(1): 13, np.int64(2): 11})\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from collections import Counter\n",
    "\n",
    "X_train_base, X_val, y_train_base, y_val = train_test_split(\n",
    "    X_train, y_train,\n",
    "    test_size=0.2,\n",
    "    random_state=SEED,\n",
    "    stratify=y_train\n",
    ")\n",
    "\n",
    "print(\"Train base:\", Counter(y_train_base))\n",
    "print(\"Val       :\", Counter(y_val))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0230f195",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_s1 = []\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aae5965d",
   "metadata": {},
   "source": [
    "## 1.1 Linear SVM (S1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "186d72fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Linear SVM (S1 baseline) ===\n",
      "Accuracy : 0.5\n",
      "F1-macro : 0.3758\n",
      "F1-weight: 0.4734\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1     0.5556    0.3125    0.4000        16\n",
      "           2     0.3333    0.0714    0.1176        14\n",
      "           3     0.3571    0.2857    0.3175        35\n",
      "           4     0.3966    0.3651    0.3802        63\n",
      "           5     0.5820    0.7717    0.6636        92\n",
      "\n",
      "    accuracy                         0.5000       220\n",
      "   macro avg     0.4449    0.3613    0.3758       220\n",
      "weighted avg     0.4754    0.5000    0.4734       220\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "svm_s1_res, _, _, _ = train_eval_svm(\n",
    "    X_train, y_train, X_test, y_test,\n",
    "    suffix=\" (S1 baseline)\",\n",
    "    class_weight=None\n",
    ")\n",
    "results_s1.append(svm_s1_res)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "01c1374e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 24 candidates, totalling 72 fits\n",
      "Best SVM params: {'clf__C': 0.5, 'clf__class_weight': 'balanced', 'tfidf__max_features': 20000, 'tfidf__ngram_range': (1, 1)}\n",
      "Best CV f1_macro: 0.33814555268502694\n",
      "=== SVM tuned (VAL) ===\n",
      "Accuracy : 0.4318\n",
      "F1-macro : 0.3328\n",
      "F1-weight: 0.4205\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1     0.3571    0.3846    0.3704        13\n",
      "           2     0.1250    0.1818    0.1481        11\n",
      "           3     0.2308    0.2222    0.2264        27\n",
      "           4     0.3636    0.2353    0.2857        51\n",
      "           5     0.5862    0.6892    0.6335        74\n",
      "\n",
      "    accuracy                         0.4318       176\n",
      "   macro avg     0.3326    0.3426    0.3328       176\n",
      "weighted avg     0.4214    0.4318    0.4205       176\n",
      "\n",
      "\n",
      "=== SVM tuned (TEST) ===\n",
      "Accuracy : 0.4636\n",
      "F1-macro : 0.3863\n",
      "F1-weight: 0.4591\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1     0.5455    0.3750    0.4444        16\n",
      "           2     0.1579    0.2143    0.1818        14\n",
      "           3     0.3548    0.3143    0.3333        35\n",
      "           4     0.3571    0.3175    0.3361        63\n",
      "           5     0.6019    0.6739    0.6359        92\n",
      "\n",
      "    accuracy                         0.4636       220\n",
      "   macro avg     0.4035    0.3790    0.3863       220\n",
      "weighted avg     0.4602    0.4636    0.4591       220\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV, StratifiedKFold\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "svm_pipe = Pipeline([\n",
    "    (\"tfidf\", TfidfVectorizer(lowercase=True)),\n",
    "    (\"clf\", LinearSVC())\n",
    "])\n",
    "\n",
    "svm_grid = {\n",
    "    \"tfidf__ngram_range\": [(1,1), (1,2)],\n",
    "    \"tfidf__max_features\": [20000, 50000],\n",
    "    \"clf__C\": [0.5, 1.0, 2.0],\n",
    "    \"clf__class_weight\": [None, \"balanced\"],\n",
    "}\n",
    "\n",
    "cv = StratifiedKFold(n_splits=3, shuffle=True, random_state=SEED)\n",
    "\n",
    "svm_gs = GridSearchCV(\n",
    "    svm_pipe,\n",
    "    svm_grid,\n",
    "    scoring=\"f1_macro\",\n",
    "    cv=cv,\n",
    "    n_jobs=1,          # ✅ most stable fix\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "svm_gs.fit(X_train_base, y_train_base)\n",
    "\n",
    "print(\"Best SVM params:\", svm_gs.best_params_)\n",
    "print(\"Best CV f1_macro:\", svm_gs.best_score_)\n",
    "\n",
    "\n",
    "# Validate\n",
    "svm_val_pred = svm_gs.predict(X_val)\n",
    "svm_val_res = eval_report(y_val, svm_val_pred, \"SVM tuned (VAL)\")\n",
    "\n",
    "# Retrain best on full train, test once\n",
    "svm_best = svm_gs.best_estimator_\n",
    "svm_best.fit(X_train, y_train)\n",
    "svm_test_pred = svm_best.predict(X_test)\n",
    "svm_test_res = eval_report(y_test, svm_test_pred, \"SVM tuned (TEST)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fe0318e",
   "metadata": {},
   "source": [
    "## 1.2 Naive Bayes (S1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "ce636951",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Naive Bayes (S1 baseline) ===\n",
      "Accuracy : 0.4364\n",
      "F1-macro : 0.1479\n",
      "F1-weight: 0.2912\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1     0.0000    0.0000    0.0000        16\n",
      "           2     0.0000    0.0000    0.0000        14\n",
      "           3     0.0000    0.0000    0.0000        35\n",
      "           4     0.5000    0.0794    0.1370        63\n",
      "           5     0.4333    0.9891    0.6026        92\n",
      "\n",
      "    accuracy                         0.4364       220\n",
      "   macro avg     0.1867    0.2137    0.1479       220\n",
      "weighted avg     0.3244    0.4364    0.2912       220\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "nb_s1_res, _, _, _ = train_eval_nb(\n",
    "    X_train, y_train, X_test, y_test,\n",
    "    suffix=\" (S1 baseline)\"\n",
    ")\n",
    "results_s1.append(nb_s1_res)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "a0213182",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 5 candidates, totalling 15 fits\n",
      "Best NB params: {'clf__alpha': 0.1}\n",
      "Best CV f1_macro: 0.2384218115536518\n",
      "=== NB tuned (VAL) ===\n",
      "Accuracy : 0.4886\n",
      "F1-macro : 0.2818\n",
      "F1-weight: 0.4361\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1     0.5000    0.0769    0.1333        13\n",
      "           2     0.0000    0.0000    0.0000        11\n",
      "           3     0.4000    0.1481    0.2162        27\n",
      "           4     0.3729    0.4314    0.4000        51\n",
      "           5     0.5619    0.7973    0.6592        74\n",
      "\n",
      "    accuracy                         0.4886       176\n",
      "   macro avg     0.3670    0.2907    0.2818       176\n",
      "weighted avg     0.4426    0.4886    0.4361       176\n",
      "\n",
      "\n",
      "=== NB tuned (TEST) ===\n",
      "Accuracy : 0.4636\n",
      "F1-macro : 0.3028\n",
      "F1-weight: 0.4157\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1     1.0000    0.1250    0.2222        16\n",
      "           2     1.0000    0.0714    0.1333        14\n",
      "           3     0.3333    0.1429    0.2000        35\n",
      "           4     0.3125    0.3175    0.3150        63\n",
      "           5     0.5362    0.8043    0.6435        92\n",
      "\n",
      "    accuracy                         0.4636       220\n",
      "   macro avg     0.6364    0.2922    0.3028       220\n",
      "weighted avg     0.5031    0.4636    0.4157       220\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "nb_pipe = Pipeline([\n",
    "    (\"tfidf\", TfidfVectorizer(lowercase=True, ngram_range=(1,2), max_features=50000)),\n",
    "    (\"clf\", MultinomialNB())\n",
    "])\n",
    "\n",
    "nb_grid = {\n",
    "    \"clf__alpha\": [0.1, 0.3, 0.5, 1.0, 2.0],\n",
    "}\n",
    "\n",
    "nb_gs = GridSearchCV(\n",
    "    nb_pipe,\n",
    "    nb_grid,\n",
    "    scoring=\"f1_macro\",\n",
    "    cv=3,\n",
    "    n_jobs=1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "nb_gs.fit(X_train_base, y_train_base)\n",
    "\n",
    "print(\"Best NB params:\", nb_gs.best_params_)\n",
    "print(\"Best CV f1_macro:\", nb_gs.best_score_)\n",
    "\n",
    "# Validate\n",
    "nb_val_pred = nb_gs.predict(X_val)\n",
    "nb_val_res = eval_report(y_val, nb_val_pred, \"NB tuned (VAL)\")\n",
    "\n",
    "# Retrain best on full train, test once\n",
    "nb_best = nb_gs.best_estimator_\n",
    "nb_best.fit(X_train, y_train)\n",
    "nb_test_pred = nb_best.predict(X_test)\n",
    "nb_test_res = eval_report(y_test, nb_test_pred, \"NB tuned (TEST)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9031566d",
   "metadata": {},
   "source": [
    "## 1.3 LSTM (S1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "a9f094ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "def train_eval_lr(train_texts, train_labels, test_texts, test_labels, suffix=\"\", class_weight=None):\n",
    "    vec, Xtr, Xte = fit_tfidf(train_texts, test_texts)\n",
    "\n",
    "    lr = LogisticRegression(\n",
    "        max_iter=2000,\n",
    "        class_weight=class_weight,\n",
    "        n_jobs=None,        # biar aman di Windows\n",
    "        solver=\"saga\",      # robust utk sparse + multiclass\n",
    "        multi_class=\"multinomial\"\n",
    "    )\n",
    "\n",
    "    lr.fit(Xtr, train_labels)\n",
    "    pred = lr.predict(Xte)\n",
    "    res = eval_report(test_labels, pred, f\"Logistic Regression{suffix}\")\n",
    "    return res, vec, lr, pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9045a6db",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "6c89b667",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Logistic Regression (S1 baseline) ===\n",
      "Accuracy : 0.4818\n",
      "F1-macro : 0.2412\n",
      "F1-weight: 0.4053\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1     0.0000    0.0000    0.0000        16\n",
      "           2     0.0000    0.0000    0.0000        14\n",
      "           3     0.4545    0.1429    0.2174        35\n",
      "           4     0.3519    0.3016    0.3248        63\n",
      "           5     0.5290    0.8913    0.6640        92\n",
      "\n",
      "    accuracy                         0.4818       220\n",
      "   macro avg     0.2671    0.2671    0.2412       220\n",
      "weighted avg     0.3943    0.4818    0.4053       220\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\LENOVO\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1272: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.8. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "lr_s1_res, _, _, _ = train_eval_lr(\n",
    "    X_train, y_train, X_test, y_test,\n",
    "    suffix=\" (S1 baseline)\",\n",
    "    class_weight=None\n",
    ")\n",
    "results_s1.append(lr_s1_res)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e5a7f5f",
   "metadata": {},
   "source": [
    "## 1.4 BERT (S1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "03a75e4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c27db472135f456497634277fb411142",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/880 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c17e356e3b546fdbb9b953b446ecf24",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/220 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\LENOVO\\AppData\\Local\\Temp\\ipykernel_36440\\3575509826.py:100: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  return cls(**base_kwargs, tokenizer=tokenizer, callbacks=callbacks)\n",
      "c:\\Users\\LENOVO\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:668: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1320' max='1320' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1320/1320 24:50, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1 Macro</th>\n",
       "      <th>F1 Weighted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.178200</td>\n",
       "      <td>1.171764</td>\n",
       "      <td>0.522727</td>\n",
       "      <td>0.290627</td>\n",
       "      <td>0.455450</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.879400</td>\n",
       "      <td>1.178577</td>\n",
       "      <td>0.559091</td>\n",
       "      <td>0.397201</td>\n",
       "      <td>0.523824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.868200</td>\n",
       "      <td>1.348772</td>\n",
       "      <td>0.577273</td>\n",
       "      <td>0.427096</td>\n",
       "      <td>0.546487</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\LENOVO\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:668: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "c:\\Users\\LENOVO\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:668: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "c:\\Users\\LENOVO\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:668: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='110' max='110' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [110/110 00:22]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== bert-base-uncased (S1 baseline) ===\n",
      "{'eval_loss': 1.3487716913223267, 'eval_accuracy': 0.5772727272727273, 'eval_f1_macro': 0.4270963497889969, 'eval_f1_weighted': 0.5464873442954624, 'eval_runtime': 23.0267, 'eval_samples_per_second': 9.554, 'eval_steps_per_second': 4.777, 'epoch': 3.0}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    bert_s1_res = train_eval_transformer(\n",
    "        \"bert-base-uncased\",\n",
    "        X_train, y_train, X_test, y_test,\n",
    "        epochs=DEFAULT_EPOCHS,\n",
    "        batch_size=DEFAULT_BATCH,\n",
    "        max_length=DEFAULT_MAX_LEN,\n",
    "        out_dir=\"./tmp_bert_s1\",\n",
    "        suffix=\" (S1 baseline)\",\n",
    "        use_class_weights=False\n",
    "    )\n",
    "    results_s1.append(bert_s1_res)\n",
    "except Exception as e:\n",
    "    print(\"BERT S1 error:\", e)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82289420",
   "metadata": {},
   "source": [
    "## 1.5 DistilBERT (S1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "b9228e6a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a622d427151d46958ba95d77143bdb3d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\LENOVO\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\LENOVO\\.cache\\huggingface\\hub\\models--distilbert-base-uncased. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "98c10fd0b647463597e5ee31e7f54199",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/483 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d105a644b10470b8ae4d5d39e5b0738",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8983d260f8a148229d5e0a29a6635276",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2be03770dfa142d9981d4a1dd9766881",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/268M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "333c2dabfb9d477eb458e81b69b1f6cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/880 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba3c350c7a6c4078a335b11ff1526218",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/220 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\LENOVO\\AppData\\Local\\Temp\\ipykernel_36440\\3575509826.py:100: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  return cls(**base_kwargs, tokenizer=tokenizer, callbacks=callbacks)\n",
      "c:\\Users\\LENOVO\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:668: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1320' max='1320' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1320/1320 20:52, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1 Macro</th>\n",
       "      <th>F1 Weighted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.158200</td>\n",
       "      <td>1.129497</td>\n",
       "      <td>0.545455</td>\n",
       "      <td>0.309544</td>\n",
       "      <td>0.491237</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.866200</td>\n",
       "      <td>1.172050</td>\n",
       "      <td>0.545455</td>\n",
       "      <td>0.318080</td>\n",
       "      <td>0.503816</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.820400</td>\n",
       "      <td>1.251063</td>\n",
       "      <td>0.581818</td>\n",
       "      <td>0.426630</td>\n",
       "      <td>0.552515</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\LENOVO\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:668: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "c:\\Users\\LENOVO\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:668: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "c:\\Users\\LENOVO\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:668: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='110' max='110' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [110/110 00:21]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== distilbert-base-uncased (S1 baseline) ===\n",
      "{'eval_loss': 1.2510628700256348, 'eval_accuracy': 0.5818181818181818, 'eval_f1_macro': 0.4266297428062134, 'eval_f1_weighted': 0.5525147869064981, 'eval_runtime': 21.2635, 'eval_samples_per_second': 10.346, 'eval_steps_per_second': 5.173, 'epoch': 3.0}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    distil_s1_res = train_eval_transformer(\n",
    "        \"distilbert-base-uncased\",\n",
    "        X_train, y_train, X_test, y_test,\n",
    "        epochs=DEFAULT_EPOCHS,\n",
    "        batch_size=DEFAULT_BATCH,\n",
    "        max_length=DEFAULT_MAX_LEN,\n",
    "        out_dir=\"./tmp_distilbert_s1\",\n",
    "        suffix=\" (S1 baseline)\",\n",
    "        use_class_weights=False\n",
    "    )\n",
    "    results_s1.append(distil_s1_res)\n",
    "except Exception as e:\n",
    "    print(\"DistilBERT S1 error:\", e)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c6a4e41",
   "metadata": {},
   "source": [
    "### Rekap Skenario 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "496b4cd0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>f1_macro</th>\n",
       "      <th>f1_weighted</th>\n",
       "      <th>eval_loss</th>\n",
       "      <th>epoch</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>bert-base-uncased (S1 baseline)</td>\n",
       "      <td>0.577273</td>\n",
       "      <td>0.427096</td>\n",
       "      <td>0.546487</td>\n",
       "      <td>1.348772</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>distilbert-base-uncased (S1 baseline)</td>\n",
       "      <td>0.581818</td>\n",
       "      <td>0.426630</td>\n",
       "      <td>0.552515</td>\n",
       "      <td>1.251063</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Linear SVM (S1 baseline)</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.375765</td>\n",
       "      <td>0.473433</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Naive Bayes (S1 baseline)</td>\n",
       "      <td>0.436364</td>\n",
       "      <td>0.147927</td>\n",
       "      <td>0.291245</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>LSTM (S1 baseline)</td>\n",
       "      <td>0.418182</td>\n",
       "      <td>0.117949</td>\n",
       "      <td>0.246620</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   model  accuracy  f1_macro  f1_weighted  \\\n",
       "0        bert-base-uncased (S1 baseline)  0.577273  0.427096     0.546487   \n",
       "1  distilbert-base-uncased (S1 baseline)  0.581818  0.426630     0.552515   \n",
       "2               Linear SVM (S1 baseline)  0.500000  0.375765     0.473433   \n",
       "3              Naive Bayes (S1 baseline)  0.436364  0.147927     0.291245   \n",
       "4                     LSTM (S1 baseline)  0.418182  0.117949     0.246620   \n",
       "\n",
       "   eval_loss  epoch  \n",
       "0   1.348772    3.0  \n",
       "1   1.251063    3.0  \n",
       "2        NaN    NaN  \n",
       "3        NaN    NaN  \n",
       "4        NaN    NaN  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display\n",
    "import pandas as pd\n",
    "\n",
    "s1_table = pd.DataFrame(results_s1)\n",
    "\n",
    "s1_best = (\n",
    "    s1_table\n",
    "    .sort_values(\"f1_macro\", ascending=False)\n",
    "    .drop_duplicates(subset=[\"model\"], keep=\"first\")\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "display(s1_best)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a31ed640",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Skenario 3 — Balancing **Dengan** Augmentasi Teks\n",
    "Di sini **augmentasi dipisah per model**.\n",
    "Tiap model akan:\n",
    "1) melakukan augment untuk setiap strategi pada `AUG_STRATEGIES`,\n",
    "2) melatih ulang model,\n",
    "3) menampilkan rekap khusus model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc4ebfce",
   "metadata": {},
   "source": [
    "## 3.1 Linear SVM (S3 — Augment per Strategi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "afff1ec0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "------------------------------------------------------------\n",
      "S3 | SVM | Strategy: eda\n",
      "Distribusi train after aug: Counter({np.int64(4): 368, np.int64(5): 368, np.int64(2): 368, np.int64(3): 368, np.int64(1): 368})\n",
      "=== Linear SVM (S3 aug=eda) ===\n",
      "Accuracy : 0.4455\n",
      "F1-macro : 0.3533\n",
      "F1-weight: 0.4403\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1     0.3846    0.3125    0.3448        16\n",
      "           2     0.1818    0.1429    0.1600        14\n",
      "           3     0.3056    0.3143    0.3099        35\n",
      "           4     0.3226    0.3175    0.3200        63\n",
      "           5     0.6122    0.6522    0.6316        92\n",
      "\n",
      "    accuracy                         0.4455       220\n",
      "   macro avg     0.3614    0.3479    0.3533       220\n",
      "weighted avg     0.4366    0.4455    0.4403       220\n",
      "\n",
      "\n",
      "\n",
      "------------------------------------------------------------\n",
      "S3 | SVM | Strategy: modified_eda\n",
      "Distribusi train after aug: Counter({np.int64(4): 368, np.int64(5): 368, np.int64(2): 368, np.int64(3): 368, np.int64(1): 368})\n",
      "=== Linear SVM (S3 aug=modified_eda) ===\n",
      "Accuracy : 0.4727\n",
      "F1-macro : 0.3774\n",
      "F1-weight: 0.4637\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1     0.4286    0.3750    0.4000        16\n",
      "           2     0.1667    0.1429    0.1538        14\n",
      "           3     0.3333    0.3429    0.3380        35\n",
      "           4     0.3704    0.3175    0.3419        63\n",
      "           5     0.6154    0.6957    0.6531        92\n",
      "\n",
      "    accuracy                         0.4727       220\n",
      "   macro avg     0.3829    0.3748    0.3774       220\n",
      "weighted avg     0.4582    0.4727    0.4637       220\n",
      "\n",
      "\n",
      "\n",
      "------------------------------------------------------------\n",
      "S3 | SVM | Strategy: backtranslation\n",
      "Distribusi train after aug: Counter({np.int64(4): 368, np.int64(5): 368, np.int64(2): 368, np.int64(3): 368, np.int64(1): 368})\n",
      "=== Linear SVM (S3 aug=backtranslation) ===\n",
      "Accuracy : 0.4727\n",
      "F1-macro : 0.3765\n",
      "F1-weight: 0.4615\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1     0.4545    0.3125    0.3704        16\n",
      "           2     0.2222    0.1429    0.1739        14\n",
      "           3     0.3636    0.3429    0.3529        35\n",
      "           4     0.3443    0.3333    0.3387        63\n",
      "           5     0.6038    0.6957    0.6465        92\n",
      "\n",
      "    accuracy                         0.4727       220\n",
      "   macro avg     0.3977    0.3654    0.3765       220\n",
      "weighted avg     0.4561    0.4727    0.4615       220\n",
      "\n",
      "\n",
      "\n",
      "------------------------------------------------------------\n",
      "S3 | SVM | Strategy: bert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following layers were not sharded: bert.encoder.layer.*.attention.self.value.bias, bert.embeddings.position_embeddings.weight, bert.embeddings.word_embeddings.weight, bert.encoder.layer.*.output.LayerNorm.bias, bert.embeddings.token_type_embeddings.weight, bert.encoder.layer.*.attention.self.key.weight, cls.predictions.transform.dense.bias, cls.predictions.decoder.bias, cls.predictions.decoder.weight, bert.embeddings.LayerNorm.bias, bert.encoder.layer.*.attention.self.query.bias, bert.encoder.layer.*.attention.self.key.bias, bert.embeddings.LayerNorm.weight, cls.predictions.transform.LayerNorm.weight, bert.encoder.layer.*.output.dense.bias, bert.encoder.layer.*.output.dense.weight, cls.predictions.bias, bert.encoder.layer.*.attention.output.LayerNorm.weight, bert.encoder.layer.*.attention.self.value.weight, cls.predictions.transform.dense.weight, bert.encoder.layer.*.attention.output.LayerNorm.bias, bert.encoder.layer.*.intermediate.dense.bias, bert.encoder.layer.*.attention.output.dense.bias, bert.encoder.layer.*.intermediate.dense.weight, bert.encoder.layer.*.output.LayerNorm.weight, bert.encoder.layer.*.attention.output.dense.weight, bert.encoder.layer.*.attention.self.query.weight, cls.predictions.transform.LayerNorm.bias\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distribusi train after aug: Counter({np.int64(4): 368, np.int64(5): 368, np.int64(2): 368, np.int64(3): 368, np.int64(1): 368})\n",
      "=== Linear SVM (S3 aug=bert) ===\n",
      "Accuracy : 0.4636\n",
      "F1-macro : 0.3695\n",
      "F1-weight: 0.457\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1     0.4000    0.3750    0.3871        16\n",
      "           2     0.1429    0.1429    0.1429        14\n",
      "           3     0.3438    0.3143    0.3284        35\n",
      "           4     0.3684    0.3333    0.3500        63\n",
      "           5     0.6078    0.6739    0.6392        92\n",
      "\n",
      "    accuracy                         0.4636       220\n",
      "   macro avg     0.3726    0.3679    0.3695       220\n",
      "weighted avg     0.4526    0.4636    0.4570       220\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>f1_macro</th>\n",
       "      <th>f1_weighted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Linear SVM (S3 aug=modified_eda)</td>\n",
       "      <td>0.472727</td>\n",
       "      <td>0.377363</td>\n",
       "      <td>0.463659</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Linear SVM (S3 aug=backtranslation)</td>\n",
       "      <td>0.472727</td>\n",
       "      <td>0.376480</td>\n",
       "      <td>0.461487</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Linear SVM (S3 aug=bert)</td>\n",
       "      <td>0.463636</td>\n",
       "      <td>0.369497</td>\n",
       "      <td>0.457001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Linear SVM (S3 aug=eda)</td>\n",
       "      <td>0.445455</td>\n",
       "      <td>0.353253</td>\n",
       "      <td>0.440307</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 model  accuracy  f1_macro  f1_weighted\n",
       "1     Linear SVM (S3 aug=modified_eda)  0.472727  0.377363     0.463659\n",
       "2  Linear SVM (S3 aug=backtranslation)  0.472727  0.376480     0.461487\n",
       "3             Linear SVM (S3 aug=bert)  0.463636  0.369497     0.457001\n",
       "0              Linear SVM (S3 aug=eda)  0.445455  0.353253     0.440307"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "results_s3_svm = []\n",
    "\n",
    "for strat in AUG_STRATEGIES:\n",
    "    print(\"\\n\" + \"-\"*60)\n",
    "    print(\"S3 | SVM | Strategy:\", strat)\n",
    "    Xa, ya = augment_minority(\n",
    "        X_train, y_train,\n",
    "        strategy=strat,\n",
    "        target_multiplier=TARGET_MULTIPLIER,\n",
    "        max_aug_per_sample=MAX_AUG_PER_SAMPLE\n",
    "    )\n",
    "    print(\"Distribusi train after aug:\", Counter(ya))\n",
    "\n",
    "    svm_s3_res, _, _, _ = train_eval_svm(\n",
    "        Xa, ya, X_test, y_test,\n",
    "        suffix=f\" (S3 aug={strat})\",\n",
    "        class_weight=None\n",
    "    )\n",
    "    results_s3_svm.append(svm_s3_res)\n",
    "\n",
    "s3_svm_table = pd.DataFrame(results_s3_svm)\n",
    "display(s3_svm_table.sort_values(\"f1_macro\", ascending=False))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d738de9e",
   "metadata": {},
   "source": [
    "## 3.2 Naive Bayes (S3 — Augment per Strategi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "4404be7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "------------------------------------------------------------\n",
      "S3 | NB | Strategy: eda\n",
      "Distribusi train after aug: Counter({np.int64(4): 368, np.int64(5): 368, np.int64(2): 368, np.int64(3): 368, np.int64(1): 368})\n",
      "=== Naive Bayes (S3 aug=eda) ===\n",
      "Accuracy : 0.4727\n",
      "F1-macro : 0.4221\n",
      "F1-weight: 0.4842\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1     0.5000    0.5000    0.5000        16\n",
      "           2     0.1724    0.3571    0.2326        14\n",
      "           3     0.3415    0.4000    0.3684        35\n",
      "           4     0.3962    0.3333    0.3621        63\n",
      "           5     0.6914    0.6087    0.6474        92\n",
      "\n",
      "    accuracy                         0.4727       220\n",
      "   macro avg     0.4203    0.4398    0.4221       220\n",
      "weighted avg     0.5042    0.4727    0.4842       220\n",
      "\n",
      "\n",
      "\n",
      "------------------------------------------------------------\n",
      "S3 | NB | Strategy: modified_eda\n",
      "Distribusi train after aug: Counter({np.int64(4): 368, np.int64(5): 368, np.int64(2): 368, np.int64(3): 368, np.int64(1): 368})\n",
      "=== Naive Bayes (S3 aug=modified_eda) ===\n",
      "Accuracy : 0.45\n",
      "F1-macro : 0.3908\n",
      "F1-weight: 0.4675\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1     0.4667    0.4375    0.4516        16\n",
      "           2     0.1389    0.3571    0.2000        14\n",
      "           3     0.2750    0.3143    0.2933        35\n",
      "           4     0.4000    0.3175    0.3540        63\n",
      "           5     0.7089    0.6087    0.6550        92\n",
      "\n",
      "    accuracy                         0.4500       220\n",
      "   macro avg     0.3979    0.4070    0.3908       220\n",
      "weighted avg     0.4975    0.4500    0.4675       220\n",
      "\n",
      "\n",
      "\n",
      "------------------------------------------------------------\n",
      "S3 | NB | Strategy: backtranslation\n",
      "Distribusi train after aug: Counter({np.int64(4): 368, np.int64(5): 368, np.int64(2): 368, np.int64(3): 368, np.int64(1): 368})\n",
      "=== Naive Bayes (S3 aug=backtranslation) ===\n",
      "Accuracy : 0.4273\n",
      "F1-macro : 0.3806\n",
      "F1-weight: 0.4471\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1     0.4615    0.3750    0.4138        16\n",
      "           2     0.1463    0.4286    0.2182        14\n",
      "           3     0.2917    0.4000    0.3373        35\n",
      "           4     0.3696    0.2698    0.3119        63\n",
      "           5     0.7083    0.5543    0.6220        92\n",
      "\n",
      "    accuracy                         0.4273       220\n",
      "   macro avg     0.3955    0.4056    0.3806       220\n",
      "weighted avg     0.4913    0.4273    0.4471       220\n",
      "\n",
      "\n",
      "\n",
      "------------------------------------------------------------\n",
      "S3 | NB | Strategy: bert\n",
      "Distribusi train after aug: Counter({np.int64(4): 368, np.int64(5): 368, np.int64(2): 368, np.int64(3): 368, np.int64(1): 368})\n",
      "=== Naive Bayes (S3 aug=bert) ===\n",
      "Accuracy : 0.4773\n",
      "F1-macro : 0.3924\n",
      "F1-weight: 0.47\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1     0.5455    0.3750    0.4444        16\n",
      "           2     0.1538    0.2857    0.2000        14\n",
      "           3     0.3333    0.3143    0.3235        35\n",
      "           4     0.3953    0.2698    0.3208        63\n",
      "           5     0.6262    0.7283    0.6734        92\n",
      "\n",
      "    accuracy                         0.4773       220\n",
      "   macro avg     0.4108    0.3946    0.3924       220\n",
      "weighted avg     0.4776    0.4773    0.4700       220\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>f1_macro</th>\n",
       "      <th>f1_weighted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Naive Bayes (S3 aug=eda)</td>\n",
       "      <td>0.472727</td>\n",
       "      <td>0.422089</td>\n",
       "      <td>0.484189</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Naive Bayes (S3 aug=bert)</td>\n",
       "      <td>0.477273</td>\n",
       "      <td>0.392419</td>\n",
       "      <td>0.469963</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Naive Bayes (S3 aug=modified_eda)</td>\n",
       "      <td>0.450000</td>\n",
       "      <td>0.390780</td>\n",
       "      <td>0.467503</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Naive Bayes (S3 aug=backtranslation)</td>\n",
       "      <td>0.427273</td>\n",
       "      <td>0.380640</td>\n",
       "      <td>0.447061</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  model  accuracy  f1_macro  f1_weighted\n",
       "0              Naive Bayes (S3 aug=eda)  0.472727  0.422089     0.484189\n",
       "3             Naive Bayes (S3 aug=bert)  0.477273  0.392419     0.469963\n",
       "1     Naive Bayes (S3 aug=modified_eda)  0.450000  0.390780     0.467503\n",
       "2  Naive Bayes (S3 aug=backtranslation)  0.427273  0.380640     0.447061"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "results_s3_nb = []\n",
    "\n",
    "for strat in AUG_STRATEGIES:\n",
    "    print(\"\\n\" + \"-\"*60)\n",
    "    print(\"S3 | NB | Strategy:\", strat)\n",
    "    Xa, ya = augment_minority(\n",
    "        X_train, y_train,\n",
    "        strategy=strat,\n",
    "        target_multiplier=TARGET_MULTIPLIER,\n",
    "        max_aug_per_sample=MAX_AUG_PER_SAMPLE\n",
    "    )\n",
    "    print(\"Distribusi train after aug:\", Counter(ya))\n",
    "\n",
    "    nb_s3_res, _, _, _ = train_eval_nb(\n",
    "        Xa, ya, X_test, y_test,\n",
    "        suffix=f\" (S3 aug={strat})\"\n",
    "    )\n",
    "    results_s3_nb.append(nb_s3_res)\n",
    "\n",
    "s3_nb_table = pd.DataFrame(results_s3_nb)\n",
    "display(s3_nb_table.sort_values(\"f1_macro\", ascending=False))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b679aff7",
   "metadata": {},
   "source": [
    "## 3.3 LSTM (S3 — Augment per Strategi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "eda94552",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "------------------------------------------------------------\n",
      "S3 | LSTM | Strategy: eda\n",
      "Distribusi train after aug: Counter({np.int64(4): 368, np.int64(5): 368, np.int64(2): 368, np.int64(3): 368, np.int64(1): 368})\n",
      "[LSTM (S3 aug=eda)] Epoch 1/3 | loss=1.6109\n",
      "[LSTM (S3 aug=eda)] Epoch 2/3 | loss=1.6102\n",
      "[LSTM (S3 aug=eda)] Epoch 3/3 | loss=1.6109\n",
      "=== LSTM (S3 aug=eda) ===\n",
      "Accuracy : 0.1591\n",
      "F1-macro : 0.0549\n",
      "F1-weight: 0.0437\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1     0.0000    0.0000    0.0000        16\n",
      "           2     0.0000    0.0000    0.0000        14\n",
      "           3     0.1591    1.0000    0.2745        35\n",
      "           4     0.0000    0.0000    0.0000        63\n",
      "           5     0.0000    0.0000    0.0000        92\n",
      "\n",
      "    accuracy                         0.1591       220\n",
      "   macro avg     0.0318    0.2000    0.0549       220\n",
      "weighted avg     0.0253    0.1591    0.0437       220\n",
      "\n",
      "\n",
      "\n",
      "------------------------------------------------------------\n",
      "S3 | LSTM | Strategy: modified_eda\n",
      "Distribusi train after aug: Counter({np.int64(4): 368, np.int64(5): 368, np.int64(2): 368, np.int64(3): 368, np.int64(1): 368})\n",
      "[LSTM (S3 aug=modified_eda)] Epoch 1/3 | loss=1.6117\n",
      "[LSTM (S3 aug=modified_eda)] Epoch 2/3 | loss=1.6097\n",
      "[LSTM (S3 aug=modified_eda)] Epoch 3/3 | loss=1.6102\n",
      "=== LSTM (S3 aug=modified_eda) ===\n",
      "Accuracy : 0.2864\n",
      "F1-macro : 0.089\n",
      "F1-weight: 0.1275\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1     0.0000    0.0000    0.0000        16\n",
      "           2     0.0000    0.0000    0.0000        14\n",
      "           3     0.0000    0.0000    0.0000        35\n",
      "           4     0.2864    1.0000    0.4452        63\n",
      "           5     0.0000    0.0000    0.0000        92\n",
      "\n",
      "    accuracy                         0.2864       220\n",
      "   macro avg     0.0573    0.2000    0.0890       220\n",
      "weighted avg     0.0820    0.2864    0.1275       220\n",
      "\n",
      "\n",
      "\n",
      "------------------------------------------------------------\n",
      "S3 | LSTM | Strategy: backtranslation\n",
      "Distribusi train after aug: Counter({np.int64(4): 368, np.int64(5): 368, np.int64(2): 368, np.int64(3): 368, np.int64(1): 368})\n",
      "[LSTM (S3 aug=backtranslation)] Epoch 1/3 | loss=1.6114\n",
      "[LSTM (S3 aug=backtranslation)] Epoch 2/3 | loss=1.6099\n",
      "[LSTM (S3 aug=backtranslation)] Epoch 3/3 | loss=1.6101\n",
      "=== LSTM (S3 aug=backtranslation) ===\n",
      "Accuracy : 0.4182\n",
      "F1-macro : 0.1179\n",
      "F1-weight: 0.2466\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1     0.0000    0.0000    0.0000        16\n",
      "           2     0.0000    0.0000    0.0000        14\n",
      "           3     0.0000    0.0000    0.0000        35\n",
      "           4     0.0000    0.0000    0.0000        63\n",
      "           5     0.4182    1.0000    0.5897        92\n",
      "\n",
      "    accuracy                         0.4182       220\n",
      "   macro avg     0.0836    0.2000    0.1179       220\n",
      "weighted avg     0.1749    0.4182    0.2466       220\n",
      "\n",
      "\n",
      "\n",
      "------------------------------------------------------------\n",
      "S3 | LSTM | Strategy: bert\n",
      "Distribusi train after aug: Counter({np.int64(4): 368, np.int64(5): 368, np.int64(2): 368, np.int64(3): 368, np.int64(1): 368})\n",
      "[LSTM (S3 aug=bert)] Epoch 1/3 | loss=1.6111\n",
      "[LSTM (S3 aug=bert)] Epoch 2/3 | loss=1.6100\n",
      "[LSTM (S3 aug=bert)] Epoch 3/3 | loss=1.6102\n",
      "=== LSTM (S3 aug=bert) ===\n",
      "Accuracy : 0.1591\n",
      "F1-macro : 0.0549\n",
      "F1-weight: 0.0437\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1     0.0000    0.0000    0.0000        16\n",
      "           2     0.0000    0.0000    0.0000        14\n",
      "           3     0.1591    1.0000    0.2745        35\n",
      "           4     0.0000    0.0000    0.0000        63\n",
      "           5     0.0000    0.0000    0.0000        92\n",
      "\n",
      "    accuracy                         0.1591       220\n",
      "   macro avg     0.0318    0.2000    0.0549       220\n",
      "weighted avg     0.0253    0.1591    0.0437       220\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>f1_macro</th>\n",
       "      <th>f1_weighted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>LSTM (S3 aug=backtranslation)</td>\n",
       "      <td>0.418182</td>\n",
       "      <td>0.117949</td>\n",
       "      <td>0.246620</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>LSTM (S3 aug=modified_eda)</td>\n",
       "      <td>0.286364</td>\n",
       "      <td>0.089046</td>\n",
       "      <td>0.127498</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LSTM (S3 aug=eda)</td>\n",
       "      <td>0.159091</td>\n",
       "      <td>0.054902</td>\n",
       "      <td>0.043672</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>LSTM (S3 aug=bert)</td>\n",
       "      <td>0.159091</td>\n",
       "      <td>0.054902</td>\n",
       "      <td>0.043672</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           model  accuracy  f1_macro  f1_weighted\n",
       "2  LSTM (S3 aug=backtranslation)  0.418182  0.117949     0.246620\n",
       "1     LSTM (S3 aug=modified_eda)  0.286364  0.089046     0.127498\n",
       "0              LSTM (S3 aug=eda)  0.159091  0.054902     0.043672\n",
       "3             LSTM (S3 aug=bert)  0.159091  0.054902     0.043672"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "results_s3_lstm = []\n",
    "\n",
    "for strat in AUG_STRATEGIES:\n",
    "    try:\n",
    "        print(\"\\n\" + \"-\"*60)\n",
    "        print(\"S3 | LSTM | Strategy:\", strat)\n",
    "        Xa, ya = augment_minority(\n",
    "            X_train, y_train,\n",
    "            strategy=strat,\n",
    "            target_multiplier=TARGET_MULTIPLIER,\n",
    "            max_aug_per_sample=MAX_AUG_PER_SAMPLE\n",
    "        )\n",
    "        print(\"Distribusi train after aug:\", Counter(ya))\n",
    "\n",
    "        lstm_s3_res = train_eval_lstm(\n",
    "            Xa, ya, X_test, y_test,\n",
    "            epochs=3, suffix=f\" (S3 aug={strat})\",\n",
    "            use_class_weights=True\n",
    "        )\n",
    "        results_s3_lstm.append(lstm_s3_res)\n",
    "    except Exception as e:\n",
    "        print(f\"LSTM S3 ({strat}) error:\", e)\n",
    "\n",
    "s3_lstm_table = pd.DataFrame(results_s3_lstm)\n",
    "display(s3_lstm_table.sort_values(\"f1_macro\", ascending=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "75e836c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "------------------------------------------------------------\n",
      "S3 | LR | Strategy: eda\n",
      "Distribusi train after aug: Counter({np.int64(4): 368, np.int64(5): 368, np.int64(2): 368, np.int64(3): 368, np.int64(1): 368})\n",
      "=== Logistic Regression (S3 aug=eda) ===\n",
      "Accuracy : 0.4818\n",
      "F1-macro : 0.39\n",
      "F1-weight: 0.4767\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1     0.4615    0.3750    0.4138        16\n",
      "           2     0.1875    0.2143    0.2000        14\n",
      "           3     0.3056    0.3143    0.3099        35\n",
      "           4     0.3636    0.3175    0.3390        63\n",
      "           5     0.6600    0.7174    0.6875        92\n",
      "\n",
      "    accuracy                         0.4818       220\n",
      "   macro avg     0.3956    0.3877    0.3900       220\n",
      "weighted avg     0.4742    0.4818    0.4767       220\n",
      "\n",
      "\n",
      "\n",
      "------------------------------------------------------------\n",
      "S3 | LR | Strategy: modified_eda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\LENOVO\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1272: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.8. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distribusi train after aug: Counter({np.int64(4): 368, np.int64(5): 368, np.int64(2): 368, np.int64(3): 368, np.int64(1): 368})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\LENOVO\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1272: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.8. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Logistic Regression (S3 aug=modified_eda) ===\n",
      "Accuracy : 0.5091\n",
      "F1-macro : 0.4347\n",
      "F1-weight: 0.5072\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1     0.5000    0.4375    0.4667        16\n",
      "           2     0.2353    0.2857    0.2581        14\n",
      "           3     0.3714    0.3714    0.3714        35\n",
      "           4     0.4211    0.3810    0.4000        63\n",
      "           5     0.6598    0.6957    0.6772        92\n",
      "\n",
      "    accuracy                         0.5091       220\n",
      "   macro avg     0.4375    0.4342    0.4347       220\n",
      "weighted avg     0.5069    0.5091    0.5072       220\n",
      "\n",
      "\n",
      "\n",
      "------------------------------------------------------------\n",
      "S3 | LR | Strategy: backtranslation\n",
      "Distribusi train after aug: Counter({np.int64(4): 368, np.int64(5): 368, np.int64(2): 368, np.int64(3): 368, np.int64(1): 368})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\LENOVO\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1272: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.8. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Logistic Regression (S3 aug=backtranslation) ===\n",
      "Accuracy : 0.4591\n",
      "F1-macro : 0.3703\n",
      "F1-weight: 0.4552\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1     0.4615    0.3750    0.4138        16\n",
      "           2     0.1176    0.1429    0.1290        14\n",
      "           3     0.3333    0.3429    0.3380        35\n",
      "           4     0.3519    0.3016    0.3248        63\n",
      "           5     0.6200    0.6739    0.6458        92\n",
      "\n",
      "    accuracy                         0.4591       220\n",
      "   macro avg     0.3769    0.3672    0.3703       220\n",
      "weighted avg     0.4541    0.4591    0.4552       220\n",
      "\n",
      "\n",
      "\n",
      "------------------------------------------------------------\n",
      "S3 | LR | Strategy: bert\n",
      "Distribusi train after aug: Counter({np.int64(4): 368, np.int64(5): 368, np.int64(2): 368, np.int64(3): 368, np.int64(1): 368})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\LENOVO\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1272: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.8. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Logistic Regression (S3 aug=bert) ===\n",
      "Accuracy : 0.4818\n",
      "F1-macro : 0.4097\n",
      "F1-weight: 0.4793\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1     0.5385    0.4375    0.4828        16\n",
      "           2     0.1667    0.2143    0.1875        14\n",
      "           3     0.4062    0.3714    0.3881        35\n",
      "           4     0.3448    0.3175    0.3306        63\n",
      "           5     0.6364    0.6848    0.6597        92\n",
      "\n",
      "    accuracy                         0.4818       220\n",
      "   macro avg     0.4185    0.4051    0.4097       220\n",
      "weighted avg     0.4793    0.4818    0.4793       220\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>f1_macro</th>\n",
       "      <th>f1_weighted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Logistic Regression (S3 aug=modified_eda)</td>\n",
       "      <td>0.509091</td>\n",
       "      <td>0.434682</td>\n",
       "      <td>0.507211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Logistic Regression (S3 aug=bert)</td>\n",
       "      <td>0.481818</td>\n",
       "      <td>0.409717</td>\n",
       "      <td>0.479313</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Logistic Regression (S3 aug=eda)</td>\n",
       "      <td>0.481818</td>\n",
       "      <td>0.390027</td>\n",
       "      <td>0.476690</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Logistic Regression (S3 aug=backtranslation)</td>\n",
       "      <td>0.459091</td>\n",
       "      <td>0.370295</td>\n",
       "      <td>0.455165</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          model  accuracy  f1_macro  \\\n",
       "1     Logistic Regression (S3 aug=modified_eda)  0.509091  0.434682   \n",
       "3             Logistic Regression (S3 aug=bert)  0.481818  0.409717   \n",
       "0              Logistic Regression (S3 aug=eda)  0.481818  0.390027   \n",
       "2  Logistic Regression (S3 aug=backtranslation)  0.459091  0.370295   \n",
       "\n",
       "   f1_weighted  \n",
       "1     0.507211  \n",
       "3     0.479313  \n",
       "0     0.476690  \n",
       "2     0.455165  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "results_s3_lr = []\n",
    "\n",
    "for strat in AUG_STRATEGIES:\n",
    "    print(\"\\n\" + \"-\"*60)\n",
    "    print(\"S3 | LR | Strategy:\", strat)\n",
    "    Xa, ya = augment_minority(\n",
    "        X_train, y_train,\n",
    "        strategy=strat,\n",
    "        target_multiplier=TARGET_MULTIPLIER,\n",
    "        max_aug_per_sample=MAX_AUG_PER_SAMPLE\n",
    "    )\n",
    "    print(\"Distribusi train after aug:\", Counter(ya))\n",
    "\n",
    "    lr_s3_res, _, _, _ = train_eval_lr(\n",
    "        Xa, ya, X_test, y_test,\n",
    "        suffix=f\" (S3 aug={strat})\"\n",
    "    )\n",
    "    results_s3_lr.append(lr_s3_res)\n",
    "\n",
    "s3_lr_table = pd.DataFrame(results_s3_lr)\n",
    "display(s3_lr_table.sort_values(\"f1_macro\", ascending=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9391a8d",
   "metadata": {},
   "source": [
    "## 3.4 BERT (S3 — Augment per Strategi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "5a808647",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "------------------------------------------------------------\n",
      "S3 | BERT | Strategy: eda\n",
      "Distribusi train after aug: Counter({np.int64(4): 368, np.int64(5): 368, np.int64(2): 368, np.int64(3): 368, np.int64(1): 368})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following layers were not sharded: bert.encoder.layer.*.attention.self.value.bias, bert.embeddings.position_embeddings.weight, bert.embeddings.word_embeddings.weight, bert.encoder.layer.*.output.LayerNorm.bias, bert.embeddings.token_type_embeddings.weight, bert.encoder.layer.*.attention.self.key.weight, bert.embeddings.LayerNorm.bias, bert.encoder.layer.*.attention.self.query.bias, bert.encoder.layer.*.attention.self.key.bias, bert.pooler.dense.bias, bert.embeddings.LayerNorm.weight, classifier.bias, bert.encoder.layer.*.output.dense.bias, bert.pooler.dense.weight, classifier.weight, bert.encoder.layer.*.output.dense.weight, bert.encoder.layer.*.attention.output.LayerNorm.weight, bert.encoder.layer.*.attention.self.value.weight, bert.encoder.layer.*.attention.output.LayerNorm.bias, bert.encoder.layer.*.intermediate.dense.bias, bert.encoder.layer.*.attention.output.dense.bias, bert.encoder.layer.*.intermediate.dense.weight, bert.encoder.layer.*.output.LayerNorm.weight, bert.encoder.layer.*.attention.output.dense.weight, bert.encoder.layer.*.attention.self.query.weight\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "658fe8f5291c4c62b001c47a673002a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1840 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c47fdbc67604e488da35a0490d3dbde",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/220 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\LENOVO\\AppData\\Local\\Temp\\ipykernel_36440\\3575509826.py:63: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `WeightedTrainer.__init__`. Use `processing_class` instead.\n",
      "  super().__init__(*args, **kwargs)\n",
      "c:\\Users\\LENOVO\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:668: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2760' max='2760' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2760/2760 47:19, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1 Macro</th>\n",
       "      <th>F1 Weighted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.909000</td>\n",
       "      <td>1.386487</td>\n",
       "      <td>0.518182</td>\n",
       "      <td>0.362407</td>\n",
       "      <td>0.473853</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.493300</td>\n",
       "      <td>2.183669</td>\n",
       "      <td>0.504545</td>\n",
       "      <td>0.385537</td>\n",
       "      <td>0.499884</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.245100</td>\n",
       "      <td>2.454143</td>\n",
       "      <td>0.527273</td>\n",
       "      <td>0.391515</td>\n",
       "      <td>0.518081</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\LENOVO\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:668: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "c:\\Users\\LENOVO\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:668: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "c:\\Users\\LENOVO\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:668: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='110' max='110' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [110/110 00:21]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== bert-base-uncased (S3 aug=eda) ===\n",
      "{'eval_loss': 2.4541425704956055, 'eval_accuracy': 0.5272727272727272, 'eval_f1_macro': 0.3915154118327493, 'eval_f1_weighted': 0.5180814154885361, 'eval_runtime': 21.5204, 'eval_samples_per_second': 10.223, 'eval_steps_per_second': 5.111, 'epoch': 3.0}\n",
      "\n",
      "\n",
      "------------------------------------------------------------\n",
      "S3 | BERT | Strategy: modified_eda\n",
      "Distribusi train after aug: Counter({np.int64(4): 368, np.int64(5): 368, np.int64(2): 368, np.int64(3): 368, np.int64(1): 368})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following layers were not sharded: bert.encoder.layer.*.attention.self.value.bias, bert.embeddings.position_embeddings.weight, bert.embeddings.word_embeddings.weight, bert.encoder.layer.*.output.LayerNorm.bias, bert.embeddings.token_type_embeddings.weight, bert.encoder.layer.*.attention.self.key.weight, bert.embeddings.LayerNorm.bias, bert.encoder.layer.*.attention.self.query.bias, bert.encoder.layer.*.attention.self.key.bias, bert.pooler.dense.bias, bert.embeddings.LayerNorm.weight, classifier.bias, bert.encoder.layer.*.output.dense.bias, bert.pooler.dense.weight, classifier.weight, bert.encoder.layer.*.output.dense.weight, bert.encoder.layer.*.attention.output.LayerNorm.weight, bert.encoder.layer.*.attention.self.value.weight, bert.encoder.layer.*.attention.output.LayerNorm.bias, bert.encoder.layer.*.intermediate.dense.bias, bert.encoder.layer.*.attention.output.dense.bias, bert.encoder.layer.*.intermediate.dense.weight, bert.encoder.layer.*.output.LayerNorm.weight, bert.encoder.layer.*.attention.output.dense.weight, bert.encoder.layer.*.attention.self.query.weight\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9788b6e13915475389cfc8468af43329",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1840 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9300cbbb32344b0c837b072396eb6246",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/220 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\LENOVO\\AppData\\Local\\Temp\\ipykernel_36440\\3575509826.py:63: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `WeightedTrainer.__init__`. Use `processing_class` instead.\n",
      "  super().__init__(*args, **kwargs)\n",
      "c:\\Users\\LENOVO\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:668: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1840' max='2760' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1840/2760 31:01 < 15:31, 0.99 it/s, Epoch 2/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1 Macro</th>\n",
       "      <th>F1 Weighted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.908200</td>\n",
       "      <td>1.453806</td>\n",
       "      <td>0.522727</td>\n",
       "      <td>0.382219</td>\n",
       "      <td>0.490945</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.462800</td>\n",
       "      <td>2.258558</td>\n",
       "      <td>0.536364</td>\n",
       "      <td>0.375182</td>\n",
       "      <td>0.515380</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\LENOVO\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:668: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "c:\\Users\\LENOVO\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:668: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='110' max='110' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [110/110 00:21]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== bert-base-uncased (S3 aug=modified_eda) ===\n",
      "{'eval_loss': 1.4538058042526245, 'eval_accuracy': 0.5227272727272727, 'eval_f1_macro': 0.3822191029644981, 'eval_f1_weighted': 0.4909453926255095, 'eval_runtime': 22.1799, 'eval_samples_per_second': 9.919, 'eval_steps_per_second': 4.959, 'epoch': 2.0}\n",
      "\n",
      "\n",
      "------------------------------------------------------------\n",
      "S3 | BERT | Strategy: backtranslation\n",
      "Distribusi train after aug: Counter({np.int64(4): 368, np.int64(5): 368, np.int64(2): 368, np.int64(3): 368, np.int64(1): 368})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following layers were not sharded: bert.encoder.layer.*.attention.self.value.bias, bert.embeddings.position_embeddings.weight, bert.embeddings.word_embeddings.weight, bert.encoder.layer.*.output.LayerNorm.bias, bert.embeddings.token_type_embeddings.weight, bert.encoder.layer.*.attention.self.key.weight, bert.embeddings.LayerNorm.bias, bert.encoder.layer.*.attention.self.query.bias, bert.encoder.layer.*.attention.self.key.bias, bert.pooler.dense.bias, bert.embeddings.LayerNorm.weight, classifier.bias, bert.encoder.layer.*.output.dense.bias, bert.pooler.dense.weight, classifier.weight, bert.encoder.layer.*.output.dense.weight, bert.encoder.layer.*.attention.output.LayerNorm.weight, bert.encoder.layer.*.attention.self.value.weight, bert.encoder.layer.*.attention.output.LayerNorm.bias, bert.encoder.layer.*.intermediate.dense.bias, bert.encoder.layer.*.attention.output.dense.bias, bert.encoder.layer.*.intermediate.dense.weight, bert.encoder.layer.*.output.LayerNorm.weight, bert.encoder.layer.*.attention.output.dense.weight, bert.encoder.layer.*.attention.self.query.weight\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2966b43b2efe4769b4c17042dcb66aca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1840 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1407669dff654eb59a7c3a6f423ff1a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/220 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\LENOVO\\AppData\\Local\\Temp\\ipykernel_36440\\3575509826.py:63: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `WeightedTrainer.__init__`. Use `processing_class` instead.\n",
      "  super().__init__(*args, **kwargs)\n",
      "c:\\Users\\LENOVO\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:668: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2760' max='2760' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2760/2760 46:41, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1 Macro</th>\n",
       "      <th>F1 Weighted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.779000</td>\n",
       "      <td>1.478938</td>\n",
       "      <td>0.495455</td>\n",
       "      <td>0.311971</td>\n",
       "      <td>0.469943</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.573200</td>\n",
       "      <td>2.210000</td>\n",
       "      <td>0.513636</td>\n",
       "      <td>0.359147</td>\n",
       "      <td>0.494484</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.201500</td>\n",
       "      <td>2.698585</td>\n",
       "      <td>0.522727</td>\n",
       "      <td>0.377247</td>\n",
       "      <td>0.509995</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\LENOVO\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:668: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "c:\\Users\\LENOVO\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:668: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "c:\\Users\\LENOVO\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:668: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='110' max='110' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [110/110 00:20]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== bert-base-uncased (S3 aug=backtranslation) ===\n",
      "{'eval_loss': 2.698585271835327, 'eval_accuracy': 0.5227272727272727, 'eval_f1_macro': 0.37724663294084726, 'eval_f1_weighted': 0.5099954211074895, 'eval_runtime': 21.1503, 'eval_samples_per_second': 10.402, 'eval_steps_per_second': 5.201, 'epoch': 3.0}\n",
      "\n",
      "\n",
      "------------------------------------------------------------\n",
      "S3 | BERT | Strategy: bert\n",
      "Distribusi train after aug: Counter({np.int64(4): 368, np.int64(5): 368, np.int64(2): 368, np.int64(3): 368, np.int64(1): 368})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following layers were not sharded: bert.encoder.layer.*.attention.self.value.bias, bert.embeddings.position_embeddings.weight, bert.embeddings.word_embeddings.weight, bert.encoder.layer.*.output.LayerNorm.bias, bert.embeddings.token_type_embeddings.weight, bert.encoder.layer.*.attention.self.key.weight, bert.embeddings.LayerNorm.bias, bert.encoder.layer.*.attention.self.query.bias, bert.encoder.layer.*.attention.self.key.bias, bert.pooler.dense.bias, bert.embeddings.LayerNorm.weight, classifier.bias, bert.encoder.layer.*.output.dense.bias, bert.pooler.dense.weight, classifier.weight, bert.encoder.layer.*.output.dense.weight, bert.encoder.layer.*.attention.output.LayerNorm.weight, bert.encoder.layer.*.attention.self.value.weight, bert.encoder.layer.*.attention.output.LayerNorm.bias, bert.encoder.layer.*.intermediate.dense.bias, bert.encoder.layer.*.attention.output.dense.bias, bert.encoder.layer.*.intermediate.dense.weight, bert.encoder.layer.*.output.LayerNorm.weight, bert.encoder.layer.*.attention.output.dense.weight, bert.encoder.layer.*.attention.self.query.weight\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0cfb5678a36c4f48a3a7ae8cd289df33",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1840 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f19e25b556de4e66add3fa9ab0689e71",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/220 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\LENOVO\\AppData\\Local\\Temp\\ipykernel_36440\\3575509826.py:63: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `WeightedTrainer.__init__`. Use `processing_class` instead.\n",
      "  super().__init__(*args, **kwargs)\n",
      "c:\\Users\\LENOVO\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:668: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2760' max='2760' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2760/2760 46:56, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1 Macro</th>\n",
       "      <th>F1 Weighted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.323900</td>\n",
       "      <td>1.170442</td>\n",
       "      <td>0.513636</td>\n",
       "      <td>0.384150</td>\n",
       "      <td>0.482230</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.747300</td>\n",
       "      <td>1.629745</td>\n",
       "      <td>0.490909</td>\n",
       "      <td>0.389370</td>\n",
       "      <td>0.482820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.333000</td>\n",
       "      <td>2.197634</td>\n",
       "      <td>0.513636</td>\n",
       "      <td>0.430704</td>\n",
       "      <td>0.514529</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\LENOVO\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:668: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "c:\\Users\\LENOVO\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:668: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "c:\\Users\\LENOVO\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:668: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='110' max='110' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [110/110 00:21]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== bert-base-uncased (S3 aug=bert) ===\n",
      "{'eval_loss': 2.197633743286133, 'eval_accuracy': 0.5136363636363637, 'eval_f1_macro': 0.43070413755159526, 'eval_f1_weighted': 0.514528580969259, 'eval_runtime': 22.1564, 'eval_samples_per_second': 9.929, 'eval_steps_per_second': 4.965, 'epoch': 3.0}\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>f1_macro</th>\n",
       "      <th>f1_weighted</th>\n",
       "      <th>eval_loss</th>\n",
       "      <th>epoch</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>bert-base-uncased (S3 aug=bert)</td>\n",
       "      <td>0.513636</td>\n",
       "      <td>0.430704</td>\n",
       "      <td>0.514529</td>\n",
       "      <td>2.197634</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>bert-base-uncased (S3 aug=eda)</td>\n",
       "      <td>0.527273</td>\n",
       "      <td>0.391515</td>\n",
       "      <td>0.518081</td>\n",
       "      <td>2.454143</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>bert-base-uncased (S3 aug=modified_eda)</td>\n",
       "      <td>0.522727</td>\n",
       "      <td>0.382219</td>\n",
       "      <td>0.490945</td>\n",
       "      <td>1.453806</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>bert-base-uncased (S3 aug=backtranslation)</td>\n",
       "      <td>0.522727</td>\n",
       "      <td>0.377247</td>\n",
       "      <td>0.509995</td>\n",
       "      <td>2.698585</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        model  accuracy  f1_macro  \\\n",
       "3             bert-base-uncased (S3 aug=bert)  0.513636  0.430704   \n",
       "0              bert-base-uncased (S3 aug=eda)  0.527273  0.391515   \n",
       "1     bert-base-uncased (S3 aug=modified_eda)  0.522727  0.382219   \n",
       "2  bert-base-uncased (S3 aug=backtranslation)  0.522727  0.377247   \n",
       "\n",
       "   f1_weighted  eval_loss  epoch  \n",
       "3     0.514529   2.197634    3.0  \n",
       "0     0.518081   2.454143    3.0  \n",
       "1     0.490945   1.453806    2.0  \n",
       "2     0.509995   2.698585    3.0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "results_s3_bert = []\n",
    "\n",
    "for strat in AUG_STRATEGIES:\n",
    "    try:\n",
    "        print(\"\\n\" + \"-\"*60)\n",
    "        print(\"S3 | BERT | Strategy:\", strat)\n",
    "        Xa, ya = augment_minority(\n",
    "            X_train, y_train,\n",
    "            strategy=strat,\n",
    "            target_multiplier=TARGET_MULTIPLIER,\n",
    "            max_aug_per_sample=MAX_AUG_PER_SAMPLE\n",
    "        )\n",
    "        print(\"Distribusi train after aug:\", Counter(ya))\n",
    "\n",
    "        bert_s3_res = train_eval_transformer(\n",
    "            \"bert-base-uncased\",\n",
    "            Xa, ya, X_test, y_test,\n",
    "            epochs=DEFAULT_EPOCHS,\n",
    "            batch_size=DEFAULT_BATCH,\n",
    "            max_length=DEFAULT_MAX_LEN,\n",
    "            out_dir=f\"./tmp_bert_s3_{strat}\",\n",
    "            suffix=f\" (S3 aug={strat})\",\n",
    "            use_class_weights=True\n",
    "        )\n",
    "        results_s3_bert.append(bert_s3_res)\n",
    "    except Exception as e:\n",
    "        print(f\"BERT S3 ({strat}) error:\", e)\n",
    "\n",
    "s3_bert_table = pd.DataFrame(results_s3_bert)\n",
    "display(s3_bert_table.sort_values(\"f1_macro\", ascending=False))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af0ee0c9",
   "metadata": {},
   "source": [
    "## 3.5 DistilBERT (S3 — Augment per Strategi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "c8b5267d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "------------------------------------------------------------\n",
      "S3 | DistilBERT | Strategy: eda\n",
      "Distribusi train after aug: Counter({np.int64(4): 368, np.int64(5): 368, np.int64(2): 368, np.int64(3): 368, np.int64(1): 368})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following layers were not sharded: distilbert.transformer.layer.*.attention.out_lin.weight, distilbert.transformer.layer.*.output_layer_norm.weight, distilbert.transformer.layer.*.ffn.lin*.bias, distilbert.transformer.layer.*.attention.out_lin.bias, distilbert.transformer.layer.*.attention.q_lin.weight, distilbert.transformer.layer.*.sa_layer_norm.bias, pre_classifier.weight, distilbert.transformer.layer.*.attention.v_lin.weight, pre_classifier.bias, distilbert.embeddings.position_embeddings.weight, classifier.bias, classifier.weight, distilbert.transformer.layer.*.attention.k_lin.bias, distilbert.transformer.layer.*.attention.q_lin.bias, distilbert.embeddings.LayerNorm.bias, distilbert.transformer.layer.*.attention.k_lin.weight, distilbert.embeddings.word_embeddings.weight, distilbert.transformer.layer.*.sa_layer_norm.weight, distilbert.transformer.layer.*.ffn.lin*.weight, distilbert.transformer.layer.*.output_layer_norm.bias, distilbert.embeddings.LayerNorm.weight, distilbert.transformer.layer.*.attention.v_lin.bias\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "911241afe63641969b66d68db9feeaf2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1840 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6516582afb924ce38e805beff5f6b885",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/220 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\LENOVO\\AppData\\Local\\Temp\\ipykernel_36440\\3575509826.py:63: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `WeightedTrainer.__init__`. Use `processing_class` instead.\n",
      "  super().__init__(*args, **kwargs)\n",
      "c:\\Users\\LENOVO\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:668: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2760' max='2760' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2760/2760 25:28, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1 Macro</th>\n",
       "      <th>F1 Weighted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.947300</td>\n",
       "      <td>1.244315</td>\n",
       "      <td>0.527273</td>\n",
       "      <td>0.418426</td>\n",
       "      <td>0.517187</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.599100</td>\n",
       "      <td>1.741582</td>\n",
       "      <td>0.504545</td>\n",
       "      <td>0.431919</td>\n",
       "      <td>0.507991</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.366300</td>\n",
       "      <td>1.971937</td>\n",
       "      <td>0.554545</td>\n",
       "      <td>0.479856</td>\n",
       "      <td>0.555061</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\LENOVO\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:668: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "c:\\Users\\LENOVO\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:668: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "c:\\Users\\LENOVO\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:668: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='110' max='110' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [110/110 00:11]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== distilbert-base-uncased (S3 aug=eda) ===\n",
      "{'eval_loss': 1.9719372987747192, 'eval_accuracy': 0.5545454545454546, 'eval_f1_macro': 0.4798564609755135, 'eval_f1_weighted': 0.5550608546258945, 'eval_runtime': 11.6469, 'eval_samples_per_second': 18.889, 'eval_steps_per_second': 9.445, 'epoch': 3.0}\n",
      "\n",
      "\n",
      "------------------------------------------------------------\n",
      "S3 | DistilBERT | Strategy: modified_eda\n",
      "Distribusi train after aug: Counter({np.int64(4): 368, np.int64(5): 368, np.int64(2): 368, np.int64(3): 368, np.int64(1): 368})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following layers were not sharded: distilbert.transformer.layer.*.attention.out_lin.weight, distilbert.transformer.layer.*.output_layer_norm.weight, distilbert.transformer.layer.*.ffn.lin*.bias, distilbert.transformer.layer.*.attention.out_lin.bias, distilbert.transformer.layer.*.attention.q_lin.weight, distilbert.transformer.layer.*.sa_layer_norm.bias, pre_classifier.weight, distilbert.transformer.layer.*.attention.v_lin.weight, pre_classifier.bias, distilbert.embeddings.position_embeddings.weight, classifier.bias, classifier.weight, distilbert.transformer.layer.*.attention.k_lin.bias, distilbert.transformer.layer.*.attention.q_lin.bias, distilbert.embeddings.LayerNorm.bias, distilbert.transformer.layer.*.attention.k_lin.weight, distilbert.embeddings.word_embeddings.weight, distilbert.transformer.layer.*.sa_layer_norm.weight, distilbert.transformer.layer.*.ffn.lin*.weight, distilbert.transformer.layer.*.output_layer_norm.bias, distilbert.embeddings.LayerNorm.weight, distilbert.transformer.layer.*.attention.v_lin.bias\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1912793c680f4b66a6357a8e65ef4c7f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1840 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c8ccb7ea6994a8eb8a42592d7609eab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/220 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\LENOVO\\AppData\\Local\\Temp\\ipykernel_36440\\3575509826.py:63: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `WeightedTrainer.__init__`. Use `processing_class` instead.\n",
      "  super().__init__(*args, **kwargs)\n",
      "c:\\Users\\LENOVO\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:668: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2760' max='2760' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2760/2760 25:38, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1 Macro</th>\n",
       "      <th>F1 Weighted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.926000</td>\n",
       "      <td>1.209728</td>\n",
       "      <td>0.550000</td>\n",
       "      <td>0.425323</td>\n",
       "      <td>0.521106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.653700</td>\n",
       "      <td>1.855112</td>\n",
       "      <td>0.540909</td>\n",
       "      <td>0.434028</td>\n",
       "      <td>0.532866</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.310000</td>\n",
       "      <td>2.122706</td>\n",
       "      <td>0.545455</td>\n",
       "      <td>0.440164</td>\n",
       "      <td>0.544512</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\LENOVO\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:668: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "c:\\Users\\LENOVO\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:668: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "c:\\Users\\LENOVO\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:668: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='110' max='110' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [110/110 00:11]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== distilbert-base-uncased (S3 aug=modified_eda) ===\n",
      "{'eval_loss': 2.122706174850464, 'eval_accuracy': 0.5454545454545454, 'eval_f1_macro': 0.44016367063268813, 'eval_f1_weighted': 0.5445119662013005, 'eval_runtime': 11.5524, 'eval_samples_per_second': 19.044, 'eval_steps_per_second': 9.522, 'epoch': 3.0}\n",
      "\n",
      "\n",
      "------------------------------------------------------------\n",
      "S3 | DistilBERT | Strategy: backtranslation\n",
      "Distribusi train after aug: Counter({np.int64(4): 368, np.int64(5): 368, np.int64(2): 368, np.int64(3): 368, np.int64(1): 368})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following layers were not sharded: distilbert.transformer.layer.*.attention.out_lin.weight, distilbert.transformer.layer.*.output_layer_norm.weight, distilbert.transformer.layer.*.ffn.lin*.bias, distilbert.transformer.layer.*.attention.out_lin.bias, distilbert.transformer.layer.*.attention.q_lin.weight, distilbert.transformer.layer.*.sa_layer_norm.bias, pre_classifier.weight, distilbert.transformer.layer.*.attention.v_lin.weight, pre_classifier.bias, distilbert.embeddings.position_embeddings.weight, classifier.bias, classifier.weight, distilbert.transformer.layer.*.attention.k_lin.bias, distilbert.transformer.layer.*.attention.q_lin.bias, distilbert.embeddings.LayerNorm.bias, distilbert.transformer.layer.*.attention.k_lin.weight, distilbert.embeddings.word_embeddings.weight, distilbert.transformer.layer.*.sa_layer_norm.weight, distilbert.transformer.layer.*.ffn.lin*.weight, distilbert.transformer.layer.*.output_layer_norm.bias, distilbert.embeddings.LayerNorm.weight, distilbert.transformer.layer.*.attention.v_lin.bias\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "745d53b61dcc4d21aa814b736cd484b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1840 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e4157f6d914844f2b16897771d310369",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/220 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\LENOVO\\AppData\\Local\\Temp\\ipykernel_36440\\3575509826.py:63: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `WeightedTrainer.__init__`. Use `processing_class` instead.\n",
      "  super().__init__(*args, **kwargs)\n",
      "c:\\Users\\LENOVO\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:668: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2760' max='2760' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2760/2760 26:38, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1 Macro</th>\n",
       "      <th>F1 Weighted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.776500</td>\n",
       "      <td>1.481464</td>\n",
       "      <td>0.472727</td>\n",
       "      <td>0.346202</td>\n",
       "      <td>0.469821</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.607300</td>\n",
       "      <td>1.931003</td>\n",
       "      <td>0.536364</td>\n",
       "      <td>0.435818</td>\n",
       "      <td>0.535759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.271100</td>\n",
       "      <td>2.231638</td>\n",
       "      <td>0.527273</td>\n",
       "      <td>0.419139</td>\n",
       "      <td>0.526569</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\LENOVO\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:668: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "c:\\Users\\LENOVO\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:668: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "c:\\Users\\LENOVO\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:668: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='110' max='110' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [110/110 00:11]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== distilbert-base-uncased (S3 aug=backtranslation) ===\n",
      "{'eval_loss': 1.9310028553009033, 'eval_accuracy': 0.5363636363636364, 'eval_f1_macro': 0.4358176174521686, 'eval_f1_weighted': 0.5357587267768955, 'eval_runtime': 11.6107, 'eval_samples_per_second': 18.948, 'eval_steps_per_second': 9.474, 'epoch': 3.0}\n",
      "\n",
      "\n",
      "------------------------------------------------------------\n",
      "S3 | DistilBERT | Strategy: bert\n",
      "Distribusi train after aug: Counter({np.int64(4): 368, np.int64(5): 368, np.int64(2): 368, np.int64(3): 368, np.int64(1): 368})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following layers were not sharded: distilbert.transformer.layer.*.attention.out_lin.weight, distilbert.transformer.layer.*.output_layer_norm.weight, distilbert.transformer.layer.*.ffn.lin*.bias, distilbert.transformer.layer.*.attention.out_lin.bias, distilbert.transformer.layer.*.attention.q_lin.weight, distilbert.transformer.layer.*.sa_layer_norm.bias, pre_classifier.weight, distilbert.transformer.layer.*.attention.v_lin.weight, pre_classifier.bias, distilbert.embeddings.position_embeddings.weight, classifier.bias, classifier.weight, distilbert.transformer.layer.*.attention.k_lin.bias, distilbert.transformer.layer.*.attention.q_lin.bias, distilbert.embeddings.LayerNorm.bias, distilbert.transformer.layer.*.attention.k_lin.weight, distilbert.embeddings.word_embeddings.weight, distilbert.transformer.layer.*.sa_layer_norm.weight, distilbert.transformer.layer.*.ffn.lin*.weight, distilbert.transformer.layer.*.output_layer_norm.bias, distilbert.embeddings.LayerNorm.weight, distilbert.transformer.layer.*.attention.v_lin.bias\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b1be470960e84d4d88fb9641c4570707",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1840 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b38da59774214330b85db5ff5960c046",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/220 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\LENOVO\\AppData\\Local\\Temp\\ipykernel_36440\\3575509826.py:63: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `WeightedTrainer.__init__`. Use `processing_class` instead.\n",
      "  super().__init__(*args, **kwargs)\n",
      "c:\\Users\\LENOVO\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:668: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1840' max='2760' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1840/2760 17:22 < 08:41, 1.76 it/s, Epoch 2/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1 Macro</th>\n",
       "      <th>F1 Weighted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.169800</td>\n",
       "      <td>1.145289</td>\n",
       "      <td>0.522727</td>\n",
       "      <td>0.473038</td>\n",
       "      <td>0.526338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.806700</td>\n",
       "      <td>1.674804</td>\n",
       "      <td>0.472727</td>\n",
       "      <td>0.374616</td>\n",
       "      <td>0.472064</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\LENOVO\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:668: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "c:\\Users\\LENOVO\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:668: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='110' max='110' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [110/110 00:11]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== distilbert-base-uncased (S3 aug=bert) ===\n",
      "{'eval_loss': 1.1452891826629639, 'eval_accuracy': 0.5227272727272727, 'eval_f1_macro': 0.47303775849195767, 'eval_f1_weighted': 0.5263382251384536, 'eval_runtime': 11.5528, 'eval_samples_per_second': 19.043, 'eval_steps_per_second': 9.522, 'epoch': 2.0}\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>f1_macro</th>\n",
       "      <th>f1_weighted</th>\n",
       "      <th>eval_loss</th>\n",
       "      <th>epoch</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>distilbert-base-uncased (S3 aug=eda)</td>\n",
       "      <td>0.554545</td>\n",
       "      <td>0.479856</td>\n",
       "      <td>0.555061</td>\n",
       "      <td>1.971937</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>distilbert-base-uncased (S3 aug=bert)</td>\n",
       "      <td>0.522727</td>\n",
       "      <td>0.473038</td>\n",
       "      <td>0.526338</td>\n",
       "      <td>1.145289</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>distilbert-base-uncased (S3 aug=modified_eda)</td>\n",
       "      <td>0.545455</td>\n",
       "      <td>0.440164</td>\n",
       "      <td>0.544512</td>\n",
       "      <td>2.122706</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>distilbert-base-uncased (S3 aug=backtranslation)</td>\n",
       "      <td>0.536364</td>\n",
       "      <td>0.435818</td>\n",
       "      <td>0.535759</td>\n",
       "      <td>1.931003</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              model  accuracy  f1_macro  \\\n",
       "0              distilbert-base-uncased (S3 aug=eda)  0.554545  0.479856   \n",
       "3             distilbert-base-uncased (S3 aug=bert)  0.522727  0.473038   \n",
       "1     distilbert-base-uncased (S3 aug=modified_eda)  0.545455  0.440164   \n",
       "2  distilbert-base-uncased (S3 aug=backtranslation)  0.536364  0.435818   \n",
       "\n",
       "   f1_weighted  eval_loss  epoch  \n",
       "0     0.555061   1.971937    3.0  \n",
       "3     0.526338   1.145289    2.0  \n",
       "1     0.544512   2.122706    3.0  \n",
       "2     0.535759   1.931003    3.0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "results_s3_distil = []\n",
    "\n",
    "for strat in AUG_STRATEGIES:\n",
    "    try:\n",
    "        print(\"\\n\" + \"-\"*60)\n",
    "        print(\"S3 | DistilBERT | Strategy:\", strat)\n",
    "        Xa, ya = augment_minority(\n",
    "            X_train, y_train,\n",
    "            strategy=strat,\n",
    "            target_multiplier=TARGET_MULTIPLIER,\n",
    "            max_aug_per_sample=MAX_AUG_PER_SAMPLE\n",
    "        )\n",
    "        print(\"Distribusi train after aug:\", Counter(ya))\n",
    "\n",
    "        distil_s3_res = train_eval_transformer(\n",
    "            \"distilbert-base-uncased\",\n",
    "            Xa, ya, X_test, y_test,\n",
    "            epochs=DEFAULT_EPOCHS,\n",
    "            batch_size=DEFAULT_BATCH,\n",
    "            max_length=DEFAULT_MAX_LEN,\n",
    "            out_dir=f\"./tmp_distilbert_s3_{strat}\",\n",
    "            suffix=f\" (S3 aug={strat})\",\n",
    "            use_class_weights=True\n",
    "        )\n",
    "        results_s3_distil.append(distil_s3_res)\n",
    "    except Exception as e:\n",
    "        print(f\"DistilBERT S3 ({strat}) error:\", e)\n",
    "\n",
    "s3_distil_table = pd.DataFrame(results_s3_distil)\n",
    "display(s3_distil_table.sort_values(\"f1_macro\", ascending=False))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b0c362c",
   "metadata": {},
   "source": [
    "### Rekap Skenario 3 (Gabungan Semua Model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "ca6850b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>f1_macro</th>\n",
       "      <th>f1_weighted</th>\n",
       "      <th>eval_loss</th>\n",
       "      <th>epoch</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>distilbert-base-uncased (S3 aug=eda)</td>\n",
       "      <td>0.554545</td>\n",
       "      <td>0.479856</td>\n",
       "      <td>0.555061</td>\n",
       "      <td>1.971937</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>distilbert-base-uncased (S3 aug=bert)</td>\n",
       "      <td>0.522727</td>\n",
       "      <td>0.473038</td>\n",
       "      <td>0.526338</td>\n",
       "      <td>1.145289</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>distilbert-base-uncased (S3 aug=modified_eda)</td>\n",
       "      <td>0.545455</td>\n",
       "      <td>0.440164</td>\n",
       "      <td>0.544512</td>\n",
       "      <td>2.122706</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>distilbert-base-uncased (S3 aug=backtranslation)</td>\n",
       "      <td>0.536364</td>\n",
       "      <td>0.435818</td>\n",
       "      <td>0.535759</td>\n",
       "      <td>1.931003</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Logistic Regression (S3 aug=modified_eda)</td>\n",
       "      <td>0.509091</td>\n",
       "      <td>0.434682</td>\n",
       "      <td>0.507211</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>bert-base-uncased (S3 aug=bert)</td>\n",
       "      <td>0.513636</td>\n",
       "      <td>0.430704</td>\n",
       "      <td>0.514529</td>\n",
       "      <td>2.197634</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Naive Bayes (S3 aug=eda)</td>\n",
       "      <td>0.472727</td>\n",
       "      <td>0.422089</td>\n",
       "      <td>0.484189</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Logistic Regression (S3 aug=bert)</td>\n",
       "      <td>0.481818</td>\n",
       "      <td>0.409717</td>\n",
       "      <td>0.479313</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Naive Bayes (S3 aug=bert)</td>\n",
       "      <td>0.477273</td>\n",
       "      <td>0.392419</td>\n",
       "      <td>0.469963</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>bert-base-uncased (S3 aug=eda)</td>\n",
       "      <td>0.527273</td>\n",
       "      <td>0.391515</td>\n",
       "      <td>0.518081</td>\n",
       "      <td>2.454143</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Naive Bayes (S3 aug=modified_eda)</td>\n",
       "      <td>0.450000</td>\n",
       "      <td>0.390780</td>\n",
       "      <td>0.467503</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Logistic Regression (S3 aug=eda)</td>\n",
       "      <td>0.481818</td>\n",
       "      <td>0.390027</td>\n",
       "      <td>0.476690</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>bert-base-uncased (S3 aug=modified_eda)</td>\n",
       "      <td>0.522727</td>\n",
       "      <td>0.382219</td>\n",
       "      <td>0.490945</td>\n",
       "      <td>1.453806</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Naive Bayes (S3 aug=backtranslation)</td>\n",
       "      <td>0.427273</td>\n",
       "      <td>0.380640</td>\n",
       "      <td>0.447061</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Linear SVM (S3 aug=modified_eda)</td>\n",
       "      <td>0.472727</td>\n",
       "      <td>0.377363</td>\n",
       "      <td>0.463659</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>bert-base-uncased (S3 aug=backtranslation)</td>\n",
       "      <td>0.522727</td>\n",
       "      <td>0.377247</td>\n",
       "      <td>0.509995</td>\n",
       "      <td>2.698585</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Linear SVM (S3 aug=backtranslation)</td>\n",
       "      <td>0.472727</td>\n",
       "      <td>0.376480</td>\n",
       "      <td>0.461487</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Logistic Regression (S3 aug=backtranslation)</td>\n",
       "      <td>0.459091</td>\n",
       "      <td>0.370295</td>\n",
       "      <td>0.455165</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Linear SVM (S3 aug=bert)</td>\n",
       "      <td>0.463636</td>\n",
       "      <td>0.369497</td>\n",
       "      <td>0.457001</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Linear SVM (S3 aug=eda)</td>\n",
       "      <td>0.445455</td>\n",
       "      <td>0.353253</td>\n",
       "      <td>0.440307</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>LSTM (S3 aug=backtranslation)</td>\n",
       "      <td>0.418182</td>\n",
       "      <td>0.117949</td>\n",
       "      <td>0.246620</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>LSTM (S3 aug=modified_eda)</td>\n",
       "      <td>0.286364</td>\n",
       "      <td>0.089046</td>\n",
       "      <td>0.127498</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>LSTM (S3 aug=eda)</td>\n",
       "      <td>0.159091</td>\n",
       "      <td>0.054902</td>\n",
       "      <td>0.043672</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>LSTM (S3 aug=bert)</td>\n",
       "      <td>0.159091</td>\n",
       "      <td>0.054902</td>\n",
       "      <td>0.043672</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               model  accuracy  f1_macro  \\\n",
       "16              distilbert-base-uncased (S3 aug=eda)  0.554545  0.479856   \n",
       "19             distilbert-base-uncased (S3 aug=bert)  0.522727  0.473038   \n",
       "17     distilbert-base-uncased (S3 aug=modified_eda)  0.545455  0.440164   \n",
       "18  distilbert-base-uncased (S3 aug=backtranslation)  0.536364  0.435818   \n",
       "21         Logistic Regression (S3 aug=modified_eda)  0.509091  0.434682   \n",
       "15                   bert-base-uncased (S3 aug=bert)  0.513636  0.430704   \n",
       "4                           Naive Bayes (S3 aug=eda)  0.472727  0.422089   \n",
       "23                 Logistic Regression (S3 aug=bert)  0.481818  0.409717   \n",
       "7                          Naive Bayes (S3 aug=bert)  0.477273  0.392419   \n",
       "12                    bert-base-uncased (S3 aug=eda)  0.527273  0.391515   \n",
       "5                  Naive Bayes (S3 aug=modified_eda)  0.450000  0.390780   \n",
       "20                  Logistic Regression (S3 aug=eda)  0.481818  0.390027   \n",
       "13           bert-base-uncased (S3 aug=modified_eda)  0.522727  0.382219   \n",
       "6               Naive Bayes (S3 aug=backtranslation)  0.427273  0.380640   \n",
       "1                   Linear SVM (S3 aug=modified_eda)  0.472727  0.377363   \n",
       "14        bert-base-uncased (S3 aug=backtranslation)  0.522727  0.377247   \n",
       "2                Linear SVM (S3 aug=backtranslation)  0.472727  0.376480   \n",
       "22      Logistic Regression (S3 aug=backtranslation)  0.459091  0.370295   \n",
       "3                           Linear SVM (S3 aug=bert)  0.463636  0.369497   \n",
       "0                            Linear SVM (S3 aug=eda)  0.445455  0.353253   \n",
       "10                     LSTM (S3 aug=backtranslation)  0.418182  0.117949   \n",
       "9                         LSTM (S3 aug=modified_eda)  0.286364  0.089046   \n",
       "8                                  LSTM (S3 aug=eda)  0.159091  0.054902   \n",
       "11                                LSTM (S3 aug=bert)  0.159091  0.054902   \n",
       "\n",
       "    f1_weighted  eval_loss  epoch  \n",
       "16     0.555061   1.971937    3.0  \n",
       "19     0.526338   1.145289    2.0  \n",
       "17     0.544512   2.122706    3.0  \n",
       "18     0.535759   1.931003    3.0  \n",
       "21     0.507211        NaN    NaN  \n",
       "15     0.514529   2.197634    3.0  \n",
       "4      0.484189        NaN    NaN  \n",
       "23     0.479313        NaN    NaN  \n",
       "7      0.469963        NaN    NaN  \n",
       "12     0.518081   2.454143    3.0  \n",
       "5      0.467503        NaN    NaN  \n",
       "20     0.476690        NaN    NaN  \n",
       "13     0.490945   1.453806    2.0  \n",
       "6      0.447061        NaN    NaN  \n",
       "1      0.463659        NaN    NaN  \n",
       "14     0.509995   2.698585    3.0  \n",
       "2      0.461487        NaN    NaN  \n",
       "22     0.455165        NaN    NaN  \n",
       "3      0.457001        NaN    NaN  \n",
       "0      0.440307        NaN    NaN  \n",
       "10     0.246620        NaN    NaN  \n",
       "9      0.127498        NaN    NaN  \n",
       "8      0.043672        NaN    NaN  \n",
       "11     0.043672        NaN    NaN  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "results_s3 = []\n",
    "if 'results_s3_svm' in globals(): results_s3 += results_s3_svm\n",
    "if 'results_s3_nb' in globals(): results_s3 += results_s3_nb\n",
    "if 'results_s3_lstm' in globals(): results_s3 += results_s3_lstm\n",
    "if 'results_s3_bert' in globals(): results_s3 += results_s3_bert\n",
    "if 'results_s3_distil' in globals(): results_s3 += results_s3_distil\n",
    "if 'results_s3_lr' in globals(): results_s3 += results_s3_lr\n",
    "\n",
    "s3_table = pd.DataFrame(results_s3)\n",
    "display(s3_table.sort_values(\"f1_macro\", ascending=False))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea1804dc",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 13. Rekap Per Skenario & Summary Akhir (Seragam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "76d2928c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "S1 (baseline):\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>f1_macro</th>\n",
       "      <th>f1_weighted</th>\n",
       "      <th>eval_loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>bert-base-uncased (S1 baseline)</td>\n",
       "      <td>0.577273</td>\n",
       "      <td>0.427096</td>\n",
       "      <td>0.546487</td>\n",
       "      <td>1.348772</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>distilbert-base-uncased (S1 baseline)</td>\n",
       "      <td>0.581818</td>\n",
       "      <td>0.426630</td>\n",
       "      <td>0.552515</td>\n",
       "      <td>1.251063</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Linear SVM (S1 baseline)</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.375765</td>\n",
       "      <td>0.473433</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Linear SVM (S1 baseline)</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.375765</td>\n",
       "      <td>0.473433</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>bert-base-uncased (S1 baseline)</td>\n",
       "      <td>0.486364</td>\n",
       "      <td>0.210667</td>\n",
       "      <td>0.400061</td>\n",
       "      <td>1.147912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Naive Bayes (S1 baseline)</td>\n",
       "      <td>0.436364</td>\n",
       "      <td>0.147927</td>\n",
       "      <td>0.291245</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Naive Bayes (S1 baseline)</td>\n",
       "      <td>0.436364</td>\n",
       "      <td>0.147927</td>\n",
       "      <td>0.291245</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>LSTM (S1 baseline)</td>\n",
       "      <td>0.418182</td>\n",
       "      <td>0.117949</td>\n",
       "      <td>0.246620</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>LSTM (S1 baseline)</td>\n",
       "      <td>0.418182</td>\n",
       "      <td>0.117949</td>\n",
       "      <td>0.246620</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   model  accuracy  f1_macro  f1_weighted  \\\n",
       "7        bert-base-uncased (S1 baseline)  0.577273  0.427096     0.546487   \n",
       "8  distilbert-base-uncased (S1 baseline)  0.581818  0.426630     0.552515   \n",
       "0               Linear SVM (S1 baseline)  0.500000  0.375765     0.473433   \n",
       "4               Linear SVM (S1 baseline)  0.500000  0.375765     0.473433   \n",
       "3        bert-base-uncased (S1 baseline)  0.486364  0.210667     0.400061   \n",
       "5              Naive Bayes (S1 baseline)  0.436364  0.147927     0.291245   \n",
       "1              Naive Bayes (S1 baseline)  0.436364  0.147927     0.291245   \n",
       "2                     LSTM (S1 baseline)  0.418182  0.117949     0.246620   \n",
       "6                     LSTM (S1 baseline)  0.418182  0.117949     0.246620   \n",
       "\n",
       "   eval_loss  \n",
       "7   1.348772  \n",
       "8   1.251063  \n",
       "0        NaN  \n",
       "4        NaN  \n",
       "3   1.147912  \n",
       "5        NaN  \n",
       "1        NaN  \n",
       "2        NaN  \n",
       "6        NaN  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "S3 (aug):\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>f1_macro</th>\n",
       "      <th>f1_weighted</th>\n",
       "      <th>eval_loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>distilbert-base-uncased (S3 aug=eda)</td>\n",
       "      <td>0.554545</td>\n",
       "      <td>0.479856</td>\n",
       "      <td>0.555061</td>\n",
       "      <td>1.971937</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>distilbert-base-uncased (S3 aug=bert)</td>\n",
       "      <td>0.522727</td>\n",
       "      <td>0.473038</td>\n",
       "      <td>0.526338</td>\n",
       "      <td>1.145289</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>distilbert-base-uncased (S3 aug=modified_eda)</td>\n",
       "      <td>0.545455</td>\n",
       "      <td>0.440164</td>\n",
       "      <td>0.544512</td>\n",
       "      <td>2.122706</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>distilbert-base-uncased (S3 aug=backtranslation)</td>\n",
       "      <td>0.536364</td>\n",
       "      <td>0.435818</td>\n",
       "      <td>0.535759</td>\n",
       "      <td>1.931003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Logistic Regression (S3 aug=modified_eda)</td>\n",
       "      <td>0.509091</td>\n",
       "      <td>0.434682</td>\n",
       "      <td>0.507211</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>bert-base-uncased (S3 aug=bert)</td>\n",
       "      <td>0.513636</td>\n",
       "      <td>0.430704</td>\n",
       "      <td>0.514529</td>\n",
       "      <td>2.197634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Naive Bayes (S3 aug=eda)</td>\n",
       "      <td>0.472727</td>\n",
       "      <td>0.422089</td>\n",
       "      <td>0.484189</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Logistic Regression (S3 aug=bert)</td>\n",
       "      <td>0.481818</td>\n",
       "      <td>0.409717</td>\n",
       "      <td>0.479313</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Naive Bayes (S3 aug=bert)</td>\n",
       "      <td>0.477273</td>\n",
       "      <td>0.392419</td>\n",
       "      <td>0.469963</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>bert-base-uncased (S3 aug=eda)</td>\n",
       "      <td>0.527273</td>\n",
       "      <td>0.391515</td>\n",
       "      <td>0.518081</td>\n",
       "      <td>2.454143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Naive Bayes (S3 aug=modified_eda)</td>\n",
       "      <td>0.450000</td>\n",
       "      <td>0.390780</td>\n",
       "      <td>0.467503</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Logistic Regression (S3 aug=eda)</td>\n",
       "      <td>0.481818</td>\n",
       "      <td>0.390027</td>\n",
       "      <td>0.476690</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>bert-base-uncased (S3 aug=modified_eda)</td>\n",
       "      <td>0.522727</td>\n",
       "      <td>0.382219</td>\n",
       "      <td>0.490945</td>\n",
       "      <td>1.453806</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Naive Bayes (S3 aug=backtranslation)</td>\n",
       "      <td>0.427273</td>\n",
       "      <td>0.380640</td>\n",
       "      <td>0.447061</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Linear SVM (S3 aug=modified_eda)</td>\n",
       "      <td>0.472727</td>\n",
       "      <td>0.377363</td>\n",
       "      <td>0.463659</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>bert-base-uncased (S3 aug=backtranslation)</td>\n",
       "      <td>0.522727</td>\n",
       "      <td>0.377247</td>\n",
       "      <td>0.509995</td>\n",
       "      <td>2.698585</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Linear SVM (S3 aug=backtranslation)</td>\n",
       "      <td>0.472727</td>\n",
       "      <td>0.376480</td>\n",
       "      <td>0.461487</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Logistic Regression (S3 aug=backtranslation)</td>\n",
       "      <td>0.459091</td>\n",
       "      <td>0.370295</td>\n",
       "      <td>0.455165</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Linear SVM (S3 aug=bert)</td>\n",
       "      <td>0.463636</td>\n",
       "      <td>0.369497</td>\n",
       "      <td>0.457001</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Linear SVM (S3 aug=eda)</td>\n",
       "      <td>0.445455</td>\n",
       "      <td>0.353253</td>\n",
       "      <td>0.440307</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>LSTM (S3 aug=backtranslation)</td>\n",
       "      <td>0.418182</td>\n",
       "      <td>0.117949</td>\n",
       "      <td>0.246620</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>LSTM (S3 aug=modified_eda)</td>\n",
       "      <td>0.286364</td>\n",
       "      <td>0.089046</td>\n",
       "      <td>0.127498</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>LSTM (S3 aug=eda)</td>\n",
       "      <td>0.159091</td>\n",
       "      <td>0.054902</td>\n",
       "      <td>0.043672</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>LSTM (S3 aug=bert)</td>\n",
       "      <td>0.159091</td>\n",
       "      <td>0.054902</td>\n",
       "      <td>0.043672</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               model  accuracy  f1_macro  \\\n",
       "16              distilbert-base-uncased (S3 aug=eda)  0.554545  0.479856   \n",
       "19             distilbert-base-uncased (S3 aug=bert)  0.522727  0.473038   \n",
       "17     distilbert-base-uncased (S3 aug=modified_eda)  0.545455  0.440164   \n",
       "18  distilbert-base-uncased (S3 aug=backtranslation)  0.536364  0.435818   \n",
       "21         Logistic Regression (S3 aug=modified_eda)  0.509091  0.434682   \n",
       "15                   bert-base-uncased (S3 aug=bert)  0.513636  0.430704   \n",
       "4                           Naive Bayes (S3 aug=eda)  0.472727  0.422089   \n",
       "23                 Logistic Regression (S3 aug=bert)  0.481818  0.409717   \n",
       "7                          Naive Bayes (S3 aug=bert)  0.477273  0.392419   \n",
       "12                    bert-base-uncased (S3 aug=eda)  0.527273  0.391515   \n",
       "5                  Naive Bayes (S3 aug=modified_eda)  0.450000  0.390780   \n",
       "20                  Logistic Regression (S3 aug=eda)  0.481818  0.390027   \n",
       "13           bert-base-uncased (S3 aug=modified_eda)  0.522727  0.382219   \n",
       "6               Naive Bayes (S3 aug=backtranslation)  0.427273  0.380640   \n",
       "1                   Linear SVM (S3 aug=modified_eda)  0.472727  0.377363   \n",
       "14        bert-base-uncased (S3 aug=backtranslation)  0.522727  0.377247   \n",
       "2                Linear SVM (S3 aug=backtranslation)  0.472727  0.376480   \n",
       "22      Logistic Regression (S3 aug=backtranslation)  0.459091  0.370295   \n",
       "3                           Linear SVM (S3 aug=bert)  0.463636  0.369497   \n",
       "0                            Linear SVM (S3 aug=eda)  0.445455  0.353253   \n",
       "10                     LSTM (S3 aug=backtranslation)  0.418182  0.117949   \n",
       "9                         LSTM (S3 aug=modified_eda)  0.286364  0.089046   \n",
       "8                                  LSTM (S3 aug=eda)  0.159091  0.054902   \n",
       "11                                LSTM (S3 aug=bert)  0.159091  0.054902   \n",
       "\n",
       "    f1_weighted  eval_loss  \n",
       "16     0.555061   1.971937  \n",
       "19     0.526338   1.145289  \n",
       "17     0.544512   2.122706  \n",
       "18     0.535759   1.931003  \n",
       "21     0.507211        NaN  \n",
       "15     0.514529   2.197634  \n",
       "4      0.484189        NaN  \n",
       "23     0.479313        NaN  \n",
       "7      0.469963        NaN  \n",
       "12     0.518081   2.454143  \n",
       "5      0.467503        NaN  \n",
       "20     0.476690        NaN  \n",
       "13     0.490945   1.453806  \n",
       "6      0.447061        NaN  \n",
       "1      0.463659        NaN  \n",
       "14     0.509995   2.698585  \n",
       "2      0.461487        NaN  \n",
       "22     0.455165        NaN  \n",
       "3      0.457001        NaN  \n",
       "0      0.440307        NaN  \n",
       "10     0.246620        NaN  \n",
       "9      0.127498        NaN  \n",
       "8      0.043672        NaN  \n",
       "11     0.043672        NaN  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "SUMMARY:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>f1_macro</th>\n",
       "      <th>f1_weighted</th>\n",
       "      <th>eval_loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>distilbert-base-uncased (S3 aug=eda)</td>\n",
       "      <td>0.554545</td>\n",
       "      <td>0.479856</td>\n",
       "      <td>0.555061</td>\n",
       "      <td>1.971937</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>distilbert-base-uncased (S3 aug=bert)</td>\n",
       "      <td>0.522727</td>\n",
       "      <td>0.473038</td>\n",
       "      <td>0.526338</td>\n",
       "      <td>1.145289</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>distilbert-base-uncased (S3 aug=modified_eda)</td>\n",
       "      <td>0.545455</td>\n",
       "      <td>0.440164</td>\n",
       "      <td>0.544512</td>\n",
       "      <td>2.122706</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>distilbert-base-uncased (S3 aug=backtranslation)</td>\n",
       "      <td>0.536364</td>\n",
       "      <td>0.435818</td>\n",
       "      <td>0.535759</td>\n",
       "      <td>1.931003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>Logistic Regression (S3 aug=modified_eda)</td>\n",
       "      <td>0.509091</td>\n",
       "      <td>0.434682</td>\n",
       "      <td>0.507211</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>bert-base-uncased (S3 aug=bert)</td>\n",
       "      <td>0.513636</td>\n",
       "      <td>0.430704</td>\n",
       "      <td>0.514529</td>\n",
       "      <td>2.197634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>bert-base-uncased (S1 baseline)</td>\n",
       "      <td>0.577273</td>\n",
       "      <td>0.427096</td>\n",
       "      <td>0.546487</td>\n",
       "      <td>1.348772</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>distilbert-base-uncased (S1 baseline)</td>\n",
       "      <td>0.581818</td>\n",
       "      <td>0.426630</td>\n",
       "      <td>0.552515</td>\n",
       "      <td>1.251063</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Naive Bayes (S3 aug=eda)</td>\n",
       "      <td>0.472727</td>\n",
       "      <td>0.422089</td>\n",
       "      <td>0.484189</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>Logistic Regression (S3 aug=bert)</td>\n",
       "      <td>0.481818</td>\n",
       "      <td>0.409717</td>\n",
       "      <td>0.479313</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Naive Bayes (S3 aug=bert)</td>\n",
       "      <td>0.477273</td>\n",
       "      <td>0.392419</td>\n",
       "      <td>0.469963</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>bert-base-uncased (S3 aug=eda)</td>\n",
       "      <td>0.527273</td>\n",
       "      <td>0.391515</td>\n",
       "      <td>0.518081</td>\n",
       "      <td>2.454143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Naive Bayes (S3 aug=modified_eda)</td>\n",
       "      <td>0.450000</td>\n",
       "      <td>0.390780</td>\n",
       "      <td>0.467503</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>Logistic Regression (S3 aug=eda)</td>\n",
       "      <td>0.481818</td>\n",
       "      <td>0.390027</td>\n",
       "      <td>0.476690</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>bert-base-uncased (S3 aug=modified_eda)</td>\n",
       "      <td>0.522727</td>\n",
       "      <td>0.382219</td>\n",
       "      <td>0.490945</td>\n",
       "      <td>1.453806</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Naive Bayes (S3 aug=backtranslation)</td>\n",
       "      <td>0.427273</td>\n",
       "      <td>0.380640</td>\n",
       "      <td>0.447061</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Linear SVM (S3 aug=modified_eda)</td>\n",
       "      <td>0.472727</td>\n",
       "      <td>0.377363</td>\n",
       "      <td>0.463659</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>bert-base-uncased (S3 aug=backtranslation)</td>\n",
       "      <td>0.522727</td>\n",
       "      <td>0.377247</td>\n",
       "      <td>0.509995</td>\n",
       "      <td>2.698585</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Linear SVM (S3 aug=backtranslation)</td>\n",
       "      <td>0.472727</td>\n",
       "      <td>0.376480</td>\n",
       "      <td>0.461487</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Linear SVM (S1 baseline)</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.375765</td>\n",
       "      <td>0.473433</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Linear SVM (S1 baseline)</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.375765</td>\n",
       "      <td>0.473433</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>Logistic Regression (S3 aug=backtranslation)</td>\n",
       "      <td>0.459091</td>\n",
       "      <td>0.370295</td>\n",
       "      <td>0.455165</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Linear SVM (S3 aug=bert)</td>\n",
       "      <td>0.463636</td>\n",
       "      <td>0.369497</td>\n",
       "      <td>0.457001</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Linear SVM (S3 aug=eda)</td>\n",
       "      <td>0.445455</td>\n",
       "      <td>0.353253</td>\n",
       "      <td>0.440307</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>bert-base-uncased (S1 baseline)</td>\n",
       "      <td>0.486364</td>\n",
       "      <td>0.210667</td>\n",
       "      <td>0.400061</td>\n",
       "      <td>1.147912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Naive Bayes (S1 baseline)</td>\n",
       "      <td>0.436364</td>\n",
       "      <td>0.147927</td>\n",
       "      <td>0.291245</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Naive Bayes (S1 baseline)</td>\n",
       "      <td>0.436364</td>\n",
       "      <td>0.147927</td>\n",
       "      <td>0.291245</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>LSTM (S1 baseline)</td>\n",
       "      <td>0.418182</td>\n",
       "      <td>0.117949</td>\n",
       "      <td>0.246620</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>LSTM (S1 baseline)</td>\n",
       "      <td>0.418182</td>\n",
       "      <td>0.117949</td>\n",
       "      <td>0.246620</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>LSTM (S3 aug=backtranslation)</td>\n",
       "      <td>0.418182</td>\n",
       "      <td>0.117949</td>\n",
       "      <td>0.246620</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>LSTM (S3 aug=modified_eda)</td>\n",
       "      <td>0.286364</td>\n",
       "      <td>0.089046</td>\n",
       "      <td>0.127498</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>LSTM (S3 aug=eda)</td>\n",
       "      <td>0.159091</td>\n",
       "      <td>0.054902</td>\n",
       "      <td>0.043672</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>LSTM (S3 aug=bert)</td>\n",
       "      <td>0.159091</td>\n",
       "      <td>0.054902</td>\n",
       "      <td>0.043672</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               model  accuracy  f1_macro  \\\n",
       "25              distilbert-base-uncased (S3 aug=eda)  0.554545  0.479856   \n",
       "28             distilbert-base-uncased (S3 aug=bert)  0.522727  0.473038   \n",
       "26     distilbert-base-uncased (S3 aug=modified_eda)  0.545455  0.440164   \n",
       "27  distilbert-base-uncased (S3 aug=backtranslation)  0.536364  0.435818   \n",
       "30         Logistic Regression (S3 aug=modified_eda)  0.509091  0.434682   \n",
       "24                   bert-base-uncased (S3 aug=bert)  0.513636  0.430704   \n",
       "7                    bert-base-uncased (S1 baseline)  0.577273  0.427096   \n",
       "8              distilbert-base-uncased (S1 baseline)  0.581818  0.426630   \n",
       "13                          Naive Bayes (S3 aug=eda)  0.472727  0.422089   \n",
       "32                 Logistic Regression (S3 aug=bert)  0.481818  0.409717   \n",
       "16                         Naive Bayes (S3 aug=bert)  0.477273  0.392419   \n",
       "21                    bert-base-uncased (S3 aug=eda)  0.527273  0.391515   \n",
       "14                 Naive Bayes (S3 aug=modified_eda)  0.450000  0.390780   \n",
       "29                  Logistic Regression (S3 aug=eda)  0.481818  0.390027   \n",
       "22           bert-base-uncased (S3 aug=modified_eda)  0.522727  0.382219   \n",
       "15              Naive Bayes (S3 aug=backtranslation)  0.427273  0.380640   \n",
       "10                  Linear SVM (S3 aug=modified_eda)  0.472727  0.377363   \n",
       "23        bert-base-uncased (S3 aug=backtranslation)  0.522727  0.377247   \n",
       "11               Linear SVM (S3 aug=backtranslation)  0.472727  0.376480   \n",
       "0                           Linear SVM (S1 baseline)  0.500000  0.375765   \n",
       "4                           Linear SVM (S1 baseline)  0.500000  0.375765   \n",
       "31      Logistic Regression (S3 aug=backtranslation)  0.459091  0.370295   \n",
       "12                          Linear SVM (S3 aug=bert)  0.463636  0.369497   \n",
       "9                            Linear SVM (S3 aug=eda)  0.445455  0.353253   \n",
       "3                    bert-base-uncased (S1 baseline)  0.486364  0.210667   \n",
       "5                          Naive Bayes (S1 baseline)  0.436364  0.147927   \n",
       "1                          Naive Bayes (S1 baseline)  0.436364  0.147927   \n",
       "2                                 LSTM (S1 baseline)  0.418182  0.117949   \n",
       "6                                 LSTM (S1 baseline)  0.418182  0.117949   \n",
       "19                     LSTM (S3 aug=backtranslation)  0.418182  0.117949   \n",
       "18                        LSTM (S3 aug=modified_eda)  0.286364  0.089046   \n",
       "17                                 LSTM (S3 aug=eda)  0.159091  0.054902   \n",
       "20                                LSTM (S3 aug=bert)  0.159091  0.054902   \n",
       "\n",
       "    f1_weighted  eval_loss  \n",
       "25     0.555061   1.971937  \n",
       "28     0.526338   1.145289  \n",
       "26     0.544512   2.122706  \n",
       "27     0.535759   1.931003  \n",
       "30     0.507211        NaN  \n",
       "24     0.514529   2.197634  \n",
       "7      0.546487   1.348772  \n",
       "8      0.552515   1.251063  \n",
       "13     0.484189        NaN  \n",
       "32     0.479313        NaN  \n",
       "16     0.469963        NaN  \n",
       "21     0.518081   2.454143  \n",
       "14     0.467503        NaN  \n",
       "29     0.476690        NaN  \n",
       "22     0.490945   1.453806  \n",
       "15     0.447061        NaN  \n",
       "10     0.463659        NaN  \n",
       "23     0.509995   2.698585  \n",
       "11     0.461487        NaN  \n",
       "0      0.473433        NaN  \n",
       "4      0.473433        NaN  \n",
       "31     0.455165        NaN  \n",
       "12     0.457001        NaN  \n",
       "9      0.440307        NaN  \n",
       "3      0.400061   1.147912  \n",
       "5      0.291245        NaN  \n",
       "1      0.291245        NaN  \n",
       "2      0.246620        NaN  \n",
       "6      0.246620        NaN  \n",
       "19     0.246620        NaN  \n",
       "18     0.127498        NaN  \n",
       "17     0.043672        NaN  \n",
       "20     0.043672        NaN  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def pick_cols(df_):\n",
    "    cols = [\"model\", \"accuracy\", \"f1_macro\", \"f1_weighted\"]\n",
    "    if \"eval_loss\" in df_.columns:\n",
    "        cols.append(\"eval_loss\")\n",
    "    return df_[cols].copy()\n",
    "\n",
    "s1_clean = pick_cols(s1_table) if 's1_table' in globals() else pd.DataFrame()\n",
    "s3_clean = pick_cols(s3_table) if 's3_table' in globals() else pd.DataFrame()\n",
    "\n",
    "print(\"S1 (baseline):\")\n",
    "display(s1_clean.sort_values(\"f1_macro\", ascending=False) if not s1_clean.empty else s1_clean)\n",
    "print(\"\\nS3 (aug):\")\n",
    "display(s3_clean.sort_values(\"f1_macro\", ascending=False) if not s3_clean.empty else s3_clean)\n",
    "\n",
    "summary = pd.concat([s1_clean, s3_clean], ignore_index=True) if not (s1_clean.empty and s3_clean.empty) else pd.DataFrame()\n",
    "summary = summary.sort_values(\"f1_macro\", ascending=False) if not summary.empty else summary\n",
    "\n",
    "print(\"\\nSUMMARY:\")\n",
    "display(summary)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49b482e9",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 14. Template Narasi Singkat (Opsional)\n",
    "- **S1**: baseline natural (tanpa penanganan imbalance) untuk 5 arsitektur.\n",
    "- **S2**: menguji efektivitas penanganan imbalance **non-semantic** tanpa mengubah teks.\n",
    "- **S3**: menguji apakah augmentasi teks memberikan peningkatan tambahan di atas balancing non-aug.  \n",
    "  Di notebook ini, **S3 dipecah per model**, jadi kamu bisa membandingkan efek EDA vs Modified EDA\n",
    "  untuk setiap arsitektur secara rapi."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
